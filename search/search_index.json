{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"scikit-lego","text":"<p>We love scikit learn but very often we find ourselves writing custom transformers, metrics and models. The goal of this project is to attempt to consolidate these into a package that offers code quality/testing. This project is a collaboration between multiple companies in the Netherlands. Note that we're not formally affiliated with the scikit-learn project at all.</p>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>LEGO\u00ae is a trademark of the LEGO Group of companies which does not sponsor, authorize or endorse this project. Also note this package, albeit designing to be used on top of scikit-learn, is not associated with that project in any formal manner.</p> <p>The goal of the package is to allow you to joyfully build with new building blocks that are scikit-learn compatible.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install <code>scikit-lego</code> via pip with</p> <pre><code>pip install scikit-lego\n</code></pre> <p>For more installation options and details, check the installation section.</p>"},{"location":"#usage","title":"Usage","text":"<pre><code>from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nfrom sklego.transformers import RandomAdder\n\nX, y = ...\n\nmod = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"random_noise\", RandomAdder()),\n    (\"model\", LogisticRegression(solver='lbfgs'))\n])\n\n_ = mod.fit(X, y)\n...\n</code></pre> <p>To see more examples, please refer to the user guide section.</p>"},{"location":"contribution/","title":"Contribution","text":"<p>This project started because we saw people rewrite the same transformers and estimators at clients over and over again. Our goal is to have a place where more experimental building blocks for scikit learn pipelines might exist. This means we're usually open to ideas to add here but there are a few things to keep in mind.</p>"},{"location":"contribution/#before-you-make-a-new-feature","title":"Before You Make a New Feature","text":"<ol> <li>Discuss the feature and implementation you want to add on Github     before you write a PR for it.</li> <li>Features need a somewhat general usecase.     If the usecase is very niche it will be hard for us to consider maintaining it.</li> <li>If you're going to add a feature consider if you could help out in the maintenance of it.</li> </ol>"},{"location":"contribution/#when-writing-a-new-feature","title":"When Writing a New Feature","text":"<p>When writing a new feature there's some more details with regard to how scikit learn likes to have it's parts implemented. We will display the a sample implementation of the <code>ColumnSelector</code> below. Please review all comments marked as Important.</p> <pre><code>from sklearn.base import BaseEstimator, TransformerMixin, MetaEstimatorMixin\nfrom sklearn.utils import check_array, check_X_y\nfrom sklearn.utils.validation import FLOAT_DTYPES, check_random_state, check_is_fitted\n\nclass PandasTypeSelector(BaseEstimator, TransformerMixin):\n    TYPES = {'number', 'category', 'float', 'int', 'object', 'datetime', 'timedelta'}\n    \"\"\"\n    Select columns in a pandas dataframe based on their dtype\n\n    Parameters\n    ----------\n    include : scalar or list-like\n        Column type(s) to be selected\n    exclude : scalar or list-like\n        Column type(s) to be excluded from selection\n    \"\"\"\n    def __init__(self, include=None, exclude=None):\n        \"\"\"\n        Important:\n            You can't use `*args` or `**kwargs` in the `__init__` method.\n            scikit-learn uses the methods' call signature to figure out what\n            hyperparameters for the estimator are.\n\n        Important:\n            Keep the same name for the function argument and the attribute stored on self.\n            If you don't, the `get_params` method will try to fetch the attribute with\n            the name it has in the function signature, but as that one doesn't exist,\n            it will return `None`. This will silently break copying.\n        \"\"\"\n        self.include = include\n        self.exclude = exclude\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Saves the column names for check during transform\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            The data on which we apply the column selection.\n        y : pd.Series, default=None\n            Ignored, present for compatibility.\n        \"\"\"\n        self._check_X_for_type(X)\n        \"\"\"\n        Important:\n            Normal software engineering practices would have you put these kinds of\n            parameter checks or basic casts inside the `__init__` method.\n            `scikit-learn` will break when you do this, as the method it uses for cloning\n            estimators (e.g. while doing gridsearch) involves setting parameters directly,\n            after the class has been constructed.\n        \"\"\"\n        if len(set(self.include) - self.TYPES) &gt; 0:\n            raise ValueError(\n                f'Unrecognized type in `include`.'\n                'Expected {self.TYPES}, got {set(self.include) - self.TYPES}'\n            )\n        if len(set(self.exclude) - self.TYPES) &gt; 0:\n            raise ValueError(\n                f'Unrecognized type in `exclude`.'\n                'Expected {self.TYPES}, got {set(self.exclude) - self.TYPES}'\n            )\n\n        \"\"\"\n        Important:\n            variables that are 'learned' during the fitting process should always have a\n            trailing underscore.\n            Please don't initialize these features inside the `__init__`, but initialize\n            them in the `fit` method.\n        \"\"\"\n        self.type_columns_ = list(X.select_dtypes(include=self.include, exclude=self.exclude))\n        self.X_dtypes_ = X.dtypes\n        if len(self.type_columns_) == 0:\n            raise ValueError(f'Provided type(s) results in empty dateframe')\n\n        \"\"\"\n        Important:\n            Always return self from the `fit` method\n        \"\"\"\n        return self\n\n    \"\"\"\n    Important:\n        `y=None` exists solely for compatibility reasons. It will be removed sometime\n        in the future.\n    \"\"\"\n    def transform(self, X, y=None):\n        \"\"\"\n        Transforms pandas dataframe by (de)selecting columns based on their dtype\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            The data to select dtype for.\n        \"\"\"\n        # Important: Check whether the variables you expected to learn during fit are indeed present\n        check_is_fitted(self, ['type_columns_', 'X_dtypes_'])\n        if (self.X_dtypes_ != X.dtypes).any():\n            raise ValueError(\n                f'Column dtypes were not equal during fit and transform. Fit types: \\n'\n                f'{self.X_dtypes_}\\n'\n                'transform: \\n'\n                f'{X.dtypes}'\n            )\n\n        self._check_X_for_type(X)\n        transformed_df = X.select_dtypes(include=self.include, exclude=self.exclude)\n\n        if set(list(transformed_df)) != set(self.type_columns_):\n            raise ValueError('Columns were not equal during fit and transform')\n\n        return transformed_df\n\n    @staticmethod\n    def _check_X_for_type(X):\n        \"\"\"Checks if input of the Selector is of the required dtype\"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise TypeError(\"Provided variable X is not of type pandas.DataFrame\")\n</code></pre> <p>There's a few good practices we observe here that we'd appreciate seeing in pull requests. We want to re-use features from sklearn as much as possible.</p> <p>In particular, for this example:</p> <ol> <li>We inherit from the mixins found in sklearn.</li> <li>We use the validation utils from sklearn in our object to confirm if the model is fitted, if the array going into     the model is of the correct type and if the random state is appropriate.</li> </ol> <p>Feel free to look at example implementations before writing your own from scratch.</p>"},{"location":"contribution/#unit-tests","title":"Unit Tests","text":"<p>We write unit tests on these objects to make sure that they will work in a Pipeline.</p> <p>This must be guaranteed. To facilitate this we have some standard tests that will check things like \"do we change the shape of the input?\".</p> <p>If your transformer belongs here: feel free to add it.</p>"},{"location":"installation/","title":"Installation","text":"<p>Warning</p> <p>This project is experimental and is in alpha. We do our best to keep things stable but you should assume that if you do not specify a version number that certain functionality can break.</p> <p>Install scikit-lego:</p> pipcondasource/gitlocal clone <pre><code>python -m pip install scikit-lego\n</code></pre> <pre><code>conda install -c conda-forge scikit-lego\n</code></pre> <pre><code>python -m pip install git+https://github.com/koaning/scikit-lego.git\n</code></pre> <pre><code>git clone https://github.com/koaning/scikit-lego.git\ncd scikit-lego\npython -m pip install .\n</code></pre>"},{"location":"installation/#dependency-installs","title":"Dependency installs","text":"<p>Some functionality can only be used if certain dependencies are installed. This can be done by specifying the extra dependencies in square brackets after the package name.</p> <p>Currently supported extras are cvxpy, formulaic, patsy and umap. You can specify these as follows:</p> piplocal clone <pre><code>python -m pip install scikit-lego\"[cvxpy]\"\npython -m pip install scikit-lego\"[formulaic]\"\npython -m pip install scikit-lego\"[patsy]\"\npython -m pip install scikit-lego\"[umap]\"\npython -m pip install scikit-lego\"[all]\"\n</code></pre> <pre><code>git clone https://github.com/koaning/scikit-lego.git\ncd scikit-lego\n\npython -m pip install \".[cvxpy]\"\npython -m pip install .\"[formulaic]\"\npython -m pip install .\"[patsy]\"\npython -m pip install .\"[umap]\"\npython -m pip install \".[all]\"\n</code></pre>"},{"location":"rstudio/","title":"Lego in Rstudio","text":"<p>Thanks to reticulate you can also use this package from R. We couldn't find any good documentation on how to build a proper scikit-learn gridsearch using reticulate so we figured we might add a resource to our documentation here.</p> <p>It should be said that we feel that the best developer experience is definitely going to be in python but we figured it be helpful to put a small example in our documentation.</p>"},{"location":"rstudio/#demo","title":"Demo","text":"<p>You'll first need to install a dependency and set up a link to a python virtualenv that has scikit-lego already installed.</p> <pre><code>install.packages(\"reticulate\")\n\n# optionally you can install miniconda\n# reticulate::install_miniconda()\n\nlibrary(reticulate)\nlibrary(tidyverse)\n\n# again optionally if you're using miniconda\n# use_condaenv(\"r-reticulate\")\npy_discover_config()\n</code></pre> <p>You can read more details about this on the reticulate docs on installation as well as their docs on package management.</p> <p>From here you can start importing the python dependencies from R.</p> <pre><code>sklearn &lt;- import(\"sklearn\")\nsklego &lt;- import(\"sklego\")\n\ninfo_filter &lt;- sklego$preprocessing$InformationFilter\nthresholder &lt;- sklego$meta$Thresholder\npp_score    &lt;- sklego$metrics$p_percent_score\n\nlr          &lt;- sklearn$linear_model$LogisticRegression\npipeline    &lt;- sklearn$pipeline$Pipeline\ngrid        &lt;- sklearn$model_selection$GridSearchCV\nmake_scorer &lt;- sklearn$metrics$make_scorer\naccuracy    &lt;- sklearn$metrics$accuracy_score\n</code></pre> <p>We can also setup a scikit-learn pipeline:</p> <pre><code>pipe &lt;- pipeline(\n  c(\n    tuple(\"filter\", info_filter(columns = c(\"colour\"), alpha=0.9)),\n    tuple(\"model\", thresholder(model = lr(), threshold = 0.5))\n  )\n)\n</code></pre> <p>Note that this pipeline contains two specific tools from the lego library:</p> <ul> <li>the information filter</li> <li>the thresholder meta model</li> </ul> <p>We can even load in some datasets:</p> <pre><code>df &lt;- sklego$datasets$load_arrests(give_pandas = TRUE)\n\nX &lt;- df %&gt;% select(year, age, colour)\nX['colour'] &lt;- as.numeric(X['colour'] == \"Black\")\ny &lt;- as.numeric(df$checks &gt; 1)\n</code></pre> <p>In this case we're taking a subset of the <code>load_arrests</code> dataset. This information contains arrests data and we're going to perform gridsearch keeping fairness in mind.</p> <pre><code>cv &lt;- grid(\n  estimator = pipe,\n  param_grid = list(\"filter__alpha\"=seq(0.1, 0.9, 0.1),\n                    \"model__threshold\"=seq(0.1, 0.9, 0.1)),\n  cv=as.integer(3),\n  scoring = list(accuracy=make_scorer(accuracy),\n                 pp_score=pp_score('colour')),\n  refit=\"accuracy\")\n\ncv$fit(X, y)\n</code></pre> <p>You'll note that we're using <code>list</code> and <code>as.integer</code> here. For details on why, check out this documentation page.</p> <p>We'll need to do some parsing of the <code>cv_results_</code> to properly get these into a tidyverse dataframe.</p> <pre><code>cv_df &lt;- cv$cv_results_ %&gt;%\n  as_tibble() %&gt;%\n  select(param_filter__alpha, param_model__threshold, mean_test_pp_score, mean_test_accuracy) %&gt;%\n  mutate(param_filter__alpha = unlist(param_filter__alpha),\n         param_model__threshold = unlist(param_model__threshold))\n</code></pre> <p>But from here we can do some plotting.</p> <pre><code>ggplot(data=cv_df) +\n  geom_line(aes(param_model__threshold, mean_test_accuracy,\n                group=param_filter__alpha, color=param_filter__alpha)) +\n  ggtitle(\"Effect of threshold on accuracy\",\n          subtitle=\"Keeping it at 0.5 is best for accuracy, note the effect of alpha!\") +\n  theme(legend.position=\"bottom\")\n</code></pre> <p> </p> <pre><code>ggplot(data=cv_df) +\n  geom_line(aes(param_model__threshold, mean_test_pp_score,\n                group=param_filter__alpha, color=param_filter__alpha)) +\n  ggtitle(\"Effect of threshold on fairness parameter\",\n        subtitle=\"For fairness we want to maybe not use 0.5\") +\n  theme(legend.position=\"bottom\")\n</code></pre> <p> </p>"},{"location":"rstudio/#important","title":"Important","text":"<p>Note that we're mainly trying to demonstrate the R api here. In terms of fairness you would want to explore the dataset further before you say anything conclusive.</p> <p>Also, it may be simpler and more preferential to use the python engine inside of R-markdown instead of translating R-datastructures to python ones manually.</p> <p>But you can certainly combine the tools from scikit-lego with your tools in R.</p>"},{"location":"this/","title":"Import This","text":"<p>In Python there's a poem that you can read by importing the <code>this</code> module.</p> <pre><code>import this\n</code></pre> <p>It has wonderful lessons that the authors of the language learned while designing the python language.</p> <p>In the same tradition we've done the same thing. Folks who have made significant contributions have also been asked to contribute to the poem.</p> <p>You can read it via:</p> <pre><code>from sklego import this\n</code></pre> <pre><code>Roses are red, violets are blue,\nnaming your package is really hard to undo.\nHaste can make decisions in one fell swoop,\nnote that LEGO\u00ae is a trademark of the LEGO Group.\nIt really makes sense, we do not need to bluff,\nLEGO does not sponsor, authorize or endorse any of this stuff.\n\nLook at all the features and look at all the extensions,\nthe path towards ruin is paved with good intentions.\nBe careful with features as they tend to go sour,\ndefer responsibility to the end user, this might just give them power.\nIf you don't know the requirements you don't know if they're met.\nIf you haven't gotten to where you're going, you aren't there yet.\n\nInfinity is ever present, the unknown may be ignored,\nnot everything needs to be built, not everything needs to be explored.\nChange everything and you'll soon be a jerk,\nyou may invent a new tool, not a way to work.\nSome problems cannot be solved in a single day,\nbut if you ignore them, they sometimes go away.\n\nThere's a lot of power in simplicity,\nit keeps you approach strong,\nif you understand the solution better than the problem,\nyou're doing it wrong.\n</code></pre>"},{"location":"_static/cross-validation/grp-summary/","title":"Grp summary","text":"index observations group obs_per_group ideal_group_size diff_from_ideal_group_size 2000 3 0 4 4 0 2001 1 0 4 4 0 2002 2 1 3 4 -1 2003 1 1 3 4 -1 2004 5 2 5 4 1 2005 2 3 5 4 1 2006 2 3 5 4 1 2007 1 3 5 4 1"},{"location":"_static/cross-validation/grp-ts/","title":"Grp ts","text":"X y 583 481 414 617 669 627 812 604 800 248 966 503 719 650 476 939 743 170 142 893"},{"location":"_static/cross-validation/summary/","title":"Summary","text":"Start date End date Period Unique days nbr samples 2018-01-01 00:00:00 2018-01-10 00:00:00 9 days 00:00:00 10 10 2018-01-12 00:00:00 2018-01-13 00:00:00 1 days 00:00:00 2 2 2018-01-06 00:00:00 2018-01-15 00:00:00 9 days 00:00:00 10 10 2018-01-17 00:00:00 2018-01-18 00:00:00 1 days 00:00:00 2 2 2018-01-10 00:00:00 2018-01-19 00:00:00 9 days 00:00:00 10 10 2018-01-21 00:00:00 2018-01-22 00:00:00 1 days 00:00:00 2 2 2018-01-15 00:00:00 2018-01-24 00:00:00 9 days 00:00:00 10 10 2018-01-26 00:00:00 2018-01-27 00:00:00 1 days 00:00:00 2 2"},{"location":"_static/cross-validation/ts/","title":"Ts","text":"A B C y date 28 9 24 5 2018-01-30 00:00:00 5 0 19 1 2018-01-29 00:00:00 8 1 29 2 2018-01-28 00:00:00 11 4 21 19 2018-01-27 00:00:00 19 26 6 2 2018-01-26 00:00:00"},{"location":"_static/datasets/abalone/","title":"Abalone","text":"sex length diameter height whole_weight shucked_weight viscera_weight shell_weight rings M 0.455 0.365 0.095 0.514 0.2245 0.101 0.15 15 M 0.35 0.265 0.09 0.2255 0.0995 0.0485 0.07 7 F 0.53 0.42 0.135 0.677 0.2565 0.1415 0.21 9 M 0.44 0.365 0.125 0.516 0.2155 0.114 0.155 10 I 0.33 0.255 0.08 0.205 0.0895 0.0395 0.055 7"},{"location":"_static/datasets/arrests/","title":"Arrests","text":"released colour year age sex employed citizen checks Yes White 2002 21 Male Yes Yes 3 No Black 1999 17 Male Yes Yes 3 Yes White 2000 24 Male Yes Yes 3 No Black 2000 46 Male Yes Yes 1 Yes Black 1999 27 Female Yes Yes 1"},{"location":"_static/datasets/chicken/","title":"Chicken","text":"weight time chick diet 42 0 1 1 51 2 1 1 59 4 1 1 64 6 1 1 76 8 1 1"},{"location":"_static/datasets/creditcards/","title":"Creditcards","text":"V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 Amount Class -1.35981 -0.0727812 2.53635 1.37816 -0.338321 0.462388 0.239599 0.0986979 0.363787 0.0907942 -0.5516 -0.617801 -0.99139 -0.311169 1.46818 -0.470401 0.207971 0.0257906 0.403993 0.251412 -0.0183068 0.277838 -0.110474 0.0669281 0.128539 -0.189115 0.133558 -0.0210531 149.62 0 1.19186 0.266151 0.16648 0.448154 0.0600176 -0.0823608 -0.078803 0.0851017 -0.255425 -0.166974 1.61273 1.06524 0.489095 -0.143772 0.635558 0.463917 -0.114805 -0.183361 -0.145783 -0.0690831 -0.225775 -0.638672 0.101288 -0.339846 0.16717 0.125895 -0.0089831 0.0147242 2.69 0 -1.35835 -1.34016 1.77321 0.37978 -0.503198 1.8005 0.791461 0.247676 -1.51465 0.207643 0.624501 0.0660837 0.717293 -0.165946 2.34586 -2.89008 1.10997 -0.121359 -2.26186 0.52498 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.0553528 -0.0597518 378.66 0 -0.966272 -0.185226 1.79299 -0.863291 -0.0103089 1.2472 0.237609 0.377436 -1.38702 -0.0549519 -0.226487 0.178228 0.507757 -0.287924 -0.631418 -1.05965 -0.684093 1.96578 -1.23262 -0.208038 -0.1083 0.0052736 -0.190321 -1.17558 0.647376 -0.221929 0.0627228 0.0614576 123.5 0 -1.15823 0.877737 1.54872 0.403034 -0.407193 0.0959215 0.592941 -0.270533 0.817739 0.753074 -0.822843 0.538196 1.34585 -1.11967 0.175121 -0.451449 -0.237033 -0.0381948 0.803487 0.408542 -0.0094307 0.798278 -0.137458 0.141267 -0.20601 0.502292 0.219422 0.215153 69.99 0"},{"location":"_static/datasets/hearts/","title":"Hearts","text":"age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target 63 1 1 145 233 1 2 150 0 2.3 3 0 fixed 0 67 1 4 160 286 0 2 108 1 1.5 2 3 normal 1 67 1 4 120 229 0 2 129 1 2.6 2 2 reversible 0 37 1 3 130 250 0 0 187 0 3.5 3 0 normal 0 41 0 2 130 204 0 2 172 0 1.4 1 0 normal 0"},{"location":"_static/datasets/heroes/","title":"Heroes","text":"name attack_type role health attack attack_spd Artanis Melee Bruiser 2470 111 1 Chen Melee Bruiser 2473 90 1.11 Dehaka Melee Bruiser 2434 100 1.11 Imperius Melee Bruiser 2450 122 0.83 Leoric Melee Bruiser 2550 109 0.77"},{"location":"_static/datasets/penguins/","title":"Penguins","text":"species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex Adelie Torgersen 39.1 18.7 181 3750 male Adelie Torgersen 39.5 17.4 186 3800 female Adelie Torgersen 40.3 18 195 3250 female Adelie Torgersen nan nan nan nan nan Adelie Torgersen 36.7 19.3 193 3450 female"},{"location":"_static/datasets/timeseries/","title":"Timeseries","text":"yt 0 -0.335058 1 -0.283375 2 0.521791 3 0.50202 4 0.310048"},{"location":"_static/fairness/information-filter-coefs/","title":"Information filter coefs","text":"crim zn indus chas nox rm age dis rad tax ptratio b lstat -0.928146 1.08157 0.1409 0.68174 -2.05672 2.67423 0.0194661 -3.10404 2.66222 -2.07678 -2.06061 0.849268 -3.74363 -1.5814 0.911004 -0.290074 0.884936 -2.56787 4.2647 -1.27073 -3.33184 2.21574 -2.05625 -2.1546 nan nan -0.763568 1.02805 0.0613932 0.697504 -1.60546 6.84677 -0.0579197 -2.5376 1.93506 -1.77983 -2.79307 nan nan"},{"location":"_static/meta-models/ordinal_data/","title":"Ordinal data","text":"apply pared public gpa apply_codes very likely 0 0 3.26 2 somewhat likely 1 0 3.21 1 unlikely 1 1 3.94 0 somewhat likely 0 0 2.81 1 somewhat likely 0 0 2.53 1"},{"location":"_static/meta-models/penguins/","title":"Penguins","text":"flipper_length_mm body_mass_g sex 181 3750 male 186 3800 female 195 3250 female 193 3450 female 190 3650 male"},{"location":"api/base/","title":"Base","text":""},{"location":"api/base/#sklego.base.ClustererMeta","title":"<code>sklego.base.ClustererMeta</code>","text":"<p>             Bases: <code>type</code></p> <p>Metaclass for <code>Clusterer</code>.</p> <p>This metaclass is responsible for checking whether a class can be considered a <code>Clusterer</code>. A class is considered a <code>Clusterer</code> if it has a \"fit_predict\" method.</p> Source code in <code>sklego/base.py</code> <pre><code>class ClustererMeta(type):\n    \"\"\"Metaclass for `Clusterer`.\n\n    This metaclass is responsible for checking whether a class can be considered a `Clusterer`.\n    A class is considered a `Clusterer` if it has a \"fit_predict\" method.\n    \"\"\"\n\n    def __instancecheck__(self, other):\n        \"\"\"Checks if the provided object is a `Clusterer`.\n\n        Parameters\n        ----------\n        self : ClustererMeta\n            `ClustererMeta` class.\n        other : object\n            The object to check for `Clusterer` compatibility.\n\n        Returns\n        -------\n        bool\n            True if the object is a `Clusterer` (has a \"fit_predict\" method ), False otherwise.\n        \"\"\"\n        return hasattr(other, \"fit_predict\")\n</code></pre>"},{"location":"api/base/#sklego.base.Clusterer","title":"<code>sklego.base.Clusterer</code>","text":"<p>Base class for <code>Clusterer</code>.</p> <p>This base class defines the <code>Clusterer</code> interface, indicating that subclasses should have a \"fit_predict\" method.</p> Source code in <code>sklego/base.py</code> <pre><code>class Clusterer(metaclass=ClustererMeta):\n    \"\"\"Base class for `Clusterer`.\n\n    This base class defines the `Clusterer` interface, indicating that subclasses should have a \"fit_predict\" method.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/base/#sklego.base.OutlierModelMeta","title":"<code>sklego.base.OutlierModelMeta</code>","text":"<p>             Bases: <code>type</code></p> <p>Metaclass for <code>OutlierModel</code>.</p> <p>This metaclass is responsible for checking whether a class can be considered an <code>OutlierModel</code>. A class is considered an <code>OutlierModel</code> if it is an instance of the <code>sklearn.base.OutlierMixin</code> class.</p> Source code in <code>sklego/base.py</code> <pre><code>class OutlierModelMeta(type):\n    \"\"\"Metaclass for `OutlierModel`.\n\n    This metaclass is responsible for checking whether a class can be considered an `OutlierModel`.\n    A class is considered an `OutlierModel` if it is an instance of the `sklearn.base.OutlierMixin` class.\n    \"\"\"\n\n    def __instancecheck__(self, other):\n        \"\"\"\n        Check if the provided object is an `OutlierModel`.\n\n        Parameters\n        ----------\n        self : OutlierModelMeta\n            The `OutlierModelMeta` class.\n        other : object\n            The object to check for `OutlierModel` compatibility.\n\n        Returns\n        -------\n        bool\n            True if the object is an `OutlierModel` (an instance of \"OutlierMixin\"), False otherwise.\n        \"\"\"\n        return isinstance(other, OutlierMixin)\n</code></pre>"},{"location":"api/base/#sklego.base.OutlierModel","title":"<code>sklego.base.OutlierModel</code>","text":"<p>Base class for <code>OutlierModel</code>.</p> <p>This base class defines the <code>OutlierModel</code> interface, indicating that subclasses should be instances of the \"OutlierMixin\" class.</p> Source code in <code>sklego/base.py</code> <pre><code>class OutlierModel(metaclass=OutlierModelMeta):\n    \"\"\"Base class for `OutlierModel`.\n\n    This base class defines the `OutlierModel` interface, indicating that subclasses should be instances of the\n    \"OutlierMixin\" class.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/base/#sklego.base.ProbabilisticClassifierMeta","title":"<code>sklego.base.ProbabilisticClassifierMeta</code>","text":"<p>             Bases: <code>type</code></p> <p>Metaclass for <code>ProbabilisticClassifier</code>.</p> <p>This metaclass is responsible for checking whether a class can be considered a <code>ProbabilisticClassifier</code>. A class is considered a <code>ProbabilisticClassifier</code> if it has a \"predict_proba\" method.</p> Source code in <code>sklego/base.py</code> <pre><code>class ProbabilisticClassifierMeta(type):\n    \"\"\"Metaclass for `ProbabilisticClassifier`.\n\n    This metaclass is responsible for checking whether a class can be considered a `ProbabilisticClassifier`.\n    A class is considered a `ProbabilisticClassifier` if it has a \"predict_proba\" method.\n    \"\"\"\n\n    def __instancecheck__(self, other):\n        \"\"\"Checks if the provided object is a `ProbabilisticClassifier`.\n\n        Parameters\n        ----------\n        self : ProbabilisticClassifierMeta\n            `ProbabilisticClassifierMeta` class.\n        other : object\n            The object to check for `ProbabilisticClassifier` compatibility.\n\n        Returns\n        -------\n        bool\n            True if the object is a `ProbabilisticClassifier` (has a \"predict_proba\" method ), False otherwise.\n        \"\"\"\n        return hasattr(other, \"predict_proba\")\n</code></pre>"},{"location":"api/base/#sklego.base.ProbabilisticClassifier","title":"<code>sklego.base.ProbabilisticClassifier</code>","text":"<p>Base class for <code>ProbabilisticClassifier</code>.</p> <p>This base class defines the <code>ProbabilisticClassifier</code> interface, indicating that subclasses should have a \"predict_proba\" method.</p> Source code in <code>sklego/base.py</code> <pre><code>class ProbabilisticClassifier(metaclass=ProbabilisticClassifierMeta):\n    \"\"\"Base class for `ProbabilisticClassifier`.\n\n    This base class defines the `ProbabilisticClassifier` interface, indicating that subclasses should have a\n    \"predict_proba\" method.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/common/","title":"Common","text":"<p>Module with common classes and functions used across the package.</p>"},{"location":"api/common/#sklego.common.TrainOnlyTransformerMixin","title":"<code>sklego.common.TrainOnlyTransformerMixin</code>","text":"<p>             Bases: <code>TransformerMixin</code></p> <p>Mixin class for transformers that can handle training and test data differently.</p> <p>This mixin allows using a separate function for transforming training and test data.</p> <p>Warning</p> <p>Transformers using this class as a mixin should:</p> <ul> <li>Call <code>super().fit</code> in their fit method.</li> <li>Implement <code>transform_train()</code> method.</li> </ul> <p>They may also implement <code>transform_test()</code> method, if not implemented, <code>transform_test()</code> will simply return the untransformed data.</p> <p>Attributes:</p> Name Type Description <code>X_hash_</code> <code>hash</code> <p>The hash of the training data - used to determine whether to use <code>transform_train</code> or <code>transform_test</code>.</p> <code>n_features_in_</code> <code>int</code> <p>The number of features seen during <code>.fit()</code> in the training data.</p> <code>dim_</code> <code>int</code> <p>Deprecated, use <code>n_features_in_</code> instead.</p> <p>Examples:</p> <pre><code>from sklearn.base import BaseEstimator\nfrom sklego.common import TrainOnlyTransformerMixin\n\nclass TrainOnlyTransformer(TrainOnlyTransformerMixin, BaseEstimator):\n    def fit(self, X, y):\n        super().fit(X, y)\n\n    def transform_train(self, X, y=None):\n        '''Add random noise to the training data.'''\n        return X + np.random.normal(0, 1, size=X.shape)\n\nX_train, X_test = np.random.randn(100, 4), np.random.randn(100, 4)\ny_train, y_test = np.random.randn(100), np.random.randn(100)\n\ntrf = TrainOnlyTransformer()\ntrf.fit(X_train, y_train)\n\nassert np.all(trf.transform(X_train) != X_train)\nassert np.all(trf.transform(X_test) == X_test)\n</code></pre> Source code in <code>sklego/common.py</code> <pre><code>class TrainOnlyTransformerMixin(TransformerMixin):\n    \"\"\"Mixin class for transformers that can handle training and test data differently.\n\n    This mixin allows using a separate function for transforming training and test data.\n\n    !!! warning\n\n        Transformers using this class as a mixin should:\n\n        - Call `super().fit` in their fit method.\n        - Implement `transform_train()` method.\n\n        They may also implement `transform_test()` method, if not implemented, `transform_test()` will simply return\n        the untransformed data.\n\n    Attributes\n    ----------\n    X_hash_ : hash\n        The hash of the training data - used to determine whether to use `transform_train` or `transform_test`.\n    n_features_in_ : int\n        The number of features seen during `.fit()` in the training data.\n    dim_ : int\n        Deprecated, use `n_features_in_` instead.\n\n    Examples\n    --------\n    ```py\n    from sklearn.base import BaseEstimator\n    from sklego.common import TrainOnlyTransformerMixin\n\n    class TrainOnlyTransformer(TrainOnlyTransformerMixin, BaseEstimator):\n        def fit(self, X, y):\n            super().fit(X, y)\n\n        def transform_train(self, X, y=None):\n            '''Add random noise to the training data.'''\n            return X + np.random.normal(0, 1, size=X.shape)\n\n    X_train, X_test = np.random.randn(100, 4), np.random.randn(100, 4)\n    y_train, y_test = np.random.randn(100), np.random.randn(100)\n\n    trf = TrainOnlyTransformer()\n    trf.fit(X_train, y_train)\n\n    assert np.all(trf.transform(X_train) != X_train)\n    assert np.all(trf.transform(X_test) == X_test)\n    ```\n    \"\"\"\n\n    _HASHERS = {\n        pd.DataFrame: lambda X: hashlib.sha256(pd.util.hash_pandas_object(X, index=True).values).hexdigest(),\n        np.ndarray: lambda X: hash(X.data.tobytes()),\n        np.memmap: lambda X: hash(X.data.tobytes()),\n    }\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the mixin by calculating the hash of `X` and stores it in `self.X_hash_`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,) | None, default=None\n            The target values.\n\n        Returns\n        -------\n        self : TrainOnlyTransformerMixin\n            The fitted transformer.\n        \"\"\"\n        if y is None:\n            check_array(X, estimator=self)\n        else:\n            check_X_y(X, y, estimator=self)\n        self.X_hash_ = self._hash(X)\n        self.n_features_in_ = X.shape[1]\n        return self\n\n    @staticmethod\n    def _hash(X):\n        \"\"\"Calculate a hash of X based on its type. Hashers are defined in TrainOnlyMixin._HASHERS.\n\n        Supported types are:\n\n            - pd.DataFrame\n            - np.ndarray\n            - np.memmap\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to hash.\n\n        Returns\n        -------\n        hash\n            The calculated hash value.\n\n        Raises\n        ------\n        ValueError\n            If the type of `X` is not supported.\n        \"\"\"\n        try:\n            hasher = TrainOnlyTransformerMixin._HASHERS[type(X)]\n        except KeyError:\n            raise ValueError(\n                f\"Unknown datatype {type(X)}, \"\n                f\"`TrainOnlyTransformerMixin` only supports: {set(TrainOnlyTransformerMixin._HASHERS.keys())}\"\n            )\n        else:\n            return hasher(X)\n\n    def transform(self, X, y=None):\n        \"\"\"Dispatch to `transform_train()` or `transform_test()` based on the data passed.\n\n        This method will check whether the hash of `X` matches the hash of the training data. If it does, it will\n        dispatch to `transform_train()`, otherwise it will dispatch to `transform_test()`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to transform.\n        y : array-like of shape (n_samples,) or None, default=None.\n            The target values.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_features)\n            The transformed data.\n\n        Raises\n        ------\n        ValueError\n            If the input dimension does not match the training dimension.\n        \"\"\"\n        check_is_fitted(self, [\"X_hash_\", \"n_features_in_\"])\n        check_array(X, estimator=self)\n\n        if X.shape[1] != self.n_features_in_:\n            raise ValueError(f\"Unexpected input dimension {X.shape[1]}, expected {self.n_features_in_}\")\n\n        if self._hash(X) == self.X_hash_:\n            return self.transform_train(X)\n        else:\n            return self.transform_test(X)\n\n    def transform_train(self, X, y=None):\n        \"\"\"Transform the training data.\n\n        This method should be implemented in subclasses to specify how training data should be transformed.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,) or None, default=None\n            The target values.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_features)\n            The transformed training data.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses of `TrainOnlyTransformerMixin` should implement `transform_train` method\")\n\n    def transform_test(self, X, y=None):\n        \"\"\"Transform the test data.\n\n        This method can be implemented in subclasses to specify how test data should be transformed.\n        If not implemented, it will return the untransformed data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The test data.\n        y : array-like of shape (n_samples,) or None, default=None\n            The target values.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_features)\n            The transformed test data or untransformed data if not implemented.\n        \"\"\"\n        return X\n\n    @property\n    def dim_(self):\n        warn(\n            \"Please use `n_features_in_` instead of `dim_`, `dim_` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self.n_features_in_\n</code></pre>"},{"location":"api/common/#sklego.common.TrainOnlyTransformerMixin.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the mixin by calculating the hash of <code>X</code> and stores it in <code>self.X_hash_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,) | None</code> <p>The target values.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>TrainOnlyTransformerMixin</code> <p>The fitted transformer.</p> Source code in <code>sklego/common.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the mixin by calculating the hash of `X` and stores it in `self.X_hash_`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,) | None, default=None\n        The target values.\n\n    Returns\n    -------\n    self : TrainOnlyTransformerMixin\n        The fitted transformer.\n    \"\"\"\n    if y is None:\n        check_array(X, estimator=self)\n    else:\n        check_X_y(X, y, estimator=self)\n    self.X_hash_ = self._hash(X)\n    self.n_features_in_ = X.shape[1]\n    return self\n</code></pre>"},{"location":"api/common/#sklego.common.TrainOnlyTransformerMixin.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Dispatch to <code>transform_train()</code> or <code>transform_test()</code> based on the data passed.</p> <p>This method will check whether the hash of <code>X</code> matches the hash of the training data. If it does, it will dispatch to <code>transform_train()</code>, otherwise it will dispatch to <code>transform_test()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to transform.</p> required <code>y</code> <code>array-like of shape (n_samples,) or None</code> <p>The target values.</p> <code>None.</code> <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_features)</code> <p>The transformed data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input dimension does not match the training dimension.</p> Source code in <code>sklego/common.py</code> <pre><code>def transform(self, X, y=None):\n    \"\"\"Dispatch to `transform_train()` or `transform_test()` based on the data passed.\n\n    This method will check whether the hash of `X` matches the hash of the training data. If it does, it will\n    dispatch to `transform_train()`, otherwise it will dispatch to `transform_test()`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to transform.\n    y : array-like of shape (n_samples,) or None, default=None.\n        The target values.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_features)\n        The transformed data.\n\n    Raises\n    ------\n    ValueError\n        If the input dimension does not match the training dimension.\n    \"\"\"\n    check_is_fitted(self, [\"X_hash_\", \"n_features_in_\"])\n    check_array(X, estimator=self)\n\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(f\"Unexpected input dimension {X.shape[1]}, expected {self.n_features_in_}\")\n\n    if self._hash(X) == self.X_hash_:\n        return self.transform_train(X)\n    else:\n        return self.transform_test(X)\n</code></pre>"},{"location":"api/common/#sklego.common.TrainOnlyTransformerMixin.transform_test","title":"<code>transform_test(X, y=None)</code>","text":"<p>Transform the test data.</p> <p>This method can be implemented in subclasses to specify how test data should be transformed. If not implemented, it will return the untransformed data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The test data.</p> required <code>y</code> <code>array-like of shape (n_samples,) or None</code> <p>The target values.</p> <code>None</code> <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_features)</code> <p>The transformed test data or untransformed data if not implemented.</p> Source code in <code>sklego/common.py</code> <pre><code>def transform_test(self, X, y=None):\n    \"\"\"Transform the test data.\n\n    This method can be implemented in subclasses to specify how test data should be transformed.\n    If not implemented, it will return the untransformed data.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The test data.\n    y : array-like of shape (n_samples,) or None, default=None\n        The target values.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_features)\n        The transformed test data or untransformed data if not implemented.\n    \"\"\"\n    return X\n</code></pre>"},{"location":"api/common/#sklego.common.TrainOnlyTransformerMixin.transform_train","title":"<code>transform_train(X, y=None)</code>","text":"<p>Transform the training data.</p> <p>This method should be implemented in subclasses to specify how training data should be transformed.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,) or None</code> <p>The target values.</p> <code>None</code> <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_features)</code> <p>The transformed training data.</p> Source code in <code>sklego/common.py</code> <pre><code>def transform_train(self, X, y=None):\n    \"\"\"Transform the training data.\n\n    This method should be implemented in subclasses to specify how training data should be transformed.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,) or None, default=None\n        The target values.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_features)\n        The transformed training data.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses of `TrainOnlyTransformerMixin` should implement `transform_train` method\")\n</code></pre>"},{"location":"api/common/#sklego.common.as_list","title":"<code>sklego.common.as_list(val)</code>","text":"<p>Ensure the input value is converted into a list.</p> <p>This helper function takes an input value and ensures that it is always returned as a list.</p> <ul> <li>If the input is a single value, it will be wrapped in a list.</li> <li>If the input is an iterable, it will be converted into a list.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>object</code> <p>The input value that needs to be converted into a list.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The input value as a list.</p> <p>Examples:</p> <pre><code>as_list(\"test\")\n# ['test']\n\nas_list([\"test1\", \"test2\"])\n# ['test1', 'test2']\n</code></pre> Source code in <code>sklego/common.py</code> <pre><code>def as_list(val):\n    \"\"\"Ensure the input value is converted into a list.\n\n    This helper function takes an input value and ensures that it is always returned as a list.\n\n    - If the input is a single value, it will be wrapped in a list.\n    - If the input is an iterable, it will be converted into a list.\n\n    Parameters\n    ----------\n    val : object\n        The input value that needs to be converted into a list.\n\n    Returns\n    -------\n    list\n        The input value as a list.\n\n    Examples\n    --------\n    ```py\n    as_list(\"test\")\n    # ['test']\n\n    as_list([\"test1\", \"test2\"])\n    # ['test1', 'test2']\n    ```\n    \"\"\"\n    treat_single_value = str\n\n    if isinstance(val, treat_single_value):\n        return [val]\n\n    if hasattr(val, \"__iter__\"):\n        return list(val)\n\n    return [val]\n</code></pre>"},{"location":"api/common/#sklego.common.flatten","title":"<code>sklego.common.flatten(nested_iterable)</code>","text":"<p>Recursively flatten an arbitrarily nested iterable into an iterator of values.</p> <p>This helper function takes an arbitrarily nested iterable and returns an iterator of flattened values. It recursively processes the input to extract individual elements and yield them in a flat structure.</p> <p>Parameters:</p> Name Type Description Default <code>nested_iterable</code> <code>Iterable</code> <p>An arbitrarily nested iterable to be flattened.</p> required <p>Yields:</p> Type Description <code>Generator</code> <p>A generator of flattened values from the nested iterable.</p> <p>Examples:</p> <pre><code>list(flatten([[\"test1\", \"test2\"], [\"a\", \"b\", [\"c\", \"d\"]]))\n# ['test1', 'test2', 'a', 'b', 'c', 'd']\n\nlist(flatten([\"test1\", [\"test2\"]])\n# ['test1', 'test2']\n</code></pre> Source code in <code>sklego/common.py</code> <pre><code>def flatten(nested_iterable):\n    \"\"\"Recursively flatten an arbitrarily nested iterable into an iterator of values.\n\n    This helper function takes an arbitrarily nested iterable and returns an iterator of flattened values.\n    It recursively processes the input to extract individual elements and yield them in a flat structure.\n\n    Parameters\n    ----------\n    nested_iterable : Iterable\n        An arbitrarily nested iterable to be flattened.\n\n    Yields\n    ------\n    Generator\n        A generator of flattened values from the nested iterable.\n\n    Examples\n    --------\n    ```py\n    list(flatten([[\"test1\", \"test2\"], [\"a\", \"b\", [\"c\", \"d\"]]))\n    # ['test1', 'test2', 'a', 'b', 'c', 'd']\n\n    list(flatten([\"test1\", [\"test2\"]])\n    # ['test1', 'test2']\n    ```\n    \"\"\"\n    for el in nested_iterable:\n        if isinstance(el, collections.abc.Iterable) and not isinstance(el, (str, bytes)):\n            yield from flatten(el)\n        else:\n            yield el\n</code></pre>"},{"location":"api/common/#sklego.common.expanding_list","title":"<code>sklego.common.expanding_list(list_to_extent, return_type=list)</code>","text":"<p>Create an expanding list of lists or tuples by making combinations of elements.</p> <p>This function takes an input list and creates an expanding list, where each element is a list or tuple containing a subset of elements from the input list. The resulting list can be composed of lists or tuples, depending on the specified <code>return_type</code>.</p> <p>Parameters:</p> Name Type Description Default <code>list_to_extent</code> <code>object</code> <p>The input to be extended.</p> required <code>return_type</code> <code>type</code> <p>The type of elements in the resulting list (list or tuple).</p> <code>list</code> <p>Returns:</p> Type Description <code>list</code> <p>An expanding list of <code>list</code>s or <code>tuple</code>s containing combinations of elements from the input.</p> <p>Examples:</p> <pre><code>expanding_list(\"test\")\n# [['test']]\n\nexpanding_list([\"test1\", \"test2\", \"test3\"])\n# [['test1'], ['test1', 'test2'], ['test1', 'test2', 'test3']]\n\nexpanding_list([\"test1\", \"test2\", \"test3\"], tuple)\n# [('test1',), ('test1', 'test2'), ('test1', 'test2', 'test3')]\n</code></pre> Source code in <code>sklego/common.py</code> <pre><code>def expanding_list(list_to_extent, return_type=list):\n    \"\"\"Create an expanding list of lists or tuples by making combinations of elements.\n\n    This function takes an input list and creates an expanding list, where each element is a list or tuple containing a\n    subset of elements from the input list. The resulting list can be composed of lists or tuples, depending on the\n    specified `return_type`.\n\n    Parameters\n    ----------\n    list_to_extent : object\n        The input to be extended.\n    return_type : type, default=list\n        The type of elements in the resulting list (list or tuple).\n\n    Returns\n    -------\n    list\n        An expanding list of `list`s or `tuple`s containing combinations of elements from the input.\n\n    Examples\n    --------\n    ```py\n    expanding_list(\"test\")\n    # [['test']]\n\n    expanding_list([\"test1\", \"test2\", \"test3\"])\n    # [['test1'], ['test1', 'test2'], ['test1', 'test2', 'test3']]\n\n    expanding_list([\"test1\", \"test2\", \"test3\"], tuple)\n    # [('test1',), ('test1', 'test2'), ('test1', 'test2', 'test3')]\n    ```\n    \"\"\"\n    listed = as_list(list_to_extent)\n    return [return_type(listed[: n + 1]) for n in range(len(listed))]\n</code></pre>"},{"location":"api/common/#sklego.common.sliding_window","title":"<code>sklego.common.sliding_window(sequence, window_size, step_size)</code>","text":"<p>Generate sliding windows over a sequence.</p> <p>This function generates sliding windows of a specified size over a given sequence, where each window is a list of elements from the sequence.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>Iterable</code> <p>The input sequence (e.g., a list).</p> required <code>window_size</code> <code>int</code> <p>The size of each sliding window.</p> required <code>step_size</code> <code>int</code> <p>The amount of steps to the next window.</p> required <p>Returns:</p> Type Description <code>Generator</code> <p>A generator object that yields sliding windows.</p> <p>Examples:</p> <pre><code>list(sliding_window([1, 2, 4, 5], 2, 1))\n# [[1, 2], [2, 4], [4, 5], [5]]\n</code></pre> Source code in <code>sklego/common.py</code> <pre><code>def sliding_window(sequence, window_size, step_size):\n    \"\"\"Generate sliding windows over a sequence.\n\n    This function generates sliding windows of a specified size over a given sequence, where each window is a list of\n    elements from the sequence.\n\n    Parameters\n    ----------\n    sequence : Iterable\n        The input sequence (e.g., a list).\n    window_size : int\n        The size of each sliding window.\n    step_size : int\n        The amount of steps to the next window.\n\n    Returns\n    -------\n    Generator\n        A generator object that yields sliding windows.\n\n    Examples\n    --------\n    ```py\n    list(sliding_window([1, 2, 4, 5], 2, 1))\n    # [[1, 2], [2, 4], [4, 5], [5]]\n    ```\n    \"\"\"\n    return (sequence[pos : pos + window_size] for pos in range(0, len(sequence), step_size))\n</code></pre>"},{"location":"api/datasets/","title":"Datasets","text":""},{"location":"api/datasets/#sklego.datasets.load_abalone","title":"<code>sklego.datasets.load_abalone(return_X_y=False, as_frame=False)</code>","text":"<p>Loads the abalone dataset where the goal is to predict the gender of the creature.</p> <p>Parameters:</p> Name Type Description Default <code>return_X_y</code> <code>bool</code> <p>If True, returns <code>(data, target)</code> instead of a dict object.</p> <code>False</code> <code>as_frame</code> <code>bool</code> <p>If True, returns <code>data</code> as a pandas DataFrame  instead of X, y matrices.</p> <code>False</code> <p>Examples:</p> <pre><code>from sklego.datasets import load_abalone\nX, y = load_abalone(return_X_y=True)\n\nX.shape\n# (4177, 8)\ny.shape\n# (4177,)\n\nload_abalone(as_frame=True).columns\n# Index(['sex', 'length', 'diameter', 'height', 'whole_weight', 'shucked_weight',\n#        'viscera_weight', 'shell_weight', 'rings'],\n#       dtype='object')\n</code></pre> <p>The dataset was copied from Kaggle and can originally be found in the following sources:</p> <ul> <li> <p>Warwick J Nash, Tracy L Sellers, Simon R Talbot, Andrew J Cawthorn and Wes B Ford (1994)</p> <p>\"The Population Biology of Abalone (Haliotis species) in Tasmania.\"</p> <p>Sea Fisheries Division, Technical Report No. 48 (ISSN 1034-3288)</p> </li> </ul> Source code in <code>sklego/datasets.py</code> <pre><code>def load_abalone(return_X_y=False, as_frame=False):\n    \"\"\"\n    Loads the abalone dataset where the goal is to predict the gender of the creature.\n\n    Parameters\n    ----------\n    return_X_y : bool, default=False\n        If True, returns `(data, target)` instead of a dict object.\n    as_frame : bool, default=False\n        If True, returns `data` as a pandas DataFrame  instead of X, y matrices.\n\n    Examples\n    ---------\n    ```py\n    from sklego.datasets import load_abalone\n    X, y = load_abalone(return_X_y=True)\n\n    X.shape\n    # (4177, 8)\n    y.shape\n    # (4177,)\n\n    load_abalone(as_frame=True).columns\n    # Index(['sex', 'length', 'diameter', 'height', 'whole_weight', 'shucked_weight',\n    #        'viscera_weight', 'shell_weight', 'rings'],\n    #       dtype='object')\n    ```\n\n    The dataset was copied from Kaggle and can originally be found in the following sources:\n\n    - Warwick J Nash, Tracy L Sellers, Simon R Talbot, Andrew J Cawthorn and Wes B Ford (1994)\n\n        \"The Population Biology of Abalone (_Haliotis_ species) in Tasmania.\"\n\n        Sea Fisheries Division, Technical Report No. 48 (ISSN 1034-3288)\n    \"\"\"\n    filepath = resource_filename(\"sklego\", os.path.join(\"data\", \"abalone.zip\"))\n    df = pd.read_csv(filepath)\n    if as_frame:\n        return df\n    X = df[\n        [\n            \"length\",\n            \"diameter\",\n            \"height\",\n            \"whole_weight\",\n            \"shucked_weight\",\n            \"viscera_weight\",\n            \"shell_weight\",\n            \"rings\",\n        ]\n    ].values\n    y = df[\"sex\"].values\n    if return_X_y:\n        return X, y\n    return {\"data\": X, \"target\": y}\n</code></pre>"},{"location":"api/datasets/#sklego.datasets.load_arrests","title":"<code>sklego.datasets.load_arrests(return_X_y=False, as_frame=False)</code>","text":"<p>Loads the arrests dataset which can serve as a benchmark for fairness.</p> <p>It is data on the police treatment of individuals arrested in Toronto for simple possession of small quantities of marijuana. The goal is to predict whether or not the arrestee was released with a summons while maintaining a degree of fairness.</p> <p>Parameters:</p> Name Type Description Default <code>return_X_y</code> <code>bool</code> <p>If True, returns <code>(data, target)</code> instead of a dict object.</p> <code>False</code> <code>as_frame</code> <code>bool</code> <p>If True, returns <code>data</code> as a pandas DataFrame  instead of X, y matrices.</p> <code>False</code> <p>Examples:</p> <pre><code>from sklego.datasets import load_arrests\nX, y = load_arrests(return_X_y=True)\n\nX.shape\n# (5226, 7)\ny.shape\n# (5226,)\n\nload_arrests(as_frame=True).columns\n# Index(['released', 'colour', 'year', 'age', 'sex', 'employed', 'citizen',\n#   'checks'],\n#  dtype='object')\n</code></pre> <p>The dataset was copied from the carData R package (dataset documentation) and can originally be found in:</p> <ul> <li>Personal communication from Michael Friendly, York University.</li> </ul> Source code in <code>sklego/datasets.py</code> <pre><code>def load_arrests(return_X_y=False, as_frame=False):\n    \"\"\"Loads the arrests dataset which can serve as a benchmark for fairness.\n\n    It is data on the police treatment of individuals arrested in Toronto for simple possession of small quantities of\n    marijuana. The goal is to predict whether or not the arrestee was released with a summons while maintaining a\n    degree of fairness.\n\n    Parameters\n    ----------\n    return_X_y : bool, default=False\n        If True, returns `(data, target)` instead of a dict object.\n    as_frame : bool, default=False\n        If True, returns `data` as a pandas DataFrame  instead of X, y matrices.\n\n    Examples\n    -------\n    ```py\n    from sklego.datasets import load_arrests\n    X, y = load_arrests(return_X_y=True)\n\n    X.shape\n    # (5226, 7)\n    y.shape\n    # (5226,)\n\n    load_arrests(as_frame=True).columns\n    # Index(['released', 'colour', 'year', 'age', 'sex', 'employed', 'citizen',\n    #   'checks'],\n    #  dtype='object')\n    ```\n\n    The dataset was copied from the carData R package\n    ([dataset documentation](https://vincentarelbundock.github.io/Rdatasets/doc/carData/Arrests.html))\n    and can originally be found in:\n\n    - Personal communication from Michael Friendly, York University.\n    \"\"\"\n    filepath = resource_filename(\"sklego\", os.path.join(\"data\", \"arrests.zip\"))\n    df = pd.read_csv(filepath)\n    if as_frame:\n        return df\n    X, y = (\n        df[[\"colour\", \"year\", \"age\", \"sex\", \"employed\", \"citizen\", \"checks\"]].values,\n        df[\"released\"].values,\n    )\n    if return_X_y:\n        return X, y\n    return {\"data\": X, \"target\": y}\n</code></pre>"},{"location":"api/datasets/#sklego.datasets.load_chicken","title":"<code>sklego.datasets.load_chicken(return_X_y=False, as_frame=False)</code>","text":"<p>Loads the chicken dataset.</p> <p>The chicken data has 578 rows and 4 columns from an experiment on the effect of diet on early growth of chicks. The body weights of the chicks were measured at birth and every second day thereafter until day 20. They were also measured on day 21. There were four groups on chicks on different protein diets.</p> <p>Parameters:</p> Name Type Description Default <code>return_X_y</code> <code>bool</code> <p>If True, returns <code>(data, target)</code> instead of a dict object.</p> <code>False</code> <code>as_frame</code> <code>bool</code> <p>If True, returns <code>data</code> as a pandas DataFrame  instead of X, y matrices.</p> <code>False</code> <p>Examples:</p> <pre><code>from sklego.datasets import load_chicken\nX, y = load_chicken(return_X_y=True)\n\nX.shape\n# (578, 3)\ny.shape\n# (578,)\n\nload_chicken(as_frame=True).columns\n# Index(['weight', 'time', 'chick', 'diet'], dtype='object')\n</code></pre> <p>The datasets can be found in the following sources:</p> <ul> <li>Crowder, M. and Hand, D. (1990), Analysis of Repeated Measures, Chapman and Hall (example 5.3)</li> <li>Hand, D. and Crowder, M. (1996), Practical Longitudinal Data Analysis, Chapman and Hall (table A.2)</li> </ul> Source code in <code>sklego/datasets.py</code> <pre><code>def load_chicken(return_X_y=False, as_frame=False):\n    \"\"\"Loads the chicken dataset.\n\n    The chicken data has 578 rows and 4 columns from an experiment on the effect of diet on early growth of chicks.\n    The body weights of the chicks were measured at birth and every second day thereafter until day 20.\n    They were also measured on day 21. There were four groups on chicks on different protein diets.\n\n    Parameters\n    ----------\n    return_X_y : bool, default=False\n        If True, returns `(data, target)` instead of a dict object.\n    as_frame : bool, default=False\n        If True, returns `data` as a pandas DataFrame  instead of X, y matrices.\n\n    Examples\n    -------\n    ```py\n    from sklego.datasets import load_chicken\n    X, y = load_chicken(return_X_y=True)\n\n    X.shape\n    # (578, 3)\n    y.shape\n    # (578,)\n\n    load_chicken(as_frame=True).columns\n    # Index(['weight', 'time', 'chick', 'diet'], dtype='object')\n    ```\n\n    The datasets can be found in the following sources:\n\n    - Crowder, M. and Hand, D. (1990), Analysis of Repeated Measures, Chapman and Hall (example 5.3)\n    - Hand, D. and Crowder, M. (1996), Practical Longitudinal Data Analysis, Chapman and Hall (table A.2)\n    \"\"\"\n    filepath = resource_filename(\"sklego\", os.path.join(\"data\", \"chickweight.zip\"))\n    df = pd.read_csv(filepath)\n    if as_frame:\n        return df\n    X, y = df[[\"time\", \"diet\", \"chick\"]].values, df[\"weight\"].values\n    if return_X_y:\n        return X, y\n    return {\"data\": X, \"target\": y}\n</code></pre>"},{"location":"api/datasets/#sklego.datasets.load_heroes","title":"<code>sklego.datasets.load_heroes(return_X_y=False, as_frame=False)</code>","text":"<p>A dataset from the video game Heroes of the storm.</p> <p>The goal of the dataset is to predict the attack type. Note that the pandas dataset returns more information. This is because we wanted to keep the X simple in the return_X_y case.</p> <p>Parameters:</p> Name Type Description Default <code>return_X_y</code> <code>bool</code> <p>If True, returns <code>(data, target)</code> instead of a dict object.</p> <code>False</code> <code>as_frame</code> <code>bool</code> <p>If True, returns <code>data</code> as a pandas DataFrame  instead of X, y matrices.</p> <code>False</code> <p>Examples:</p> <pre><code>from sklego.datasets import load_heroes\nX, y = load_heroes(return_X_y=True)\n\nX.shape\n# (84, 2)\ny.shape\n# (84,)\n\nload_heroes(as_frame=True).columns\n# Index(['name', 'attack_type', 'role', 'health', 'attack', 'attack_spd'], dtype='object')\n</code></pre> Source code in <code>sklego/datasets.py</code> <pre><code>def load_heroes(return_X_y=False, as_frame=False):\n    \"\"\"A dataset from the video game [Heroes of the storm](https://heroesofthestorm.blizzard.com/en-us/).\n\n    The goal of the dataset is to predict the attack type. Note that the pandas dataset returns more information.\n    This is because we wanted to keep the X simple in the return_X_y case.\n\n    Parameters\n    ----------\n    return_X_y : bool, default=False\n        If True, returns `(data, target)` instead of a dict object.\n    as_frame : bool, default=False\n        If True, returns `data` as a pandas DataFrame  instead of X, y matrices.\n\n    Examples\n    --------\n    ```py\n    from sklego.datasets import load_heroes\n    X, y = load_heroes(return_X_y=True)\n\n    X.shape\n    # (84, 2)\n    y.shape\n    # (84,)\n\n    load_heroes(as_frame=True).columns\n    # Index(['name', 'attack_type', 'role', 'health', 'attack', 'attack_spd'], dtype='object')\n    ```\n    \"\"\"\n    filepath = resource_filename(\"sklego\", os.path.join(\"data\", \"heroes.zip\"))\n    df = pd.read_csv(filepath)\n    if as_frame:\n        return df\n    X = df[[\"health\", \"attack\"]].values\n    y = df[\"attack_type\"].values\n    if return_X_y:\n        return X, y\n    return {\"data\": X, \"target\": y}\n</code></pre>"},{"location":"api/datasets/#sklego.datasets.load_hearts","title":"<code>sklego.datasets.load_hearts(return_X_y=False, as_frame=False)</code>","text":"<p>Loads the Cleveland Heart Diseases dataset.</p> <p>The goal is to predict the presence of a heart disease (target values 1, 2, 3, and 4). The data originates from research to heart diseases by four institutions and originally contains 76 attributes. Yet, all published experiments refer to using a subset of 13 features and one target. This implementation loads the Cleveland dataset of the research which is the only set used by ML researchers to this date.</p> <p>Parameters:</p> Name Type Description Default <code>return_X_y</code> <code>bool</code> <p>If True, returns <code>(data, target)</code> instead of a dict object.</p> <code>False</code> <code>as_frame</code> <code>bool</code> <p>If True, returns <code>data</code> as a pandas DataFrame  instead of X, y matrices.</p> <code>False</code> <p>Examples:</p> <pre><code>from sklego.datasets import load_hearts\nX, y = load_hearts(return_X_y=True)\n\nX.shape\n# (303, 13)\ny.shape\n# (303,)\n\nload_hearts(as_frame=True).columns\n# Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n#        'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n#       dtype='object')\n</code></pre> <p>The dataset can originally be found in the UC Irvine Machine Learning Repository</p> <p>The responsible institutions for the entire database are:</p> <ol> <li>Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.</li> <li>University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.</li> <li>University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.</li> <li>V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.</li> </ol> <p>The documentation of the dataset can be viewed at: https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names</p> Source code in <code>sklego/datasets.py</code> <pre><code>def load_hearts(return_X_y=False, as_frame=False):\n    \"\"\"Loads the Cleveland Heart Diseases dataset.\n\n    The goal is to predict the presence of a heart disease (target values 1, 2, 3, and 4).\n    The data originates from research to heart diseases by four institutions and originally contains 76 attributes.\n    Yet, all published experiments refer to using a subset of 13 features and one target.\n    This implementation loads the Cleveland dataset of the research which is the only set used by ML researchers\n    to this date.\n\n    Parameters\n    ----------\n    return_X_y : bool, default=False\n        If True, returns `(data, target)` instead of a dict object.\n    as_frame : bool, default=False\n        If True, returns `data` as a pandas DataFrame  instead of X, y matrices.\n\n    Examples\n    --------\n    ```py\n    from sklego.datasets import load_hearts\n    X, y = load_hearts(return_X_y=True)\n\n    X.shape\n    # (303, 13)\n    y.shape\n    # (303,)\n\n    load_hearts(as_frame=True).columns\n    # Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n    #        'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n    #       dtype='object')\n    ```\n\n    The dataset can originally be found in the\n    [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)\n\n    The responsible institutions for the entire database are:\n\n    1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n    2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n    3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n    4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\n\n    The documentation of the dataset can be viewed at:\n    https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names\n    \"\"\"\n    filepath = resource_filename(\"sklego\", os.path.join(\"data\", \"hearts.zip\"))\n    df = pd.read_csv(filepath)\n    if as_frame:\n        return df\n    X = df[\n        [\n            \"age\",\n            \"sex\",\n            \"cp\",\n            \"trestbps\",\n            \"chol\",\n            \"fbs\",\n            \"restecg\",\n            \"thalach\",\n            \"exang\",\n            \"oldpeak\",\n            \"slope\",\n            \"ca\",\n            \"thal\",\n        ]\n    ].values\n    y = df[\"target\"].values\n    if return_X_y:\n        return X, y\n    return {\"data\": X, \"target\": y}\n</code></pre>"},{"location":"api/datasets/#sklego.datasets.load_penguins","title":"<code>sklego.datasets.load_penguins(return_X_y=False, as_frame=False)</code>","text":"<p>Loads the penguins dataset, which is a lovely alternative for the iris dataset. We've added this dataset for educational use.</p> <p>Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. The goal of the dataset is to predict which species of penguin a penguin belongs to.</p> <p>This data originally appeared as a R package and R users can find this data in the palmerpenguins package. You can also go to the repository for some lovely images that explain the dataset.</p> <p>To cite this dataset in publications use:</p> <pre><code>Gorman KB, Williams TD, Fraser WR (2014) Ecological Sexual Dimorphism\nand Environmental Variability within a Community of Antarctic\nPenguins (Genus Pygoscelis). PLoS ONE 9(3): e90081.\nhttps://doi.org/10.1371/journal.pone.0090081\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>return_X_y</code> <code>bool</code> <p>If True, returns <code>(data, target)</code> instead of a dict object.</p> <code>False</code> <code>as_frame</code> <code>bool</code> <p>If True, returns <code>data</code> as a pandas DataFrame  instead of X, y matrices.</p> <code>False</code> <p>Examples:</p> <pre><code>from sklego.datasets import load_penguins\nX, y = load_penguins(return_X_y=True)\n\nX.shape\n# (344, 6)\ny.shape\n# (344,)\n\nload_penguins(as_frame=True).columns\n# Index(['species', 'island', 'bill_length_mm', 'bill_depth_mm',\n#    'flipper_length_mm', 'body_mass_g', 'sex'],\n#   dtype='object')\n</code></pre>"},{"location":"api/datasets/#sklego.datasets.load_penguins--notes","title":"Notes","text":"<p>Anyone interested in publishing the data should contact <code>Dr. Kristen Gorman</code> about analysis and working together on any final products.</p> <p>From Gorman et al. (2014)</p> <p>Data reported here are publicly available within the PAL-LTER data system (datasets 219, 220, and 221).</p> <p>Individuals interested in using these data are therefore expected to follow the US LTER Network's Data Access Policy, Requirements and Use Agreement</p>"},{"location":"api/datasets/#sklego.datasets.load_penguins--please-cite-data-using-the-following","title":"Please cite data using the following","text":"<p>Ad\u00e9lie penguins:</p> <ul> <li> <p>Palmer Station Antarctica LTER and K. Gorman, 2020. Structural size measurements and isotopic signatures of     foraging among adult male and female Ad\u00e9lie penguins (Pygoscelis adeliae) nesting along the Palmer Archipelago     near Palmer Station, 2007-2009 ver 5. Environmental Data     Initiative.</p> <p>(Accessed 2020-06-08).</p> </li> </ul> <p>Gentoo penguins:</p> <ul> <li> <p>Palmer Station Antarctica LTER and K. Gorman, 2020. Structural size measurements and isotopic signatures of     foraging among adult male and female Gentoo penguin (Pygoscelis papua) nesting along the Palmer Archipelago     near Palmer Station, 2007-2009 ver 5. Environmental Data     Initiative.</p> <p>(Accessed 2020-06-08).</p> </li> </ul> <p>Chinstrap penguins:</p> <ul> <li> <p>Palmer Station Antarctica LTER and K. Gorman, 2020. Structural size measurements and isotopic signatures of     foraging among adult male and female Chinstrap penguin (Pygoscelis antarcticus) nesting along the Palmer     Archipelago near Palmer Station, 2007-2009 ver 6.     Environmental Data Initiative.</p> <p>(Accessed 2020-06-08).</p> </li> </ul> Source code in <code>sklego/datasets.py</code> <pre><code>def load_penguins(return_X_y=False, as_frame=False):\n    \"\"\"Loads the penguins dataset, which is a lovely alternative for the iris dataset.\n    We've added this dataset for educational use.\n\n    Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of\n    the Long Term Ecological Research Network. The goal of the dataset is to predict which species of penguin a penguin\n    belongs to.\n\n    This data originally appeared as a R package and R users can find this data in the\n    [palmerpenguins package](https://github.com/allisonhorst/palmerpenguins).\n    You can also go to the repository for some lovely images that explain the dataset.\n\n    To cite this dataset in publications use:\n\n        Gorman KB, Williams TD, Fraser WR (2014) Ecological Sexual Dimorphism\n        and Environmental Variability within a Community of Antarctic\n        Penguins (Genus Pygoscelis). PLoS ONE 9(3): e90081.\n        https://doi.org/10.1371/journal.pone.0090081\n\n    Parameters\n    ----------\n    return_X_y : bool, default=False\n        If True, returns `(data, target)` instead of a dict object.\n    as_frame : bool, default=False\n        If True, returns `data` as a pandas DataFrame  instead of X, y matrices.\n\n    Examples\n    -------\n    ```py\n    from sklego.datasets import load_penguins\n    X, y = load_penguins(return_X_y=True)\n\n    X.shape\n    # (344, 6)\n    y.shape\n    # (344,)\n\n    load_penguins(as_frame=True).columns\n    # Index(['species', 'island', 'bill_length_mm', 'bill_depth_mm',\n    #    'flipper_length_mm', 'body_mass_g', 'sex'],\n    #   dtype='object')\n    ```\n\n    Notes\n    -----\n    Anyone interested in publishing the data should contact\n    [`Dr. Kristen Gorman`](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php)\n    about analysis and working together on any final products.\n\n    !!! quote \"From Gorman et al. (2014)\"\n\n        Data reported here are publicly available within the\n        [PAL-LTER data system (datasets 219, 220, and 221)](http://oceaninformatics.ucsd.edu/datazoo/data/pallter/datasets).\n\n        Individuals interested in using these data are therefore expected to follow the [US LTER Network's Data Access\n        Policy, Requirements and Use Agreement](https://lternet.edu/data-access-policy/)\n\n    Please cite data using the following\n    ------------------------------------\n    **Ad\u00e9lie penguins:**\n\n      - [Palmer Station Antarctica LTER and K. Gorman, 2020. Structural size measurements and isotopic signatures of\n        foraging among adult male and female Ad\u00e9lie penguins (*Pygoscelis adeliae*) nesting along the Palmer Archipelago\n        near Palmer Station, 2007-2009 ver 5. Environmental Data\n        Initiative](https://doi.org/10.6073/pasta/98b16d7d563f265cb52372c8ca99e60f).\n\n        (Accessed 2020-06-08).\n\n    **Gentoo penguins:**\n\n      - [Palmer Station Antarctica LTER and K. Gorman, 2020. Structural size measurements and isotopic signatures of\n        foraging among adult male and female Gentoo penguin (*Pygoscelis papua*) nesting along the Palmer Archipelago\n        near Palmer Station, 2007-2009 ver 5. Environmental Data\n        Initiative](https://doi.org/10.6073/pasta/7fca67fb28d56ee2ffa3d9370ebda689).\n\n        (Accessed 2020-06-08).\n\n    **Chinstrap penguins:**\n\n      - [Palmer Station Antarctica LTER and K. Gorman, 2020. Structural size measurements and isotopic signatures of\n        foraging among adult male and female Chinstrap penguin (*Pygoscelis antarcticus*) nesting along the Palmer\n        Archipelago near Palmer Station, 2007-2009 ver 6.\n        Environmental Data Initiative](https://doi.org/10.6073/pasta/c14dfcfada8ea13a17536e73eb6fbe9e).\n\n        (Accessed 2020-06-08).\n    \"\"\"\n    filepath = resource_filename(\"sklego\", os.path.join(\"data\", \"penguins.zip\"))\n    df = pd.read_csv(filepath)\n    if as_frame:\n        return df\n    X, y = (\n        df[\n            [\n                \"island\",\n                \"bill_length_mm\",\n                \"bill_depth_mm\",\n                \"flipper_length_mm\",\n                \"body_mass_g\",\n                \"sex\",\n            ]\n        ].values,\n        df[\"species\"].values,\n    )\n    if return_X_y:\n        return X, y\n    return {\"data\": X, \"target\": y}\n</code></pre>"},{"location":"api/datasets/#sklego.datasets.make_simpleseries","title":"<code>sklego.datasets.make_simpleseries(n_samples=365 * 5, trend=0.001, season_trend=0.001, noise=0.5, as_frame=False, seed=None, stack_noise=False, start_date=None)</code>","text":"<p>Generate a very simple timeseries dataset to play with.</p> <p>The generator returns a daily time-series with a yearly seasonality, trend, and noise.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of days to simulate the timeseries for.</p> <code>365 * 5</code> <code>trend</code> <code>float</code> <p>The long term trend in the dataset.</p> <code>0.001</code> <code>season_trend</code> <code>float</code> <p>The long term trend in the seasonality.</p> <code>0.001</code> <code>noise</code> <code>float</code> <p>The noise that is applied to the dataset.</p> <code>0.5</code> <code>as_frame</code> <code>bool</code> <p>Whether to return a pandas dataframe instead of a numpy array.</p> <code>False</code> <code>seed</code> <code>int | None</code> <p>The seed value for the randomness.</p> <code>None</code> <code>stack_noise</code> <code>bool</code> <p>Set the noise to be stacked by a cumulative sum.</p> <code>False</code> <code>start_date</code> <code>str | None</code> <p>Also add a start date (only works if <code>as_frame</code>=True).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray | DataFrame</code> <p>Timeseries dataset with specified characteristics.</p> <p>Examples:</p> <pre><code>from sklego.datasets import make_simpleseries\n\nmake_simpleseries(seed=42)\n# array([-0.34078806, -0.61828731, -0.18458236, ..., -0.27547402,\n#        -0.38237413,  0.13489355])\n\nmake_simpleseries(as_frame=True, start_date=\"2018-01-01\", seed=42).head(3)\n'''\n         yt       date\n0 -0.340788 2018-01-01\n1 -0.618287 2018-01-02\n2 -0.184582 2018-01-03\n'''\n</code></pre> Source code in <code>sklego/datasets.py</code> <pre><code>def make_simpleseries(\n    n_samples=365 * 5,\n    trend=0.001,\n    season_trend=0.001,\n    noise=0.5,\n    as_frame=False,\n    seed=None,\n    stack_noise=False,\n    start_date=None,\n):\n    \"\"\"Generate a very simple timeseries dataset to play with.\n\n    The generator returns a daily time-series with a yearly seasonality, trend, and noise.\n\n    Parameters\n    ----------\n    n_samples : int, default=365 * 5\n        The number of days to simulate the timeseries for.\n    trend : float, default=0.001\n        The long term trend in the dataset.\n    season_trend : float, default=0.001\n        The long term trend in the seasonality.\n    noise : float, default=0.5\n        The noise that is applied to the dataset.\n    as_frame : bool, default=False\n        Whether to return a pandas dataframe instead of a numpy array.\n    seed : int | None, default=None\n        The seed value for the randomness.\n    stack_noise : bool, default=False\n        Set the noise to be stacked by a cumulative sum.\n    start_date : str | None, default=None\n        Also add a start date (only works if `as_frame`=True).\n\n    Returns\n    -------\n    np.ndarray | pd.DataFrame\n        Timeseries dataset with specified characteristics.\n\n    Examples\n    --------\n    ```py\n    from sklego.datasets import make_simpleseries\n\n    make_simpleseries(seed=42)\n    # array([-0.34078806, -0.61828731, -0.18458236, ..., -0.27547402,\n    #        -0.38237413,  0.13489355])\n\n    make_simpleseries(as_frame=True, start_date=\"2018-01-01\", seed=42).head(3)\n    '''\n             yt       date\n    0 -0.340788 2018-01-01\n    1 -0.618287 2018-01-02\n    2 -0.184582 2018-01-03\n    '''\n    ```\n    \"\"\"\n    if seed:\n        np.random.seed(seed)\n    time = np.arange(0, n_samples)\n    noise = np.random.normal(0, noise, n_samples)\n    if stack_noise:\n        noise = noise.cumsum()\n    r1, r2 = np.random.normal(0, 1, 2)\n    seasonality = r1 * np.sin(time / 365 * 2 * np.pi) + r2 * np.cos(time / 365 * 4 * np.pi + 1)\n    result = seasonality + season_trend * seasonality * time + trend * time + noise\n    if as_frame:\n        if start_date:\n            stamps = pd.date_range(start_date, periods=n_samples)\n            return pd.DataFrame({\"yt\": result, \"date\": stamps})\n        return pd.DataFrame({\"yt\": result})\n    return result\n</code></pre>"},{"location":"api/datasets/#sklego.datasets.fetch_creditcard","title":"<code>sklego.datasets.fetch_creditcard(*, cache=True, data_home=None, as_frame=False, return_X_y=False)</code>","text":"<p>Load the creditcard dataset. Download it if necessary.</p> <p>Note that internally this is using <code>fetch_openml</code> from scikit-learn, which is experimental.</p> <pre><code>==============   ==============\nSamples total            284807\nDimensionality               29\nFeatures                   real\nTarget                 int 0, 1\n==============   ==============\n</code></pre> <p>The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset present transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.</p> <p>Please cite:</p> <pre><code>Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi.\nCalibrating Probability with Undersampling for Unbalanced Classification.\nIn Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cache</code> <code>bool</code> <p>Whether to cache downloaded datasets using joblib.</p> <code>True</code> <code>data_home</code> <code>str | None</code> <p>Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in '~/scikit_learn_data' subfolders.</p> <code>None</code> <code>as_frame</code> <code>bool</code> <p>If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric, string or categorical). The target is a pandas DataFrame or Series depending on the number of target_columns. The Bunch will contain a <code>frame</code> attribute with the target and the data. If <code>return_X_y</code> is True, then <code>(data, target)</code> will be pandas DataFrames or Series as describe above.</p> <code>False</code> <code>return_X_y</code> <code>bool</code> <p>If True, returns <code>(data.data, data.target)</code> instead of a Bunch object.</p> <code>False.</code> <p>Returns:</p> Type Description <code>Dictionary - like</code> <p>With the following attributes:</p> <ul> <li>data     ndarray, shape (284807, 29) if <code>as_frame</code> is True, <code>data</code> is a pandas object.</li> <li>target     ndarray, shape (284807, ) if <code>as_frame</code> is True, <code>target</code> is a pandas object.</li> <li>feature_names     Array of ordered feature names used in the dataset.</li> <li>DESCR     Description of the creditcard dataset. Best to use print.</li> </ul>"},{"location":"api/datasets/#sklego.datasets.fetch_creditcard--notes","title":"Notes","text":"<p>This dataset consists of 284807 samples and 29 features.</p> Source code in <code>sklego/datasets.py</code> <pre><code>def fetch_creditcard(*, cache=True, data_home=None, as_frame=False, return_X_y=False):\n    \"\"\"Load the creditcard dataset. Download it if necessary.\n\n    Note that internally this is using\n    [`fetch_openml`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html) from\n    scikit-learn, which is experimental.\n\n        ==============   ==============\n        Samples total            284807\n        Dimensionality               29\n        Features                   real\n        Target                 int 0, 1\n        ==============   ==============\n\n    The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n    This dataset present transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions.\n    The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\n    Please cite:\n\n        Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi.\n        Calibrating Probability with Undersampling for Unbalanced Classification.\n        In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n\n    Parameters\n    ----------\n    cache : bool, default=True\n        Whether to cache downloaded datasets using joblib.\n    data_home : str | None, default=None\n        Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in\n        '~/scikit_learn_data' subfolders.\n    as_frame : bool, default=False\n        If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric, string or\n        categorical). The target is a pandas DataFrame or Series depending on the number of target_columns.\n        The Bunch will contain a `frame` attribute with the target and the data. If `return_X_y` is True, then\n        `(data, target)` will be pandas DataFrames or Series as describe above.\n    return_X_y : bool, default=False.\n        If True, returns `(data.data, data.target)` instead of a Bunch object.\n\n    Returns\n    -------\n    Dictionary-like\n        With the following attributes:\n\n         * data\n            ndarray, shape (284807, 29) if ``as_frame`` is True, ``data`` is a pandas object.\n         * target\n            ndarray, shape (284807, ) if ``as_frame`` is True, ``target`` is a pandas object.\n         * feature_names\n            Array of ordered feature names used in the dataset.\n         * DESCR\n            Description of the creditcard dataset. Best to use print.\n\n    Notes\n    -----\n    This dataset consists of 284807 samples and 29 features.\n    \"\"\"\n    return fetch_openml(\n        data_id=1597,\n        data_home=data_home,\n        cache=cache,\n        as_frame=as_frame,\n        return_X_y=return_X_y,\n    )\n</code></pre>"},{"location":"api/decay-functions/","title":"Decay Functions","text":"<p>These functions are used in the <code>DecayEstimator</code> to generate sample weights for the wrapped model.</p>"},{"location":"api/decay-functions/#sklego.meta._decay_utils.exponential_decay","title":"<code>sklego.meta._decay_utils.exponential_decay(X, y, decay_rate=0.999)</code>","text":"<p>Generates an exponential decay by mapping input data <code>X</code>, <code>y</code> to a exponential decreasing range \\(w_{t-1} = decay\\_rate * w_{t}\\). The length of the decay is determined by the number of samples in <code>y</code>.</p> <p>Warning</p> <p>It is up to the user to sort the dataset appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, shape=(n_samples, n_features,)</code> <p>Training data. Unused, present for API consistency by convention.</p> required <code>y</code> <code>array-like, shape=(n_samples,)</code> <p>Target values. Used to determine the number of samples in the decay.</p> required <code>decay_rate</code> <code>float</code> <p>The rate of decay.</p> <code>0.999</code> <p>Returns:</p> Type Description <code>np.ndarray, shape=(n_samples,)</code> <p>The decay values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>decay_rate</code> not between 0 and 1.</p> Source code in <code>sklego/meta/_decay_utils.py</code> <pre><code>def exponential_decay(X, y, decay_rate=0.999):\n    r\"\"\"Generates an exponential decay by mapping input data `X`, `y` to a exponential decreasing range\n    $w_{t-1} = decay\\_rate * w_{t}$. The length of the decay is determined by the number of samples in `y`.\n\n    !!! warning\n        It is up to the user to sort the dataset appropriately.\n\n    Parameters\n    ----------\n    X : array-like, shape=(n_samples, n_features,)\n        Training data. Unused, present for API consistency by convention.\n    y : array-like, shape=(n_samples,)\n        Target values. Used to determine the number of samples in the decay.\n    decay_rate : float, default=0.999\n        The rate of decay.\n\n    Returns\n    -------\n    np.ndarray, shape=(n_samples,)\n        The decay values.\n\n    Raises\n    ------\n    ValueError\n        If `decay_rate` not between 0 and 1.\n    \"\"\"\n\n    if decay_rate &lt;= 0 or decay_rate &gt;= 1:\n        raise ValueError(f\"`decay_rate` must be between 0. and 1., found {decay_rate}\")\n    n_samples = y.shape[0]\n    return decay_rate ** np.arange(n_samples, 0, -1)\n</code></pre>"},{"location":"api/decay-functions/#sklego.meta._decay_utils.linear_decay","title":"<code>sklego.meta._decay_utils.linear_decay(X, y, min_value=0.0, max_value=1.0)</code>","text":"<p>Generates a linear decay by mapping input data <code>X</code>, <code>y</code> to a linearly decreasing range from <code>max_value</code> to <code>min_value</code>. The length and step of the decay is determined by the number of samples in <code>y</code>.</p> <p>Warning</p> <p>It is up to the user to sort the dataset appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, shape=(n_samples, n_features,)</code> <p>Training data. Unused, present for API consistency by convention.</p> required <code>y</code> <code>array-like, shape=(n_samples,)</code> <p>Target values. Used to determine the number of samples in the decay.</p> required <code>min_value</code> <code>float</code> <p>The minimum value of the decay.</p> <code>0.</code> <code>max_value</code> <code>float</code> <p>The maximum value of the decay.</p> <code>1.</code> <p>Returns:</p> Type Description <code>np.ndarray, shape=(n_samples,)</code> <p>The decay values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>min_value</code> is greater than <code>max_value</code>.</p> Source code in <code>sklego/meta/_decay_utils.py</code> <pre><code>def linear_decay(X, y, min_value=0.0, max_value=1.0):\n    \"\"\"Generates a linear decay by mapping input data `X`, `y` to a linearly decreasing range from `max_value`\n    to `min_value`. The length and step of the decay is determined by the number of samples in `y`.\n\n    !!! warning\n        It is up to the user to sort the dataset appropriately.\n\n    Parameters\n    ----------\n    X : array-like, shape=(n_samples, n_features,)\n        Training data. Unused, present for API consistency by convention.\n    y : array-like, shape=(n_samples,)\n        Target values. Used to determine the number of samples in the decay.\n    min_value : float, default=0.\n        The minimum value of the decay.\n    max_value : float, default=1.\n        The maximum value of the decay.\n\n    Returns\n    -------\n    np.ndarray, shape=(n_samples,)\n        The decay values.\n\n    Raises\n    ------\n    ValueError\n        If `min_value` is greater than `max_value`.\n    \"\"\"\n\n    if min_value &gt; max_value:\n        raise ValueError(\"`min_value` must be less than or equal to `max_value`\")\n\n    n_samples = y.shape[0]\n    return np.linspace(min_value, max_value, n_samples + 1)[1:]\n</code></pre>"},{"location":"api/decay-functions/#sklego.meta._decay_utils.sigmoid_decay","title":"<code>sklego.meta._decay_utils.sigmoid_decay(X, y, growth_rate=None, min_value=0.0, max_value=1.0)</code>","text":"<p>Generates a sigmoid decay function that maps input data <code>X</code>, <code>y</code> to a non-linearly decreasing range from <code>max_value</code> to <code>min_value</code>. The steepness of the decay is determined by the <code>growth_rate</code> parameter. If not provided this will be set to <code>10 / n_samples</code>, which is a \"good enough\" default for most cases.</p> <p>Warning</p> <p>It is up to the user to sort the dataset appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, shape=(n_samples, n_features,)</code> <p>Training data. Unused, present for API consistency by convention.</p> required <code>y</code> <code>array-like, shape=(n_samples,)</code> <p>Target values. Used to determine the number of samples in the decay.</p> required <code>growth_rate</code> <code>float | None</code> <p>The growth rate of the sigmoid function. If not provided this will be set to <code>10 / n_samples</code>.</p> <code>None</code> <code>min_value</code> <code>float</code> <p>The minimum value of the decay.</p> <code>0.</code> <code>max_value</code> <code>float</code> <p>The maximum value of the decay.</p> <code>1.</code> <p>Returns:</p> Type Description <code>np.ndarray, shape=(n_samples,)</code> <p>The decay values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If <code>min_value</code> is greater than <code>max_value</code>.</li> <li>If <code>growth_rate</code> is specified and not between 0 and 1.</li> </ul> Source code in <code>sklego/meta/_decay_utils.py</code> <pre><code>def sigmoid_decay(X, y, growth_rate=None, min_value=0.0, max_value=1.0):\n    \"\"\"Generates a sigmoid decay function that maps input data `X`, `y` to a non-linearly decreasing range from\n    `max_value` to `min_value`. The steepness of the decay is determined by the `growth_rate` parameter.\n    If not provided this will be set to `10 / n_samples`, which is a \"good enough\" default for most cases.\n\n    !!! warning\n        It is up to the user to sort the dataset appropriately.\n\n    Parameters\n    ----------\n    X : array-like, shape=(n_samples, n_features,)\n        Training data. Unused, present for API consistency by convention.\n    y : array-like, shape=(n_samples,)\n        Target values. Used to determine the number of samples in the decay.\n    growth_rate : float | None, default=None\n        The growth rate of the sigmoid function. If not provided this will be set to `10 / n_samples`.\n    min_value : float, default=0.\n        The minimum value of the decay.\n    max_value : float, default=1.\n        The maximum value of the decay.\n\n    Returns\n    -------\n    np.ndarray, shape=(n_samples,)\n        The decay values.\n\n    Raises\n    ------\n    ValueError\n        - If `min_value` is greater than `max_value`.\n        - If `growth_rate` is specified and not between 0 and 1.\n    \"\"\"\n\n    if min_value &gt; max_value:\n        raise ValueError(\"`min_value` must be less than or equal to `max_value`\")\n\n    if growth_rate is not None and (growth_rate &lt;= 0 or growth_rate &gt;= 1):\n        raise ValueError(\"`growth_rate` must be between 0. and 1.\")\n\n    n_samples = y.shape[0]\n    growth_rate = growth_rate or 10 / n_samples\n\n    return min_value + (max_value - min_value) * _sigmoid(\n        x=np.arange(n_samples), growth_rate=growth_rate, offset=n_samples // 2\n    )\n</code></pre>"},{"location":"api/decay-functions/#sklego.meta._decay_utils.stepwise_decay","title":"<code>sklego.meta._decay_utils.stepwise_decay(X, y, n_steps=None, step_size=None, min_value=0.0, max_value=1.0)</code>","text":"<p>Generates a stepwise decay function that maps input data <code>X</code>, <code>y</code> to a decreasing range from <code>max_value</code> to <code>min_value</code>.</p> <p>It is possible to specify one of <code>n_steps</code> or <code>step_size</code> to determine the behaviour of the decay.</p> <ul> <li>If <code>step_size</code> is provided, the decay will be split into <code>n_samples // step_size</code> steps, each of which will     decrease the value by <code>step_width = (max_value - min_value) / n_steps</code>.</li> <li>If <code>n_steps</code> is provided, the decay will be split into <code>n_steps</code> steps, each of which will decrease the value     by <code>step_width = (max_value - min_value) / n_steps</code>.</li> </ul> <p>Each step of length step_size has constant weight, and then decreases by <code>step_width</code> until the minimum value is reached.</p> <p>Warning</p> <p>It is up to the user to sort the dataset appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, shape=(n_samples, n_features,)</code> <p>Training data. Unused, present for API consistency by convention.</p> required <code>y</code> <code>array-like, shape=(n_samples,)</code> <p>Target values. Used to determine the number of samples in the decay.</p> required <code>n_steps</code> <code>int | None</code> <p>The total number of steps in the decay.</p> <code>None</code> <code>step_size</code> <code>int | None</code> <p>The number of samples for each step in the decay.</p> <code>None</code> <code>min_value</code> <code>float</code> <p>The minimum value of the decay.</p> <code>0.</code> <code>max_value</code> <code>float</code> <p>The maximum value of the decay.</p> <code>1.</code> <p>Returns:</p> Type Description <code>np.ndarray, shape=(n_samples,)</code> <p>The decay values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If <code>min_value</code> is greater than <code>max_value</code>.</li> <li>If no value or both values are provided for <code>n_steps</code> or <code>step_size</code>.</li> <li>If <code>step_size</code> less than 0 or greater than the number of samples.</li> <li>If <code>n_steps</code> less than 0 or greater than the number of samples.</li> </ul> <code>TypeError</code> <ul> <li>If <code>n_steps</code> is not an integer.</li> <li>If <code>step_size</code> is not an integer.</li> </ul> Source code in <code>sklego/meta/_decay_utils.py</code> <pre><code>def stepwise_decay(X, y, n_steps=None, step_size=None, min_value=0.0, max_value=1.0):\n    \"\"\"Generates a stepwise decay function that maps input data `X`, `y` to a decreasing range from `max_value` to\n    `min_value`.\n\n    It is possible to specify one of `n_steps` or `step_size` to determine the behaviour of the decay.\n\n    - If `step_size` is provided, the decay will be split into `n_samples // step_size` steps, each of which will\n        decrease the value by `step_width = (max_value - min_value) / n_steps`.\n    - If `n_steps` is provided, the decay will be split into `n_steps` steps, each of which will decrease the value\n        by `step_width = (max_value - min_value) / n_steps`.\n\n    Each *step* of length *step_size* has constant weight, and then decreases by `step_width` until the minimum value is\n    reached.\n\n    !!! warning\n        It is up to the user to sort the dataset appropriately.\n\n    Parameters\n    ----------\n    X : array-like, shape=(n_samples, n_features,)\n        Training data. Unused, present for API consistency by convention.\n    y : array-like, shape=(n_samples,)\n        Target values. Used to determine the number of samples in the decay.\n    n_steps : int | None, default=None\n        The total number of steps in the decay.\n    step_size : int | None, default=None\n        The number of samples for each step in the decay.\n    min_value : float, default=0.\n        The minimum value of the decay.\n    max_value : float, default=1.\n        The maximum value of the decay.\n\n    Returns\n    -------\n    np.ndarray, shape=(n_samples,)\n        The decay values.\n\n    Raises\n    ------\n    ValueError\n        - If `min_value` is greater than `max_value`.\n        - If no value or both values are provided for `n_steps` or `step_size`.\n        - If `step_size` less than 0 or greater than the number of samples.\n        - If `n_steps` less than 0 or greater than the number of samples.\n    TypeError\n        - If `n_steps` is not an integer.\n        - If `step_size` is not an integer.\n    \"\"\"\n\n    if min_value &gt; max_value:\n        raise ValueError(\"`min_value` must be less than or equal to `max_value`\")\n\n    if step_size is None and n_steps is None:\n        raise ValueError(\"Either `step_size` or `n_steps` must be provided\")\n\n    elif step_size is not None and n_steps is not None:\n        raise ValueError(\"Only one of `step_size` or `n_steps` must be provided\")\n\n    elif step_size is not None and n_steps is None:\n        if not isinstance(step_size, int):\n            raise TypeError(\"`step_size` must be an integer\")\n\n        if step_size &lt;= 0:\n            raise ValueError(\"`step_size` must be greater than 0\")\n\n    elif step_size is None and n_steps is not None:\n        if not isinstance(n_steps, int):\n            raise TypeError(\"`n_steps` must be an integer\")\n\n        if n_steps &lt;= 0:\n            raise ValueError(\"`n_steps` must be greater than 0\")\n\n    n_samples = y.shape[0]\n\n    if step_size is not None and step_size &gt; n_samples:\n        raise ValueError(\"`step_size` must be less than or equal to the number of samples\")\n\n    if n_steps is not None and n_steps &gt; n_samples:\n        raise ValueError(\"`n_steps` must be less than or equal to the number of samples\")\n\n    n_steps = n_samples // step_size if step_size is not None else n_steps\n    step_size = n_samples // n_steps\n    step_width = (max_value - min_value) / n_steps\n\n    return max_value - (np.arange(n_samples, 0, -1) // step_size) * step_width\n</code></pre>"},{"location":"api/decomposition/","title":"Decomposition","text":""},{"location":"api/decomposition/#sklego.decomposition.pca_reconstruction.PCAOutlierDetection","title":"<code>sklego.decomposition.pca_reconstruction.PCAOutlierDetection</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>OutlierMixin</code></p> <p><code>PCAOutlierDetection</code> is an outlier detector based on the reconstruction error from PCA.</p> <p>If the difference between original and reconstructed data is larger than the <code>threshold</code>, the point is considered an outlier.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int | None</code> <p>Number of components of the PCA model.</p> <code>None</code> <code>threshold</code> <code>float | None</code> <p>The threshold used for the decision function.</p> <code>None</code> <code>variant</code> <code>Literal[relative, absolute]</code> <p>The variant used for the difference calculation. \"relative\" means that the difference between original and reconstructed data is divided by the sum of the original data.</p> <code>\"relative\"</code> <code>whiten</code> <code>bool</code> <p><code>whiten</code> parameter of PCA model.</p> <code>False</code> <code>svd_solver</code> <code>Literal[auto, full, arpack, randomized]</code> <p><code>svd_solver</code> parameter of PCA model.</p> <code>\"auto\"</code> <code>tol</code> <code>float</code> <p><code>tol</code> parameter of PCA model.</p> <code>0.0</code> <code>iterated_power</code> <code>int | Literal[auto]</code> <p><code>iterated_power</code> parameter of PCA model.</p> <code>\"auto\"</code> <code>random_state</code> <code>int | None</code> <p><code>random_state</code> parameter of PCA model.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>pca_</code> <code>PCA</code> <p>The underlying PCA model.</p> <code>offset_</code> <code>float</code> <p>The offset used for the decision function.</p> Source code in <code>sklego/decomposition/pca_reconstruction.py</code> <pre><code>class PCAOutlierDetection(BaseEstimator, OutlierMixin):\n    \"\"\"`PCAOutlierDetection` is an outlier detector based on the reconstruction error from PCA.\n\n    If the difference between original and reconstructed data is larger than the `threshold`, the point is\n    considered an outlier.\n\n    Parameters\n    ----------\n    n_components : int | None, default=None\n        Number of components of the PCA model.\n    threshold : float | None, default=None\n        The threshold used for the decision function.\n    variant : Literal[\"relative\", \"absolute\"], default=\"relative\"\n        The variant used for the difference calculation. \"relative\" means that the difference between original and\n        reconstructed data is divided by the sum of the original data.\n    whiten : bool, default=False\n        `whiten` parameter of PCA model.\n    svd_solver : Literal[\"auto\", \"full\", \"arpack\", \"randomized\"], default=\"auto\"\n        `svd_solver` parameter of PCA model.\n    tol : float, default=0.0\n        `tol` parameter of PCA model.\n    iterated_power : int | Literal[\"auto\"], default=\"auto\"\n        `iterated_power` parameter of PCA model.\n    random_state : int | None, default=None\n        `random_state` parameter of PCA model.\n\n    Attributes\n    ----------\n    pca_ : PCA\n        The underlying PCA model.\n    offset_ : float\n        The offset used for the decision function.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components=None,\n        threshold=None,\n        variant=\"relative\",\n        whiten=False,\n        svd_solver=\"auto\",\n        tol=0.0,\n        iterated_power=\"auto\",\n        random_state=None,\n    ):\n        self.n_components = n_components\n        self.threshold = threshold\n        self.whiten = whiten\n        self.variant = variant\n        self.svd_solver = svd_solver\n        self.tol = tol\n        self.iterated_power = iterated_power\n        self.random_state = random_state\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the `PCAOutlierDetection` model using `X` as training data by fitting the underlying PCA model, and\n        checking the `threshold` value.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,) or None, default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : PCAOutlierDetection\n            The fitted estimator.\n\n        Raises\n        ------\n        ValueError\n            If `threshold` is `None`.\n        \"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        if not self.threshold:\n            raise ValueError(\"The `threshold` value cannot be `None`.\")\n\n        self.pca_ = PCA(\n            n_components=self.n_components,\n            whiten=self.whiten,\n            svd_solver=self.svd_solver,\n            tol=self.tol,\n            iterated_power=self.iterated_power,\n            random_state=self.random_state,\n        )\n        self.pca_.fit(X, y)\n        self.offset_ = -self.threshold\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform the data using the underlying PCA method.\"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        check_is_fitted(self, [\"pca_\", \"offset_\"])\n        return self.pca_.transform(X)\n\n    def difference(self, X):\n        \"\"\"Return the calculated difference between original and reconstructed data. Row by row.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            Data to calculate the difference for.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The calculated difference.\n        \"\"\"\n        check_is_fitted(self, [\"pca_\", \"offset_\"])\n        reduced = self.pca_.transform(X)\n        diff = np.sum(np.abs(self.pca_.inverse_transform(reduced) - X), axis=1)\n        if self.variant == \"relative\":\n            diff = diff / X.sum(axis=1)\n        return diff\n\n    def decision_function(self, X):\n        \"\"\"Calculate the decision function for the data as the difference between `threshold` and the `.difference(X)`\n        (which is the difference between original data and reconstructed data).\"\"\"\n        return self.threshold - self.difference(X)\n\n    def score_samples(self, X):\n        \"\"\"Calculate the score for the samples\"\"\"\n        return -self.difference(X)\n\n    def predict(self, X):\n        \"\"\"Predict if a point is an outlier using fitted estimator.\n\n        If the difference between original and reconstructed data is larger than the `threshold`, the point is\n        considered an outlier.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted data. 1 for inliers, -1 for outliers.\n        \"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        check_is_fitted(self, [\"pca_\", \"offset_\"])\n        result = np.ones(X.shape[0])\n        result[self.difference(X) &gt; self.threshold] = -1\n        return result.astype(int)\n</code></pre>"},{"location":"api/decomposition/#sklego.decomposition.pca_reconstruction.PCAOutlierDetection.decision_function","title":"<code>decision_function(X)</code>","text":"<p>Calculate the decision function for the data as the difference between <code>threshold</code> and the <code>.difference(X)</code> (which is the difference between original data and reconstructed data).</p> Source code in <code>sklego/decomposition/pca_reconstruction.py</code> <pre><code>def decision_function(self, X):\n    \"\"\"Calculate the decision function for the data as the difference between `threshold` and the `.difference(X)`\n    (which is the difference between original data and reconstructed data).\"\"\"\n    return self.threshold - self.difference(X)\n</code></pre>"},{"location":"api/decomposition/#sklego.decomposition.pca_reconstruction.PCAOutlierDetection.difference","title":"<code>difference(X)</code>","text":"<p>Return the calculated difference between original and reconstructed data. Row by row.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>Data to calculate the difference for.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The calculated difference.</p> Source code in <code>sklego/decomposition/pca_reconstruction.py</code> <pre><code>def difference(self, X):\n    \"\"\"Return the calculated difference between original and reconstructed data. Row by row.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        Data to calculate the difference for.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The calculated difference.\n    \"\"\"\n    check_is_fitted(self, [\"pca_\", \"offset_\"])\n    reduced = self.pca_.transform(X)\n    diff = np.sum(np.abs(self.pca_.inverse_transform(reduced) - X), axis=1)\n    if self.variant == \"relative\":\n        diff = diff / X.sum(axis=1)\n    return diff\n</code></pre>"},{"location":"api/decomposition/#sklego.decomposition.pca_reconstruction.PCAOutlierDetection.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the <code>PCAOutlierDetection</code> model using <code>X</code> as training data by fitting the underlying PCA model, and checking the <code>threshold</code> value.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,) or None</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>PCAOutlierDetection</code> <p>The fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>threshold</code> is <code>None</code>.</p> Source code in <code>sklego/decomposition/pca_reconstruction.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the `PCAOutlierDetection` model using `X` as training data by fitting the underlying PCA model, and\n    checking the `threshold` value.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,) or None, default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : PCAOutlierDetection\n        The fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        If `threshold` is `None`.\n    \"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    if not self.threshold:\n        raise ValueError(\"The `threshold` value cannot be `None`.\")\n\n    self.pca_ = PCA(\n        n_components=self.n_components,\n        whiten=self.whiten,\n        svd_solver=self.svd_solver,\n        tol=self.tol,\n        iterated_power=self.iterated_power,\n        random_state=self.random_state,\n    )\n    self.pca_.fit(X, y)\n    self.offset_ = -self.threshold\n    return self\n</code></pre>"},{"location":"api/decomposition/#sklego.decomposition.pca_reconstruction.PCAOutlierDetection.predict","title":"<code>predict(X)</code>","text":"<p>Predict if a point is an outlier using fitted estimator.</p> <p>If the difference between original and reconstructed data is larger than the <code>threshold</code>, the point is considered an outlier.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted data. 1 for inliers, -1 for outliers.</p> Source code in <code>sklego/decomposition/pca_reconstruction.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict if a point is an outlier using fitted estimator.\n\n    If the difference between original and reconstructed data is larger than the `threshold`, the point is\n    considered an outlier.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted data. 1 for inliers, -1 for outliers.\n    \"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    check_is_fitted(self, [\"pca_\", \"offset_\"])\n    result = np.ones(X.shape[0])\n    result[self.difference(X) &gt; self.threshold] = -1\n    return result.astype(int)\n</code></pre>"},{"location":"api/decomposition/#sklego.decomposition.pca_reconstruction.PCAOutlierDetection.score_samples","title":"<code>score_samples(X)</code>","text":"<p>Calculate the score for the samples</p> Source code in <code>sklego/decomposition/pca_reconstruction.py</code> <pre><code>def score_samples(self, X):\n    \"\"\"Calculate the score for the samples\"\"\"\n    return -self.difference(X)\n</code></pre>"},{"location":"api/decomposition/#sklego.decomposition.pca_reconstruction.PCAOutlierDetection.transform","title":"<code>transform(X)</code>","text":"<p>Transform the data using the underlying PCA method.</p> Source code in <code>sklego/decomposition/pca_reconstruction.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform the data using the underlying PCA method.\"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    check_is_fitted(self, [\"pca_\", \"offset_\"])\n    return self.pca_.transform(X)\n</code></pre>"},{"location":"api/decomposition/#sklego.decomposition.umap_reconstruction.UMAPOutlierDetection","title":"<code>sklego.decomposition.umap_reconstruction.UMAPOutlierDetection</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>OutlierMixin</code></p> <p><code>UMAPOutlierDetection</code> is an outlier detector based on the reconstruction error from UMAP.</p> <p>If the difference between original and reconstructed data is larger than the <code>threshold</code>, the point is     considered an outlier.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of components of the UMAP model.</p> <code>2</code> <code>threshold</code> <code>float | None</code> <p>The threshold used for the decision function.</p> <code>None</code> <code>variant</code> <code>Literal[relative, absolute]</code> <p>The variant used for the difference calculation. \"relative\" means that the difference between original and reconstructed data is divided by the sum of the original data.</p> <code>\"relative\"</code> <code>n_neighbors</code> <code>int</code> <p><code>n_neighbors</code> parameter of UMAP model.</p> <code>15</code> <code>min_dist</code> <code>float</code> <p><code>min_dist</code> parameter of UMAP model.</p> <code>0.1</code> <code>metric</code> <code>str</code> <p><code>metric</code> parameter of UMAP model (see UMAP documentation for the full list of available metrics and more information).</p> <code>\"euclidean\"</code> <code>random_state</code> <code>int | None</code> <p><code>random_state</code> parameter of UMAP model.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>umap_</code> <code>UMAP</code> <p>The underlying UMAP model.</p> <code>offset_</code> <code>float</code> <p>The offset used for the decision function.</p> Source code in <code>sklego/decomposition/umap_reconstruction.py</code> <pre><code>class UMAPOutlierDetection(BaseEstimator, OutlierMixin):\n    \"\"\"`UMAPOutlierDetection` is an outlier detector based on the reconstruction error from UMAP.\n\n    If the difference between original and reconstructed data is larger than the `threshold`, the point is\n        considered an outlier.\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Number of components of the UMAP model.\n    threshold : float | None, default=None\n        The threshold used for the decision function.\n    variant : Literal[\"relative\", \"absolute\"], default=\"relative\"\n        The variant used for the difference calculation. \"relative\" means that the difference between original and\n        reconstructed data is divided by the sum of the original data.\n    n_neighbors : int, default=15\n        `n_neighbors` parameter of UMAP model.\n    min_dist : float, default=0.1\n        `min_dist` parameter of UMAP model.\n    metric : str, default=\"euclidean\"\n        `metric` parameter of UMAP model\n        (see [UMAP documentation](https://umap-learn.readthedocs.io/en/latest/parameters.html#metric) for the full list\n        of available metrics and more information).\n    random_state : int | None, default=None\n        `random_state` parameter of UMAP model.\n\n    Attributes\n    ----------\n    umap_ : UMAP\n        The underlying UMAP model.\n    offset_ : float\n        The offset used for the decision function.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components=2,\n        threshold=None,\n        variant=\"relative\",\n        n_neighbors=15,\n        min_dist=0.1,\n        metric=\"euclidean\",\n        random_state=None,\n    ):\n        self.n_components = n_components\n        self.threshold = threshold\n        self.variant = variant\n        self.n_neighbors = n_neighbors\n        self.min_dist = min_dist\n        self.metric = metric\n        self.random_state = random_state\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the `UMAPOutlierDetection` model using `X` as training data by fitting the underlying UMAP model, and\n        checking the `threshold` value.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,) or None, default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : UMAPOutlierDetection\n            The fitted estimator.\n\n        Raises\n        ------\n        ValueError\n            - If `n_components` is less than 2.\n            - If `threshold` is `None`.\n        \"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        if self.n_components &lt; 2:\n            raise ValueError(\"Number of components must be at least two.\")\n        if not self.threshold:\n            raise ValueError(\"The `threshold` value cannot be `None`.\")\n\n        self.umap_ = umap.UMAP(\n            n_components=self.n_components,\n            n_neighbors=self.n_neighbors,\n            min_dist=self.min_dist,\n            metric=self.metric,\n            random_state=self.random_state,\n        )\n        self.umap_.fit(X, y)\n        self.offset_ = -self.threshold\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform the data using the underlying UMAP method.\"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        check_is_fitted(self, [\"umap_\", \"offset_\"])\n        return self.umap_.transform(X)\n\n    def difference(self, X):\n        \"\"\"Return the calculated difference between original and reconstructed data. Row by row.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            Data to calculate the difference for.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The calculated difference.\n        \"\"\"\n        check_is_fitted(self, [\"umap_\", \"offset_\"])\n        reduced = self.umap_.transform(X)\n        diff = np.sum(np.abs(self.umap_.inverse_transform(reduced) - X), axis=1)\n        if self.variant == \"relative\":\n            diff = diff / X.sum(axis=1)\n        return diff\n\n    def predict(self, X):\n        \"\"\"Predict if a point is an outlier using fitted estimator.\n\n        If the difference between original and reconstructed data is larger than the `threshold`, the point is\n        considered an outlier.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted data. 1 for inliers, -1 for outliers.\n        \"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        check_is_fitted(self, [\"umap_\", \"offset_\"])\n        result = np.ones(X.shape[0])\n        result[self.difference(X) &gt; self.threshold] = -1\n        return result.astype(int)\n</code></pre>"},{"location":"api/decomposition/#sklego.decomposition.umap_reconstruction.UMAPOutlierDetection.difference","title":"<code>difference(X)</code>","text":"<p>Return the calculated difference between original and reconstructed data. Row by row.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>Data to calculate the difference for.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The calculated difference.</p> Source code in <code>sklego/decomposition/umap_reconstruction.py</code> <pre><code>def difference(self, X):\n    \"\"\"Return the calculated difference between original and reconstructed data. Row by row.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        Data to calculate the difference for.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The calculated difference.\n    \"\"\"\n    check_is_fitted(self, [\"umap_\", \"offset_\"])\n    reduced = self.umap_.transform(X)\n    diff = np.sum(np.abs(self.umap_.inverse_transform(reduced) - X), axis=1)\n    if self.variant == \"relative\":\n        diff = diff / X.sum(axis=1)\n    return diff\n</code></pre>"},{"location":"api/decomposition/#sklego.decomposition.umap_reconstruction.UMAPOutlierDetection.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the <code>UMAPOutlierDetection</code> model using <code>X</code> as training data by fitting the underlying UMAP model, and checking the <code>threshold</code> value.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,) or None</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>UMAPOutlierDetection</code> <p>The fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If <code>n_components</code> is less than 2.</li> <li>If <code>threshold</code> is <code>None</code>.</li> </ul> Source code in <code>sklego/decomposition/umap_reconstruction.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the `UMAPOutlierDetection` model using `X` as training data by fitting the underlying UMAP model, and\n    checking the `threshold` value.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,) or None, default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : UMAPOutlierDetection\n        The fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        - If `n_components` is less than 2.\n        - If `threshold` is `None`.\n    \"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    if self.n_components &lt; 2:\n        raise ValueError(\"Number of components must be at least two.\")\n    if not self.threshold:\n        raise ValueError(\"The `threshold` value cannot be `None`.\")\n\n    self.umap_ = umap.UMAP(\n        n_components=self.n_components,\n        n_neighbors=self.n_neighbors,\n        min_dist=self.min_dist,\n        metric=self.metric,\n        random_state=self.random_state,\n    )\n    self.umap_.fit(X, y)\n    self.offset_ = -self.threshold\n    return self\n</code></pre>"},{"location":"api/decomposition/#sklego.decomposition.umap_reconstruction.UMAPOutlierDetection.predict","title":"<code>predict(X)</code>","text":"<p>Predict if a point is an outlier using fitted estimator.</p> <p>If the difference between original and reconstructed data is larger than the <code>threshold</code>, the point is considered an outlier.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted data. 1 for inliers, -1 for outliers.</p> Source code in <code>sklego/decomposition/umap_reconstruction.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict if a point is an outlier using fitted estimator.\n\n    If the difference between original and reconstructed data is larger than the `threshold`, the point is\n    considered an outlier.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted data. 1 for inliers, -1 for outliers.\n    \"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    check_is_fitted(self, [\"umap_\", \"offset_\"])\n    result = np.ones(X.shape[0])\n    result[self.difference(X) &gt; self.threshold] = -1\n    return result.astype(int)\n</code></pre>"},{"location":"api/decomposition/#sklego.decomposition.umap_reconstruction.UMAPOutlierDetection.transform","title":"<code>transform(X)</code>","text":"<p>Transform the data using the underlying UMAP method.</p> Source code in <code>sklego/decomposition/umap_reconstruction.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform the data using the underlying UMAP method.\"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    check_is_fitted(self, [\"umap_\", \"offset_\"])\n    return self.umap_.transform(X)\n</code></pre>"},{"location":"api/dummy/","title":"Dummy","text":""},{"location":"api/dummy/#sklego.dummy.RandomRegressor","title":"<code>sklego.dummy.RandomRegressor</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>A <code>RandomRegressor</code> makes random predictions only based on the <code>y</code> value that is seen.</p> <p>The goal is that such a regressor can be used for benchmarking. It should be easily beatable.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>Literal[uniform, normal]</code> <p>How we want to select random values, either \"uniform\" or \"normal\"</p> <code>\"uniform\"</code> <code>random_state</code> <code>int | None</code> <p>The seed value used for the random number generator.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>min_</code> <code>float</code> <p>The minimum value of <code>y</code> seen during <code>fit</code>.</p> <code>max_</code> <code>float</code> <p>The maximum value of <code>y</code> seen during <code>fit</code>.</p> <code>mu_</code> <code>float</code> <p>The mean value of <code>y</code> seen during <code>fit</code>.</p> <code>sigma_</code> <code>float</code> <p>The standard deviation of <code>y</code> seen during <code>fit</code>.</p> <code>n_features_in_</code> <code>int</code> <p>The number of features seen during <code>fit</code>.</p> <code>dim_</code> <code>int</code> <p>Deprecated, please use <code>n_features_in_</code> instead.</p> <p>Examples:</p> <pre><code>from sklego.dummy import RandomRegressor\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=10, n_features=2, random_state=42)\n\nRandomRegressor(strategy=\"uniform\", random_state=123).fit(X, y).predict(X).round(2)\n# array([ 57.63, -66.05, -83.92,  13.88,  64.56, -24.77, 143.33,  54.12,\n#     -7.34, -34.11])\n\nRandomRegressor(strategy=\"normal\", random_state=123).fit(X, y).predict(X).round(2)\n# array([-128.45,   78.05,    7.23, -170.15,  -78.18,  142.9 , -261.39,\n#     -63.34,  104.68, -106.75])\n</code></pre> Source code in <code>sklego/dummy.py</code> <pre><code>class RandomRegressor(BaseEstimator, RegressorMixin):\n    \"\"\"A `RandomRegressor` makes random predictions only based on the `y` value that is seen.\n\n    The goal is that such a regressor can be used for benchmarking. It _should be_ easily beatable.\n\n    Parameters\n    ----------\n    strategy : Literal[\"uniform\", \"normal\"], default=\"uniform\"\n        How we want to select random values, either \"uniform\" or \"normal\"\n    random_state : int | None, default=None\n        The seed value used for the random number generator.\n\n    Attributes\n    ----------\n    min_ : float\n        The minimum value of `y` seen during `fit`.\n    max_ : float\n        The maximum value of `y` seen during `fit`.\n    mu_ : float\n        The mean value of `y` seen during `fit`.\n    sigma_ : float\n        The standard deviation of `y` seen during `fit`.\n    n_features_in_ : int\n        The number of features seen during `fit`.\n    dim_ : int\n        Deprecated, please use `n_features_in_` instead.\n\n    Examples\n    --------\n    ```py\n    from sklego.dummy import RandomRegressor\n    from sklearn.datasets import make_regression\n\n    X, y = make_regression(n_samples=10, n_features=2, random_state=42)\n\n    RandomRegressor(strategy=\"uniform\", random_state=123).fit(X, y).predict(X).round(2)\n    # array([ 57.63, -66.05, -83.92,  13.88,  64.56, -24.77, 143.33,  54.12,\n    #     -7.34, -34.11])\n\n    RandomRegressor(strategy=\"normal\", random_state=123).fit(X, y).predict(X).round(2)\n    # array([-128.45,   78.05,    7.23, -170.15,  -78.18,  142.9 , -261.39,\n    #     -63.34,  104.68, -106.75])\n    ```\n    \"\"\"\n\n    _ALLOWED_STRATEGIES = (\"uniform\", \"normal\")\n\n    def __init__(self, strategy=\"uniform\", random_state=None):\n        self.strategy = strategy\n        self.random_state = random_state\n\n    def fit(self, X: np.array, y: np.array) -&gt; \"RandomRegressor\":\n        \"\"\"\n        Fit the model using X, y as training data.\n\n        :param X: array-like, shape=(n_columns, n_samples,) training data.\n        :param y: array-like, shape=(n_samples,) training data.\n        :return: Returns an instance of self.\n        \"\"\"\n        if self.strategy not in self._ALLOWED_STRATEGIES:\n            raise ValueError(f\"strategy {self.strategy} is not in {self._ALLOWED_STRATEGIES}\")\n        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n        self.n_features_in_ = X.shape[1]\n\n        self.min_ = np.min(y)\n        self.max_ = np.max(y)\n        self.mu_ = np.mean(y)\n        self.sigma_ = np.std(y)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict new data by generating random guesses following the given `strategy` based on the `y` statistics seen\n        during `fit`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        rs = check_random_state(self.random_state)\n        check_is_fitted(self, [\"n_features_in_\", \"min_\", \"max_\", \"mu_\", \"sigma_\"])\n\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        if X.shape[1] != self.n_features_in_:\n            raise ValueError(f\"Unexpected input dimension {X.shape[1]}, expected {self.dim_}\")\n\n        if self.strategy == \"normal\":\n            return rs.normal(self.mu_, self.sigma_, X.shape[0])\n        if self.strategy == \"uniform\":\n            return rs.uniform(self.min_, self.max_, X.shape[0])\n\n    @property\n    def dim_(self):\n        warn(\n            \"Please use `n_features_in_` instead of `dim_`, `dim_` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self.n_features_in_\n\n    @property\n    def allowed_strategies(self):\n        warn(\n            \"Please use `_ALLOWED_STRATEGIES` instead of `allowed_strategies`,\"\n            \"`allowed_strategies` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self._ALLOWED_STRATEGIES\n</code></pre>"},{"location":"api/dummy/#sklego.dummy.RandomRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the model using X, y as training data.</p> <p>:param X: array-like, shape=(n_columns, n_samples,) training data. :param y: array-like, shape=(n_samples,) training data. :return: Returns an instance of self.</p> Source code in <code>sklego/dummy.py</code> <pre><code>def fit(self, X: np.array, y: np.array) -&gt; \"RandomRegressor\":\n    \"\"\"\n    Fit the model using X, y as training data.\n\n    :param X: array-like, shape=(n_columns, n_samples,) training data.\n    :param y: array-like, shape=(n_samples,) training data.\n    :return: Returns an instance of self.\n    \"\"\"\n    if self.strategy not in self._ALLOWED_STRATEGIES:\n        raise ValueError(f\"strategy {self.strategy} is not in {self._ALLOWED_STRATEGIES}\")\n    X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n    self.n_features_in_ = X.shape[1]\n\n    self.min_ = np.min(y)\n    self.max_ = np.max(y)\n    self.mu_ = np.mean(y)\n    self.sigma_ = np.std(y)\n\n    return self\n</code></pre>"},{"location":"api/dummy/#sklego.dummy.RandomRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predict new data by generating random guesses following the given <code>strategy</code> based on the <code>y</code> statistics seen during <code>fit</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted data.</p> Source code in <code>sklego/dummy.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict new data by generating random guesses following the given `strategy` based on the `y` statistics seen\n    during `fit`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted data.\n    \"\"\"\n    rs = check_random_state(self.random_state)\n    check_is_fitted(self, [\"n_features_in_\", \"min_\", \"max_\", \"mu_\", \"sigma_\"])\n\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(f\"Unexpected input dimension {X.shape[1]}, expected {self.dim_}\")\n\n    if self.strategy == \"normal\":\n        return rs.normal(self.mu_, self.sigma_, X.shape[0])\n    if self.strategy == \"uniform\":\n        return rs.uniform(self.min_, self.max_, X.shape[0])\n</code></pre>"},{"location":"api/linear-model/","title":"Linear Models","text":""},{"location":"api/linear-model/#sklego.linear_model.LowessRegression","title":"<code>sklego.linear_model.LowessRegression</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p><code>LowessRegression</code> estimator: LOWESS (Locally Weighted Scatterplot Smoothing) is a type of local regression.</p> <p>Warning</p> <p>This model can get expensive to predict. In fact the prediction step needs to compute the distance between each sample to predict <code>x_i</code> with all the training samples.</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>float</code> <p>The bandwidth parameter that determines the width of the smoothing.</p> <code>1.0</code> <code>span</code> <code>float | None</code> <p>The fraction of data points to consider during smoothing.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>X_</code> <code>np.ndarray of shape (n_samples, n_features)</code> <p>The training data.</p> <code>y_</code> <code>np.ndarray of shape (n_samples,)</code> <p>The target (training) values.</p> Source code in <code>sklego/linear_model.py</code> <pre><code>class LowessRegression(BaseEstimator, RegressorMixin):\n    \"\"\"`LowessRegression` estimator: LOWESS (Locally Weighted Scatterplot Smoothing) is a type of\n    [local regression](https://en.wikipedia.org/wiki/Local_regression).\n\n    !!! warning\n        This model *can* get expensive to predict.\n        In fact the prediction step needs to compute the distance between each sample to predict `x_i` with all the\n        training samples.\n\n    Parameters\n    ----------\n    sigma : float, default=1.0\n        The bandwidth parameter that determines the width of the smoothing.\n    span : float | None, default=None\n        The fraction of data points to consider during smoothing.\n\n    Attributes\n    ----------\n    X_ : np.ndarray of shape (n_samples, n_features)\n        The training data.\n    y_ : np.ndarray of shape (n_samples,)\n        The target (training) values.\n    \"\"\"\n\n    def __init__(self, sigma=1, span=None):\n        self.sigma = sigma\n        self.span = span\n\n    def fit(self, X, y):\n        \"\"\"Fit the estimator on training data `X` and `y` by storing them in `self.X_` and `self.y_`, and\n        validating the parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n\n        Returns\n        -------\n        self : LowessRegression\n            The fitted estimator.\n\n        Raises\n        ------\n        ValueError\n            - If `span` is not between 0 and 1.\n            - If `sigma` is negative.\n        \"\"\"\n        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n        if self.span is not None:\n            if not 0 &lt;= self.span &lt;= 1:\n                raise ValueError(f\"Param `span` must be 0 &lt;= span &lt;= 1, got: {self.span}\")\n        if self.sigma &lt; 0:\n            raise ValueError(f\"Param `sigma` must be &gt;= 0, got: {self.sigma}\")\n        self.X_ = X\n        self.y_ = y\n        return self\n\n    def _calc_wts(self, x_i):\n        \"\"\"Calculate the weights for a single point `x_i` using the training data `self.X_` and the parameters\n        `self.sigma` and `self.span`. The weights are calculated as `np.exp(-(distances**2) / self.sigma)`,\n        where distances are the distances between `x_i` and all the training samples.\n\n        If `self.span` is not None, then the weights are multiplied by\n        `(distances &lt;= np.quantile(distances, q=self.span))`.\n        \"\"\"\n        distances = np.linalg.norm(self.X_ - x_i, axis=1)\n        weights = np.exp(-(distances**2) / self.sigma)\n        if self.span:\n            weights = weights * (distances &lt;= np.quantile(distances, q=self.span))\n        return weights\n\n    def predict(self, X):\n        \"\"\"Predict target values for `X` using fitted estimator. This process is expensive because it needs to compute\n        the distance between each sample `x_i` with all the training samples.\n\n        Then it calculates the weights for **each sample** `x_i` as `np.exp(-(distances**2) / self.sigma)` and finally\n        it computes the weighted average of the `y` values weighted by these weights.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        check_is_fitted(self, [\"X_\", \"y_\"])\n\n        results = np.stack([np.average(self.y_, weights=self._calc_wts(x_i=x_i)) for x_i in X])\n        return results\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.LowessRegression.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the estimator on training data <code>X</code> and <code>y</code> by storing them in <code>self.X_</code> and <code>self.y_</code>, and validating the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>LowessRegression</code> <p>The fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If <code>span</code> is not between 0 and 1.</li> <li>If <code>sigma</code> is negative.</li> </ul> Source code in <code>sklego/linear_model.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the estimator on training data `X` and `y` by storing them in `self.X_` and `self.y_`, and\n    validating the parameters.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n\n    Returns\n    -------\n    self : LowessRegression\n        The fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        - If `span` is not between 0 and 1.\n        - If `sigma` is negative.\n    \"\"\"\n    X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n    if self.span is not None:\n        if not 0 &lt;= self.span &lt;= 1:\n            raise ValueError(f\"Param `span` must be 0 &lt;= span &lt;= 1, got: {self.span}\")\n    if self.sigma &lt; 0:\n        raise ValueError(f\"Param `sigma` must be &gt;= 0, got: {self.sigma}\")\n    self.X_ = X\n    self.y_ = y\n    return self\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.LowessRegression.predict","title":"<code>predict(X)</code>","text":"<p>Predict target values for <code>X</code> using fitted estimator. This process is expensive because it needs to compute the distance between each sample <code>x_i</code> with all the training samples.</p> <p>Then it calculates the weights for each sample <code>x_i</code> as <code>np.exp(-(distances**2) / self.sigma)</code> and finally it computes the weighted average of the <code>y</code> values weighted by these weights.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted values.</p> Source code in <code>sklego/linear_model.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict target values for `X` using fitted estimator. This process is expensive because it needs to compute\n    the distance between each sample `x_i` with all the training samples.\n\n    Then it calculates the weights for **each sample** `x_i` as `np.exp(-(distances**2) / self.sigma)` and finally\n    it computes the weighted average of the `y` values weighted by these weights.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted values.\n    \"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    check_is_fitted(self, [\"X_\", \"y_\"])\n\n    results = np.stack([np.average(self.y_, weights=self._calc_wts(x_i=x_i)) for x_i in X])\n    return results\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.ProbWeightRegression","title":"<code>sklego.linear_model.ProbWeightRegression</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p><code>ProbWeightRegression</code> assumes that all input signals in <code>X</code> need to be reweighted with weights that sum up to one in order to predict <code>y</code>.</p> <p>This can be very useful in combination with <code>sklego.meta.EstimatorTransformer</code> because it allows to construct an ensemble.</p> <p>Parameters:</p> Name Type Description Default <code>non_negative</code> <code>bool</code> <p>If True, forces all weights to be non-negative.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>n_features_in_</code> <code>int</code> <p>The number of features seen during <code>fit</code>.</p> <code>coef_</code> <code>(ndarray, shape(n_columns))</code> <p>The learned coefficients after fitting the model.</p> <code>coefs_</code> <code>(ndarray, shape(n_columns))</code> <p>Deprecated, please use <code>coef_</code> instead.</p> <p>Info</p> <p>This model requires <code>cvxpy</code> to be installed. If you don't have it installed, you can install it with:</p> <pre><code>pip install cvxpy\n# or pip install scikit-lego\"[cvxpy]\"\n</code></pre> Source code in <code>sklego/linear_model.py</code> <pre><code>class ProbWeightRegression(BaseEstimator, RegressorMixin):\n    \"\"\"`ProbWeightRegression` assumes that all input signals in `X` need to be reweighted with weights that sum up to\n    one in order to predict `y`.\n\n    This can be very useful in combination with `sklego.meta.EstimatorTransformer` because it allows to construct\n    an ensemble.\n\n    Parameters\n    ----------\n    non_negative : bool, default=True\n        If True, forces all weights to be non-negative.\n\n    Attributes\n    ----------\n    n_features_in_ : int\n        The number of features seen during `fit`.\n    coef_ : np.ndarray, shape (n_columns,)\n        The learned coefficients after fitting the model.\n    coefs_ : np.ndarray, shape (n_columns,)\n        Deprecated, please use `coef_` instead.\n\n    !!! info\n\n        This model requires [`cvxpy`](https://www.cvxpy.org/) to be installed. If you don't have it installed, you can\n        install it with:\n\n        ```bash\n        pip install cvxpy\n        # or pip install scikit-lego\"[cvxpy]\"\n        ```\n    \"\"\"\n\n    def __init__(self, non_negative=True):\n        self.non_negative = non_negative\n\n    def fit(self, X, y):\n        r\"\"\"Fit the estimator on training data `X` and `y` by solving the following convex optimization problem:\n\n        $$\\begin{array}{ll}{\\operatorname{minimize}} &amp; {\\sum_{i=1}^{N}\\left(\\mathbf{x}_{i}\n        \\boldsymbol{\\beta}-y_{i}\\right)^{2}} \\\\\n        {\\text { subject to }} &amp; {\\sum_{j=1}^{p} \\beta_{j}=1} \\\\\n        {(\\text{If non_negative=True})} &amp; {\\beta_{j} \\geq 0, \\quad j=1, \\ldots, p} \\end{array}$$\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n\n        Returns\n        -------\n        self : ProbWeightRegression\n            The fitted estimator.\n        \"\"\"\n        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n\n        # Construct the problem.\n        betas = cp.Variable(X.shape[1])\n        objective = cp.Minimize(cp.sum_squares(X @ betas - y))\n        constraints = [sum(betas) == 1]\n        if self.non_negative:\n            constraints.append(0 &lt;= betas)\n\n        # Solve the problem.\n        prob = cp.Problem(objective, constraints)\n        prob.solve()\n        self.coef_ = betas.value\n        self.n_features_in_ = X.shape[1]\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict target values for `X` using fitted estimator by multiplying `X` with the learned coefficients.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        check_is_fitted(self, [\"coef_\"])\n        return np.dot(X, self.coef_)\n\n    @property\n    def coefs_(self):\n        warn(\n            \"Please use `coef_` instead of `coefs_`, `coefs_` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self.coef_\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.ProbWeightRegression.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the estimator on training data <code>X</code> and <code>y</code> by solving the following convex optimization problem:</p> \\[\\begin{array}{ll}{\\operatorname{minimize}} &amp; {\\sum_{i=1}^{N}\\left(\\mathbf{x}_{i} \\boldsymbol{\\beta}-y_{i}\\right)^{2}} \\\\ {\\text { subject to }} &amp; {\\sum_{j=1}^{p} \\beta_{j}=1} \\\\ {(\\text{If non_negative=True})} &amp; {\\beta_{j} \\geq 0, \\quad j=1, \\ldots, p} \\end{array}\\] <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>ProbWeightRegression</code> <p>The fitted estimator.</p> Source code in <code>sklego/linear_model.py</code> <pre><code>def fit(self, X, y):\n    r\"\"\"Fit the estimator on training data `X` and `y` by solving the following convex optimization problem:\n\n    $$\\begin{array}{ll}{\\operatorname{minimize}} &amp; {\\sum_{i=1}^{N}\\left(\\mathbf{x}_{i}\n    \\boldsymbol{\\beta}-y_{i}\\right)^{2}} \\\\\n    {\\text { subject to }} &amp; {\\sum_{j=1}^{p} \\beta_{j}=1} \\\\\n    {(\\text{If non_negative=True})} &amp; {\\beta_{j} \\geq 0, \\quad j=1, \\ldots, p} \\end{array}$$\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n\n    Returns\n    -------\n    self : ProbWeightRegression\n        The fitted estimator.\n    \"\"\"\n    X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n\n    # Construct the problem.\n    betas = cp.Variable(X.shape[1])\n    objective = cp.Minimize(cp.sum_squares(X @ betas - y))\n    constraints = [sum(betas) == 1]\n    if self.non_negative:\n        constraints.append(0 &lt;= betas)\n\n    # Solve the problem.\n    prob = cp.Problem(objective, constraints)\n    prob.solve()\n    self.coef_ = betas.value\n    self.n_features_in_ = X.shape[1]\n\n    return self\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.ProbWeightRegression.predict","title":"<code>predict(X)</code>","text":"<p>Predict target values for <code>X</code> using fitted estimator by multiplying <code>X</code> with the learned coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted data.</p> Source code in <code>sklego/linear_model.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict target values for `X` using fitted estimator by multiplying `X` with the learned coefficients.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted data.\n    \"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    check_is_fitted(self, [\"coef_\"])\n    return np.dot(X, self.coef_)\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.DeadZoneRegressor","title":"<code>sklego.linear_model.DeadZoneRegressor</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>The <code>DeadZoneRegressor</code> estimator implements a regression model that incorporates a dead zone effect for improving the robustness of regression predictions.</p> <p>The dead zone effect allows the model to reduce the impact of small errors in the training data on the regression results, which can be particularly useful when dealing with noisy or unreliable data.</p> <p>The estimator minimizes the following loss function using gradient descent:</p> \\[\\frac{1}{n} \\sum_{i=1}^{n} \\text{deadzone}\\left(\\left|X_i \\cdot w - y_i\\right|\\right)\\] <p>where:</p> \\[\\text{deadzone}(e) = \\begin{cases} 1 &amp; \\text{if } e &gt; \\text{threshold} \\text{ &amp; effect=\"constant\"} \\\\ e &amp; \\text{if } e &gt; \\text{threshold} \\text{ &amp; effect=\"linear\"} \\\\ e^2 &amp; \\text{if } e &gt; \\text{threshold} \\text{ &amp; effect=\"quadratic\"} \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold value for the dead zone effect.</p> <code>0.3</code> <code>relative</code> <code>bool</code> <p>If True, the threshold is relative to the target value. Namely the dead zone effect is applied to the relative error between the predicted and target values.</p> <code>False</code> <code>effect</code> <code>Literal[linear, quadratic, constant]</code> <p>The type of dead zone effect to apply. It can be one of the following:</p> <ul> <li>\"linear\": the errors within the threshold have no impact (their contribution is effectively zero), and errors     outside the threshold are penalized linearly.</li> <li>\"quadratic\": the errors within the threshold have no impact (their contribution is effectively zero), and     errors outside the threshold are penalized quadratically (squared).</li> <li>\"constant\": the errors within the threshold have no impact, and errors outside the threshold are penalized     with a constant value.</li> </ul> <code>\"linear\"</code> <code>n_iter</code> <code>int</code> <p>The number of iterations to run the gradient descent algorithm.</p> <code>2000</code> <code>stepsize</code> <code>float</code> <p>The step size for the gradient descent algorithm.</p> <code>0.01</code> <code>check_grad</code> <code>bool</code> <p>If True, check the gradients numerically, just to be safe.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>coef_</code> <code>(ndarray, shape(n_columns))</code> <p>The learned coefficients after fitting the model.</p> <code>coefs_</code> <code>(ndarray, shape(n_columns))</code> <p>Deprecated, please use <code>coef_</code> instead.</p> Source code in <code>sklego/linear_model.py</code> <pre><code>class DeadZoneRegressor(BaseEstimator, RegressorMixin):\n    r\"\"\"The `DeadZoneRegressor` estimator implements a regression model that incorporates a _dead zone effect_ for\n    improving the robustness of regression predictions.\n\n    The dead zone effect allows the model to reduce the impact of small errors in the training data on the regression\n    results, which can be particularly useful when dealing with noisy or unreliable data.\n\n    The estimator minimizes the following loss function using gradient descent:\n\n    $$\\frac{1}{n} \\sum_{i=1}^{n} \\text{deadzone}\\left(\\left|X_i \\cdot w - y_i\\right|\\right)$$\n\n    where:\n\n    $$\\text{deadzone}(e) =\n    \\begin{cases}\n    1 &amp; \\text{if } e &gt; \\text{threshold} \\text{ &amp; effect=\"constant\"} \\\\\n    e &amp; \\text{if } e &gt; \\text{threshold} \\text{ &amp; effect=\"linear\"} \\\\\n    e^2 &amp; \\text{if } e &gt; \\text{threshold} \\text{ &amp; effect=\"quadratic\"} \\\\\n    0 &amp; \\text{otherwise}\n    \\end{cases}\n    $$\n\n    Parameters\n    ----------\n    threshold : float, default=0.3\n        The threshold value for the dead zone effect.\n    relative : bool, default=False\n        If True, the threshold is relative to the target value. Namely the _dead zone effect_ is applied to the\n        relative error between the predicted and target values.\n    effect : Literal[\"linear\", \"quadratic\", \"constant\"], default=\"linear\"\n        The type of dead zone effect to apply. It can be one of the following:\n\n        - \"linear\": the errors within the threshold have no impact (their contribution is effectively zero), and errors\n            outside the threshold are penalized linearly.\n        - \"quadratic\": the errors within the threshold have no impact (their contribution is effectively zero), and\n            errors outside the threshold are penalized quadratically (squared).\n        - \"constant\": the errors within the threshold have no impact, and errors outside the threshold are penalized\n            with a constant value.\n    n_iter : int, default=2000\n        The number of iterations to run the gradient descent algorithm.\n    stepsize : float, default=0.01\n        The step size for the gradient descent algorithm.\n    check_grad : bool, default=False\n        If True, check the gradients numerically, _just to be safe_.\n\n    Attributes\n    ----------\n    coef_ : np.ndarray, shape (n_columns,)\n        The learned coefficients after fitting the model.\n    coefs_ : np.ndarray, shape (n_columns,)\n        Deprecated, please use `coef_` instead.\n    \"\"\"\n\n    _ALLOWED_EFFECTS = (\"linear\", \"quadratic\", \"constant\")\n\n    def __init__(\n        self,\n        threshold=0.3,\n        relative=False,\n        effect=\"linear\",\n    ):\n        self.threshold = threshold\n        self.relative = relative\n        self.effect = effect\n\n    def fit(self, X, y):\n        \"\"\"Fit the estimator on training data `X` and `y` by optimizing the loss function using gradient descent.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n\n        Returns\n        -------\n        self : DeadZoneRegressor\n            The fitted estimator.\n\n        Raises\n        ------\n        ValueError\n            If `effect` is not one of \"linear\", \"quadratic\" or \"constant\".\n        \"\"\"\n        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n        if self.effect not in self._ALLOWED_EFFECTS:\n            raise ValueError(f\"effect {self.effect} must be in {self._ALLOWED_EFFECTS}\")\n\n        def deadzone(errors):\n            if self.effect == \"constant\":\n                error_weight = errors.shape[0]\n            elif self.effect == \"linear\":\n                error_weight = errors\n            elif self.effect == \"quadratic\":\n                error_weight = errors**2\n\n            return np.where(errors &gt; self.threshold, error_weight, 0.0)\n\n        def training_loss(weights):\n            prediction = np.dot(X, weights)\n            errors = np.abs(prediction - y)\n\n            if self.relative:\n                errors /= np.abs(y)\n\n            loss = np.mean(deadzone(errors))\n            return loss\n\n        def deadzone_derivative(errors):\n            if self.effect == \"constant\":\n                error_weight = 0.0\n            elif self.effect == \"linear\":\n                error_weight = 1.0\n            elif self.effect == \"quadratic\":\n                error_weight = 2 * errors\n\n            return np.where(errors &gt; self.threshold, error_weight, 0.0)\n\n        def training_loss_derivative(weights):\n            prediction = np.dot(X, weights)\n            errors = np.abs(prediction - y)\n\n            if self.relative:\n                errors /= np.abs(y)\n\n            loss_derivative = deadzone_derivative(errors)\n            errors_derivative = np.sign(prediction - y)\n\n            if self.relative:\n                errors_derivative /= np.abs(y)\n\n            derivative = np.dot(errors_derivative * loss_derivative, X) / X.shape[0]\n\n            return derivative\n\n        self.n_features_in_ = X.shape[1]\n\n        minimize_result = minimize(\n            training_loss,\n            x0=np.zeros(self.n_features_in_),  # np.random.normal(0, 1, n_features_)\n            tol=1e-20,\n            jac=training_loss_derivative,\n        )\n\n        self.convergence_status_ = minimize_result.message\n        self.coef_ = minimize_result.x\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict target values for `X` using fitted estimator by multiplying `X` with the learned coefficients.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        check_is_fitted(self, [\"coef_\"])\n        return np.dot(X, self.coef_)\n\n    @property\n    def coefs_(self):\n        warn(\n            \"Please use `coef_` instead of `coefs_`, `coefs_` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self.coef_\n\n    @property\n    def allowed_effects(self):\n        warn(\n            \"Please use `_ALLOWED_EFFECTS` instead of `allowed_effects`,\"\n            \"`allowed_effects` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self._ALLOWED_EFFECTS\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.DeadZoneRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the estimator on training data <code>X</code> and <code>y</code> by optimizing the loss function using gradient descent.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>DeadZoneRegressor</code> <p>The fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>effect</code> is not one of \"linear\", \"quadratic\" or \"constant\".</p> Source code in <code>sklego/linear_model.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the estimator on training data `X` and `y` by optimizing the loss function using gradient descent.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n\n    Returns\n    -------\n    self : DeadZoneRegressor\n        The fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        If `effect` is not one of \"linear\", \"quadratic\" or \"constant\".\n    \"\"\"\n    X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n    if self.effect not in self._ALLOWED_EFFECTS:\n        raise ValueError(f\"effect {self.effect} must be in {self._ALLOWED_EFFECTS}\")\n\n    def deadzone(errors):\n        if self.effect == \"constant\":\n            error_weight = errors.shape[0]\n        elif self.effect == \"linear\":\n            error_weight = errors\n        elif self.effect == \"quadratic\":\n            error_weight = errors**2\n\n        return np.where(errors &gt; self.threshold, error_weight, 0.0)\n\n    def training_loss(weights):\n        prediction = np.dot(X, weights)\n        errors = np.abs(prediction - y)\n\n        if self.relative:\n            errors /= np.abs(y)\n\n        loss = np.mean(deadzone(errors))\n        return loss\n\n    def deadzone_derivative(errors):\n        if self.effect == \"constant\":\n            error_weight = 0.0\n        elif self.effect == \"linear\":\n            error_weight = 1.0\n        elif self.effect == \"quadratic\":\n            error_weight = 2 * errors\n\n        return np.where(errors &gt; self.threshold, error_weight, 0.0)\n\n    def training_loss_derivative(weights):\n        prediction = np.dot(X, weights)\n        errors = np.abs(prediction - y)\n\n        if self.relative:\n            errors /= np.abs(y)\n\n        loss_derivative = deadzone_derivative(errors)\n        errors_derivative = np.sign(prediction - y)\n\n        if self.relative:\n            errors_derivative /= np.abs(y)\n\n        derivative = np.dot(errors_derivative * loss_derivative, X) / X.shape[0]\n\n        return derivative\n\n    self.n_features_in_ = X.shape[1]\n\n    minimize_result = minimize(\n        training_loss,\n        x0=np.zeros(self.n_features_in_),  # np.random.normal(0, 1, n_features_)\n        tol=1e-20,\n        jac=training_loss_derivative,\n    )\n\n    self.convergence_status_ = minimize_result.message\n    self.coef_ = minimize_result.x\n    return self\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.DeadZoneRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predict target values for <code>X</code> using fitted estimator by multiplying <code>X</code> with the learned coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted data.</p> Source code in <code>sklego/linear_model.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict target values for `X` using fitted estimator by multiplying `X` with the learned coefficients.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted data.\n    \"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    check_is_fitted(self, [\"coef_\"])\n    return np.dot(X, self.coef_)\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.DemographicParityClassifier","title":"<code>sklego.linear_model.DemographicParityClassifier</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>LinearClassifierMixin</code></p> <p><code>DemographicParityClassifier</code> is a logistic regression classifier which can be constrained on demographic parity (p% score).</p> <p>It minimizes the log loss while constraining the correlation between the specified <code>sensitive_cols</code> and the distance to the decision boundary of the classifier.</p> <p>Warning</p> <p>This classifier only works for binary classification problems.</p> \\[\\begin{array}{cl}{\\operatorname{minimize}} &amp; -\\sum_{i=1}^{N} \\log p\\left(y_{i} | \\mathbf{x}_{i},     \\boldsymbol{\\theta}\\right) \\\\     {\\text { subject to }} &amp; {\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right)     d_\\boldsymbol{\\theta}\\left(\\mathbf{x}_{i}\\right) \\leq \\mathbf{c}} \\\\     {} &amp; {\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right)     d_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{i}\\right) \\geq-\\mathbf{c}}\\end{array}\\] <p>Parameters:</p> Name Type Description Default <code>covariance_threshold</code> <code>float | None</code> <p>The maximum allowed covariance between the sensitive attributes and the distance to the decision boundary. If set to None, no fairness constraint is enforced.</p> required <code>sensitive_cols</code> <code>List[str] | List[int] | None</code> <p>List of sensitive column names (if X is a dataframe) or a list of column indices (if X is a numpy array).</p> <code>None</code> <code>C</code> <code>float</code> <p>Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.</p> <code>1.0</code> <code>penalty</code> <code>Literal[l1, none]</code> <p>Used to specify the norm used in the penalization.</p> <code>\"l1\"</code> <code>fit_intercept</code> <code>bool</code> <p>Whether or not a constant term (a.k.a. bias or intercept) should be added to the decision function.</p> <code>True</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations taken for the solvers to converge.</p> <code>100</code> <code>train_sensitive_cols</code> <code>bool</code> <p>Indicates whether the model should use the sensitive columns in the fit step.</p> <code>False</code> <code>multi_class</code> <code>Literal[ovr, ovo]</code> <p>The method to use for multiclass predictions.</p> <code>\"ovr\"</code> <code>n_jobs</code> <code>int | None</code> <p>The amount of parallel jobs that should be used to fit the model.</p> <code>1</code>"},{"location":"api/linear-model/#sklego.linear_model.DemographicParityClassifier--source","title":"Source","text":"<p>M. Zafar et al. (2017), Fairness Constraints: Mechanisms for Fair Classification</p> Source code in <code>sklego/linear_model.py</code> <pre><code>class DemographicParityClassifier(BaseEstimator, LinearClassifierMixin):\n    r\"\"\"`DemographicParityClassifier` is a logistic regression classifier which can be constrained on demographic\n    parity (p% score).\n\n    It minimizes the log loss while constraining the correlation between the specified `sensitive_cols` and the\n    distance to the decision boundary of the classifier.\n\n    !!! warning\n        This classifier only works for binary classification problems.\n\n    $$\\begin{array}{cl}{\\operatorname{minimize}} &amp; -\\sum_{i=1}^{N} \\log p\\left(y_{i} | \\mathbf{x}_{i},\n        \\boldsymbol{\\theta}\\right) \\\\\n        {\\text { subject to }} &amp; {\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right)\n        d_\\boldsymbol{\\theta}\\left(\\mathbf{x}_{i}\\right) \\leq \\mathbf{c}} \\\\\n        {} &amp; {\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right)\n        d_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{i}\\right) \\geq-\\mathbf{c}}\\end{array}$$\n\n    Parameters\n    ----------\n    covariance_threshold : float | None\n        The maximum allowed covariance between the sensitive attributes and the distance to the decision boundary.\n        If set to None, no fairness constraint is enforced.\n    sensitive_cols : List[str] | List[int] | None, default=None\n        List of sensitive column names (if X is a dataframe) or a list of column indices (if X is a numpy array).\n    C : float, default=1.0\n        Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values\n        specify stronger regularization.\n    penalty : Literal[\"l1\", \"none\"], default=\"l1\"\n        Used to specify the norm used in the penalization.\n    fit_intercept : bool, default=True\n        Whether or not a constant term (a.k.a. bias or intercept) should be added to the decision function.\n    max_iter : int, default=100\n        Maximum number of iterations taken for the solvers to converge.\n    train_sensitive_cols : bool, default=False\n        Indicates whether the model should use the sensitive columns in the fit step.\n    multi_class : Literal[\"ovr\", \"ovo\"], default=\"ovr\"\n        The method to use for multiclass predictions.\n    n_jobs : int | None, default=1\n        The amount of parallel jobs that should be used to fit the model.\n\n    Source\n    ------\n    M. Zafar et al. (2017), Fairness Constraints: Mechanisms for Fair Classification\n    \"\"\"\n\n    def __new__(cls, *args, multi_class=\"ovr\", n_jobs=1, **kwargs):\n        multiclass_meta = {\"ovr\": OneVsRestClassifier, \"ovo\": OneVsOneClassifier}[multi_class]\n        return multiclass_meta(_DemographicParityClassifier(*args, **kwargs), n_jobs=n_jobs)\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.EqualOpportunityClassifier","title":"<code>sklego.linear_model.EqualOpportunityClassifier</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>LinearClassifierMixin</code></p> <p><code>EqualOpportunityClassifier</code> is a logistic regression classifier which can be constrained on equal opportunity score.</p> <p>It minimizes the log loss while constraining the correlation between the specified <code>sensitive_cols</code> and the distance to the decision boundary of the classifier for those examples that have a y_true of 1.</p> <p>Warning</p> <p>This classifier only works for binary classification problems.</p> \\[\\begin{array}{cl}{\\operatorname{minimize}} &amp; -\\sum_{i=1}^{N} \\log p\\left(y_{i} | \\mathbf{x}_{i},     \\boldsymbol{\\theta}\\right) \\\\     {\\text { subject to }} &amp; {\\frac{1}{POS} \\sum_{i=1}^{POS}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right)     d_\\boldsymbol{\\theta}\\left(\\mathbf{x}_{i}\\right) \\leq \\mathbf{c}} \\\\     {} &amp; {\\frac{1}{POS} \\sum_{i=1}^{POS}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right)     d_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{i}\\right) \\geq-\\mathbf{c}}\\end{array}\\] <p>where POS is the subset of the population where \\(\\text{y_true} = 1\\)</p> <p>Parameters:</p> Name Type Description Default <code>covariance_threshold</code> <code>float | None</code> <p>The maximum allowed covariance between the sensitive attributes and the distance to the decision boundary. If set to None, no fairness constraint is enforced.</p> required <code>positive_target</code> <code>int</code> <p>The name of the class which is associated with a positive outcome</p> required <code>sensitive_cols</code> <code>List[str] | List[int] | None</code> <p>List of sensitive column names (if X is a dataframe) or a list of column indices (if X is a numpy array).</p> <code>None</code> <code>C</code> <code>float</code> <p>Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.</p> <code>1.0</code> <code>penalty</code> <code>Literal[l1, none]</code> <p>Used to specify the norm used in the penalization.</p> <code>\"l1\"</code> <code>fit_intercept</code> <code>bool</code> <p>Whether or not a constant term (a.k.a. bias or intercept) should be added to the decision function.</p> <code>True</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations taken for the solvers to converge.</p> <code>100</code> <code>train_sensitive_cols</code> <code>bool</code> <p>Indicates whether the model should use the sensitive columns in the fit step.</p> <code>False</code> <code>multi_class</code> <code>Literal[ovr, ovo]</code> <p>The method to use for multiclass predictions.</p> <code>\"ovr\"</code> <code>n_jobs</code> <code>int | None</code> <p>The amount of parallel jobs that should be used to fit the model.</p> <code>1</code> Source code in <code>sklego/linear_model.py</code> <pre><code>class EqualOpportunityClassifier(BaseEstimator, LinearClassifierMixin):\n    r\"\"\"`EqualOpportunityClassifier` is a logistic regression classifier which can be constrained on equal opportunity\n    score.\n\n    It minimizes the log loss while constraining the correlation between the specified `sensitive_cols` and the\n    distance to the decision boundary of the classifier for those examples that have a y_true of 1.\n\n    !!! warning\n        This classifier only works for binary classification problems.\n\n    $$\\begin{array}{cl}{\\operatorname{minimize}} &amp; -\\sum_{i=1}^{N} \\log p\\left(y_{i} | \\mathbf{x}_{i},\n        \\boldsymbol{\\theta}\\right) \\\\\n        {\\text { subject to }} &amp; {\\frac{1}{POS} \\sum_{i=1}^{POS}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right)\n        d_\\boldsymbol{\\theta}\\left(\\mathbf{x}_{i}\\right) \\leq \\mathbf{c}} \\\\\n        {} &amp; {\\frac{1}{POS} \\sum_{i=1}^{POS}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right)\n        d_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{i}\\right) \\geq-\\mathbf{c}}\\end{array}$$\n\n    where POS is the subset of the population where $\\text{y_true} = 1$\n\n    Parameters\n    ----------\n    covariance_threshold : float | None\n        The maximum allowed covariance between the sensitive attributes and the distance to the decision boundary.\n        If set to None, no fairness constraint is enforced.\n    positive_target : int\n        The name of the class which is associated with a positive outcome\n    sensitive_cols : List[str] | List[int] | None, default=None\n        List of sensitive column names (if X is a dataframe) or a list of column indices (if X is a numpy array).\n    C : float, default=1.0\n        Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values\n        specify stronger regularization.\n    penalty : Literal[\"l1\", \"none\"], default=\"l1\"\n        Used to specify the norm used in the penalization.\n    fit_intercept : bool, default=True\n        Whether or not a constant term (a.k.a. bias or intercept) should be added to the decision function.\n    max_iter : int, default=100\n        Maximum number of iterations taken for the solvers to converge.\n    train_sensitive_cols : bool, default=False\n        Indicates whether the model should use the sensitive columns in the fit step.\n    multi_class : Literal[\"ovr\", \"ovo\"], default=\"ovr\"\n        The method to use for multiclass predictions.\n    n_jobs : int | None, default=1\n        The amount of parallel jobs that should be used to fit the model.\n    \"\"\"\n\n    def __new__(cls, *args, multi_class=\"ovr\", n_jobs=1, **kwargs):\n        multiclass_meta = {\"ovr\": OneVsRestClassifier, \"ovo\": OneVsOneClassifier}[multi_class]\n        return multiclass_meta(_EqualOpportunityClassifier(*args, **kwargs), n_jobs=n_jobs)\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.BaseScipyMinimizeRegressor","title":"<code>sklego.linear_model.BaseScipyMinimizeRegressor</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code>, <code>ABC</code></p> <p>Abstract base class for regressors relying on Scipy's minimize method to minimize a (custom) loss function.</p> <p>Derive a class from this one and give it the function to be minimized. The derived class should implement the <code>_get_objective</code> method, which should return the loss function and its gradient.</p> <p>Info</p> <p>This implementation uses scipy.optimize.minimize.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Constant that multiplies the penalty terms.</p> <code>0.0</code> <code>l1_ratio</code> <code>float</code> <p>The ElasticNet mixing parameter, with <code>0 &lt;= l1_ratio &lt;= 1</code>:</p> <ul> <li><code>l1_ratio = 0</code> is equivalent to an L2 penalty.</li> <li><code>l1_ratio = 1</code> is equivalent to an L1 penalty.</li> <li><code>0 &lt; l1_ratio &lt; 1</code> is the combination of L1 and L2.</li> </ul> <code>0.0</code> <code>fit_intercept</code> <code>bool</code> <p>Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).</p> <code>True</code> <code>copy_X</code> <code>bool</code> <p>If True, <code>X</code> will be copied; else, it may be overwritten.</p> <code>True</code> <code>positive</code> <code>bool</code> <p>When set to True, forces the coefficients to be positive.</p> <code>False</code> <code>method</code> <code>Literal[SLSQP, TNC, L - BFGS - B]</code> <p>Type of solver to use for optimization.</p> <code>\"SLSQP\"</code> <p>Attributes:</p> Name Type Description <code>coef_</code> <code>np.ndarray of shape (n_features,)</code> <p>Estimated coefficients of the model.</p> <code>intercept_</code> <code>float</code> <p>Independent term in the linear model. Set to 0.0 if <code>fit_intercept = False</code>.</p> <code>n_features_in_</code> <code>int</code> <p>Number of features seen during <code>fit</code>.</p> Source code in <code>sklego/linear_model.py</code> <pre><code>class BaseScipyMinimizeRegressor(BaseEstimator, RegressorMixin, ABC):\n    \"\"\"Abstract base class for regressors relying on Scipy's\n    [minimize method](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) to minimize a\n    (custom) loss function.\n\n    Derive a class from this one and give it the function to be minimized. The derived class should implement the\n    `_get_objective` method, which should return the loss function and its gradient.\n\n    !!! info\n        This implementation uses\n        [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html).\n\n    Parameters\n    ----------\n    alpha : float, default=0.0\n        Constant that multiplies the penalty terms.\n    l1_ratio : float, default=0.0\n        The ElasticNet mixing parameter, with `0 &lt;= l1_ratio &lt;= 1`:\n\n        - `l1_ratio = 0` is equivalent to an L2 penalty.\n        - `l1_ratio = 1` is equivalent to an L1 penalty.\n        - `0 &lt; l1_ratio &lt; 1` is the combination of L1 and L2.\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n    copy_X : bool, default=True\n        If True, `X` will be copied; else, it may be overwritten.\n    positive : bool, default=False\n        When set to True, forces the coefficients to be positive.\n    method : Literal[\"SLSQP\", \"TNC\", \"L-BFGS-B\"], default=\"SLSQP\"\n        Type of solver to use for optimization.\n\n    Attributes\n    ----------\n    coef_ : np.ndarray of shape (n_features,)\n        Estimated coefficients of the model.\n    intercept_ : float\n        Independent term in the linear model. Set to 0.0 if `fit_intercept = False`.\n    n_features_in_ : int\n        Number of features seen during `fit`.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha=0.0,\n        l1_ratio=0.0,\n        fit_intercept=True,\n        copy_X=True,\n        positive=False,\n        method=\"SLSQP\",\n    ):\n        self.alpha = alpha\n        self.l1_ratio = l1_ratio\n        self.fit_intercept = fit_intercept\n        self.copy_X = copy_X\n        self.positive = positive\n        if method not in (\"SLSQP\", \"TNC\", \"L-BFGS-B\"):\n            raise ValueError(f'method should be one of \"SLSQP\", \"TNC\", \"L-BFGS-B\", ' f\"got {method} instead\")\n        self.method = method\n\n    @abstractmethod\n    def _get_objective(self, X, y, sample_weight):\n        \"\"\"Produce the loss function to be minimized, and its gradient to speed up computations.\n\n        Parameters\n        ----------\n        X : np.ndarray of shape (n_samples, n_features)\n            The training data.\n        y : np.ndarray of shape (n_samples,)\n            The target values.\n        sample_weight : np.ndarray of shape (n_samples,) | None, default=None\n            Individual weights for each sample.\n\n        Returns\n        -------\n        loss : Callable[[np.ndarray], float]\n            The loss function to be minimized.\n        grad_loss : Callable[[np.ndarray], np.ndarray]\n            The gradient of the loss function. Speeds up finding the minimum.\n        \"\"\"\n        ...\n\n    def _regularized_loss(self, params):\n        return +self.alpha * self.l1_ratio * np.sum(np.abs(params)) + 0.5 * self.alpha * (1 - self.l1_ratio) * np.sum(\n            params**2\n        )\n\n    def _regularized_grad_loss(self, params):\n        return +self.alpha * self.l1_ratio * np.sign(params) + self.alpha * (1 - self.l1_ratio) * params\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the linear model on training data `X` and `y` by optimizing the loss function using gradient descent.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n        sample_weight : array-like of shape (n_samples,) | None, default=None\n            Individual weights for each sample.\n\n        Returns\n        -------\n        self : BaseScipyMinimizeRegressor\n            Fitted linear model.\n        \"\"\"\n        X_, grad_loss, loss = self._prepare_inputs(X, sample_weight, y)\n\n        d = X_.shape[1] - self.n_features_in_  # This is either zero or one.\n        bounds = self.n_features_in_ * [(0, np.inf)] + d * [(-np.inf, np.inf)] if self.positive else None\n        minimize_result = minimize(\n            loss,\n            x0=np.zeros(self.n_features_in_ + d),\n            bounds=bounds,\n            method=self.method,\n            jac=grad_loss,\n            tol=1e-20,\n        )\n        self.convergence_status_ = minimize_result.message\n\n        if self.fit_intercept:\n            *self.coef_, self.intercept_ = minimize_result.x\n        else:\n            self.coef_ = minimize_result.x\n            self.intercept_ = 0.0\n\n        self.coef_ = np.array(self.coef_)\n\n        return self\n\n    def _prepare_inputs(self, X, sample_weight, y):\n        \"\"\"Prepare the inputs for the optimization problem.\n\n        This method is called by `fit` to prepare the inputs for the optimization problem. It adds an intercept column\n        to `X` if `fit_intercept=True`, and returns the loss function and its gradient.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n        sample_weight = _check_sample_weight(sample_weight, X)\n        self.n_features_in_ = X.shape[1]\n\n        n = X.shape[0]\n        if self.copy_X:\n            X_ = X.copy()\n        else:\n            X_ = X\n        if self.fit_intercept:\n            X_ = np.hstack([X_, np.ones(shape=(n, 1))])\n\n        loss, grad_loss = self._get_objective(X_, y, sample_weight)\n\n        return X_, grad_loss, loss\n\n    def predict(self, X):\n        \"\"\"Predict target values for `X` using fitted linear model by multiplying `X` with the learned coefficients.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X)\n\n        return X @ self.coef_ + self.intercept_\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.BaseScipyMinimizeRegressor.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit the linear model on training data <code>X</code> and <code>y</code> by optimizing the loss function using gradient descent.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <code>sample_weight</code> <code>array-like of shape (n_samples,) | None</code> <p>Individual weights for each sample.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>BaseScipyMinimizeRegressor</code> <p>Fitted linear model.</p> Source code in <code>sklego/linear_model.py</code> <pre><code>def fit(self, X, y, sample_weight=None):\n    \"\"\"Fit the linear model on training data `X` and `y` by optimizing the loss function using gradient descent.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n    sample_weight : array-like of shape (n_samples,) | None, default=None\n        Individual weights for each sample.\n\n    Returns\n    -------\n    self : BaseScipyMinimizeRegressor\n        Fitted linear model.\n    \"\"\"\n    X_, grad_loss, loss = self._prepare_inputs(X, sample_weight, y)\n\n    d = X_.shape[1] - self.n_features_in_  # This is either zero or one.\n    bounds = self.n_features_in_ * [(0, np.inf)] + d * [(-np.inf, np.inf)] if self.positive else None\n    minimize_result = minimize(\n        loss,\n        x0=np.zeros(self.n_features_in_ + d),\n        bounds=bounds,\n        method=self.method,\n        jac=grad_loss,\n        tol=1e-20,\n    )\n    self.convergence_status_ = minimize_result.message\n\n    if self.fit_intercept:\n        *self.coef_, self.intercept_ = minimize_result.x\n    else:\n        self.coef_ = minimize_result.x\n        self.intercept_ = 0.0\n\n    self.coef_ = np.array(self.coef_)\n\n    return self\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.BaseScipyMinimizeRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predict target values for <code>X</code> using fitted linear model by multiplying <code>X</code> with the learned coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted data.</p> Source code in <code>sklego/linear_model.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict target values for `X` using fitted linear model by multiplying `X` with the learned coefficients.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted data.\n    \"\"\"\n    check_is_fitted(self)\n    X = check_array(X)\n\n    return X @ self.coef_ + self.intercept_\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.ImbalancedLinearRegression","title":"<code>sklego.linear_model.ImbalancedLinearRegression</code>","text":"<p>             Bases: <code>BaseScipyMinimizeRegressor</code></p> <p>Linear regression where overestimating is <code>overestimation_punishment_factor</code> times worse than underestimating.</p> <p>A value of <code>overestimation_punishment_factor=5</code> implies that overestimations by the model are penalized with a factor of 5 while underestimations have a default factor of 1. The formula optimized for is</p> \\[\\frac{1}{2 N} \\|s \\circ (y - Xw) \\|_2^2 + \\alpha \\cdot l_1 \\cdot\\|w\\|_1 + \\frac{\\alpha}{2} \\cdot (1-l_1)\\cdot \\|w\\|_2^2\\] <p>where \\(\\circ\\) is component-wise multiplication and</p> \\[ s = \\begin{cases} \\text{overestimation_punishment_factor} &amp; \\text{if } y - Xw &lt; 0 \\\\ 1 &amp; \\text{otherwise} \\end{cases} \\] <p><code>ImbalancedLinearRegression</code> fits a linear model to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Compared to normal linear regression, this approach allows for a different treatment of over or under estimations.</p> <p>Info</p> <p>This implementation uses scipy.optimize.minimize.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Constant that multiplies the penalty terms.</p> <code>0.0</code> <code>l1_ratio</code> <code>float</code> <p>The ElasticNet mixing parameter, with <code>0 &lt;= l1_ratio &lt;= 1</code>:</p> <ul> <li><code>l1_ratio = 0</code> is equivalent to an L2 penalty.</li> <li><code>l1_ratio = 1</code> is equivalent to an L1 penalty.</li> <li><code>0 &lt; l1_ratio &lt; 1</code> is the combination of L1 and L2.</li> </ul> <code>0.0</code> <code>fit_intercept</code> <code>bool</code> <p>Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).</p> <code>True</code> <code>copy_X</code> <code>bool</code> <p>If True, <code>X</code> will be copied; else, it may be overwritten.</p> <code>True</code> <code>positive</code> <code>bool</code> <p>When set to True, forces the coefficients to be positive.</p> <code>False</code> <code>method</code> <code>Literal[SLSQP, TNC, L - BFGS - B]</code> <p>Type of solver to use for optimization.</p> <code>\"SLSQP\"</code> <code>overestimation_punishment_factor</code> <code>float</code> <p>Factor to punish overestimations more (if the value is larger than 1) or less (if the value is between 0 and 1).</p> <code>1.0</code> <p>Attributes:</p> Name Type Description <code>coef_</code> <code>np.ndarray of shape (n_features,)</code> <p>Estimated coefficients of the model.</p> <code>intercept_</code> <code>float</code> <p>Independent term in the linear model. Set to 0.0 if <code>fit_intercept = False</code>.</p> <code>n_features_in_</code> <code>int</code> <p>Number of features seen during <code>fit</code>.</p> <p>Examples:</p> <pre><code>import numpy as np\nfrom sklego.linear_model import ImbalancedLinearRegression\n\nnp.random.seed(0)\nX = np.random.randn(100, 4)\ny = X @ np.array([1, 2, 3, 4]) + 2*np.random.randn(100)\n\nover_bad = ImbalancedLinearRegression(overestimation_punishment_factor=50).fit(X, y)\nover_bad.coef_\n# array([0.36267036, 1.39526844, 3.4247146 , 3.93679175])\n\nunder_bad = ImbalancedLinearRegression(overestimation_punishment_factor=0.01).fit(X, y)\nunder_bad.coef_\n# array([0.73519586, 1.28698197, 2.61362614, 4.35989806])\n</code></pre> Source code in <code>sklego/linear_model.py</code> <pre><code>class ImbalancedLinearRegression(BaseScipyMinimizeRegressor):\n    r\"\"\"Linear regression where overestimating is `overestimation_punishment_factor` times worse than underestimating.\n\n    A value of `overestimation_punishment_factor=5` implies that overestimations by the model are penalized with a\n    factor of 5 while underestimations have a default factor of 1. The formula optimized for is\n\n    $$\\frac{1}{2 N} \\|s \\circ (y - Xw) \\|_2^2 + \\alpha \\cdot l_1 \\cdot\\|w\\|_1 + \\frac{\\alpha}{2} \\cdot (1-l_1)\\cdot\n    \\|w\\|_2^2$$\n\n    where $\\circ$ is component-wise multiplication and\n\n    $$ s = \\begin{cases}\n    \\text{overestimation_punishment_factor} &amp; \\text{if } y - Xw &lt; 0 \\\\\n    1 &amp; \\text{otherwise}\n    \\end{cases}\n    $$\n\n    `ImbalancedLinearRegression` fits a linear model to minimize the residual sum of squares between the observed\n    targets in the dataset, and the targets predicted by the linear approximation.\n    Compared to normal linear regression, this approach allows for a different treatment of over or under estimations.\n\n    !!! info\n        This implementation uses\n        [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html).\n\n    Parameters\n    ----------\n    alpha : float, default=0.0\n        Constant that multiplies the penalty terms.\n    l1_ratio : float, default=0.0\n        The ElasticNet mixing parameter, with `0 &lt;= l1_ratio &lt;= 1`:\n\n        - `l1_ratio = 0` is equivalent to an L2 penalty.\n        - `l1_ratio = 1` is equivalent to an L1 penalty.\n        - `0 &lt; l1_ratio &lt; 1` is the combination of L1 and L2.\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n    copy_X : bool, default=True\n        If True, `X` will be copied; else, it may be overwritten.\n    positive : bool, default=False\n        When set to True, forces the coefficients to be positive.\n    method : Literal[\"SLSQP\", \"TNC\", \"L-BFGS-B\"], default=\"SLSQP\"\n        Type of solver to use for optimization.\n    overestimation_punishment_factor : float, default=1.0\n        Factor to punish overestimations more (if the value is larger than 1) or less (if the value is between 0 and 1).\n\n    Attributes\n    ----------\n    coef_ : np.ndarray of shape (n_features,)\n        Estimated coefficients of the model.\n    intercept_ : float\n        Independent term in the linear model. Set to 0.0 if `fit_intercept = False`.\n    n_features_in_ : int\n        Number of features seen during `fit`.\n\n    Examples\n    --------\n    ```py\n    import numpy as np\n    from sklego.linear_model import ImbalancedLinearRegression\n\n    np.random.seed(0)\n    X = np.random.randn(100, 4)\n    y = X @ np.array([1, 2, 3, 4]) + 2*np.random.randn(100)\n\n    over_bad = ImbalancedLinearRegression(overestimation_punishment_factor=50).fit(X, y)\n    over_bad.coef_\n    # array([0.36267036, 1.39526844, 3.4247146 , 3.93679175])\n\n    under_bad = ImbalancedLinearRegression(overestimation_punishment_factor=0.01).fit(X, y)\n    under_bad.coef_\n    # array([0.73519586, 1.28698197, 2.61362614, 4.35989806])\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha=0.0,\n        l1_ratio=0.0,\n        fit_intercept=True,\n        copy_X=True,\n        positive=False,\n        method=\"SLSQP\",\n        overestimation_punishment_factor=1.0,\n    ):\n        super().__init__(alpha, l1_ratio, fit_intercept, copy_X, positive, method)\n        self.overestimation_punishment_factor = overestimation_punishment_factor\n\n    def _get_objective(self, X, y, sample_weight):\n        def imbalanced_loss(params):\n            return 0.5 * np.mean(\n                sample_weight\n                * np.where(X @ params &gt; y, self.overestimation_punishment_factor, 1)\n                * np.square(y - X @ params)\n            ) + self._regularized_loss(params)\n\n        def grad_imbalanced_loss(params):\n            return (\n                -(sample_weight * np.where(X @ params &gt; y, self.overestimation_punishment_factor, 1) * (y - X @ params))\n                @ X\n                / X.shape[0]\n            ) + self._regularized_grad_loss(params)\n\n        return imbalanced_loss, grad_imbalanced_loss\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.QuantileRegression","title":"<code>sklego.linear_model.QuantileRegression</code>","text":"<p>             Bases: <code>BaseScipyMinimizeRegressor</code></p> <p>Compute quantile regression. This can be used for computing confidence intervals of linear regressions.</p> <p><code>QuantileRegression</code> fits a linear model to minimize a weighted residual sum of absolute deviations between the observed targets in the dataset and the targets predicted by the linear approximation, i.e.</p> \\[\\frac{\\text{switch} \\cdot ||y - Xw||_1}{2 N} + \\alpha \\cdot l_1 \\cdot ||w||_1     + \\frac{\\alpha}{2} \\cdot (1 - l_1) \\cdot ||w||^2_2\\] <p>where</p> \\[\\text{switch} = \\begin{cases} \\text{quantile} &amp; \\text{if } y - Xw &lt; 0 \\\\ 1-\\text{quantile} &amp; \\text{otherwise} \\end{cases}\\] <p>The regressor defaults to <code>LADRegression</code> for its default value of <code>quantile=0.5</code>.</p> <p>Compared to linear regression, this approach is robust to outliers.</p> <p>Info</p> <p>This implementation uses scipy.optimize.minimize.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Constant that multiplies the penalty terms.</p> <code>0.0</code> <code>l1_ratio</code> <code>float</code> <p>The ElasticNet mixing parameter, with <code>0 &lt;= l1_ratio &lt;= 1</code>:</p> <ul> <li><code>l1_ratio = 0</code> is equivalent to an L2 penalty.</li> <li><code>l1_ratio = 1</code> is equivalent to an L1 penalty.</li> <li><code>0 &lt; l1_ratio &lt; 1</code> is the combination of L1 and L2.</li> </ul> <code>0.0</code> <code>fit_intercept</code> <code>bool</code> <p>Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).</p> <code>True</code> <code>copy_X</code> <code>bool</code> <p>If True, <code>X</code> will be copied; else, it may be overwritten.</p> <code>True</code> <code>positive</code> <code>bool</code> <p>When set to True, forces the coefficients to be positive.</p> <code>False</code> <code>method</code> <code>Literal[SLSQP, TNC, L - BFGS - B]</code> <p>Type of solver to use for optimization.</p> <code>\"SLSQP\"</code> <code>quantile</code> <code>float</code> <p>The line output by the model will have a share of approximately <code>quantile</code> data points under it. It  should be a value between 0 and 1.</p> <p>A value of <code>quantile=1</code> outputs a line that is above each data point, for example. <code>quantile=0.5</code> corresponds to LADRegression.</p> <code>0.5</code> <p>Attributes:</p> Name Type Description <code>coef_</code> <code>np.ndarray of shape (n_features,)</code> <p>Estimated coefficients of the model.</p> <code>intercept_</code> <code>float</code> <p>Independent term in the linear model. Set to 0.0 if <code>fit_intercept = False</code>.</p> <code>n_features_in_</code> <code>int</code> <p>Number of features seen during <code>fit</code>.</p> <p>Examples:</p> <pre><code>import numpy as np\nfrom sklego.linear_model import QuantileRegression\n\nnp.random.seed(0)\nX = np.random.randn(100, 4)\ny = X @ np.array([1, 2, 3, 4])\n\nmodel = QuantileRegression().fit(X, y)\nmodel.coef_\n# array([1., 2., 3., 4.])\n\ny = X @ np.array([-1, 2, -3, 4])\nmodel = QuantileRegression(quantile=0.8).fit(X, y)\nmodel.coef_\n# array([-1.,  2., -3.,  4.])\n</code></pre> Source code in <code>sklego/linear_model.py</code> <pre><code>class QuantileRegression(BaseScipyMinimizeRegressor):\n    r\"\"\"Compute quantile regression. This can be used for computing confidence intervals of linear regressions.\n\n    `QuantileRegression` fits a linear model to minimize a weighted residual sum of absolute deviations between\n    the observed targets in the dataset and the targets predicted by the linear approximation, i.e.\n\n    $$\\frac{\\text{switch} \\cdot ||y - Xw||_1}{2 N} + \\alpha \\cdot l_1 \\cdot ||w||_1\n        + \\frac{\\alpha}{2} \\cdot (1 - l_1) \\cdot ||w||^2_2$$\n\n    where\n\n    $$\\text{switch} = \\begin{cases}\n    \\text{quantile} &amp; \\text{if } y - Xw &lt; 0 \\\\\n    1-\\text{quantile} &amp; \\text{otherwise}\n    \\end{cases}$$\n\n    The regressor defaults to `LADRegression` for its default value of `quantile=0.5`.\n\n    Compared to linear regression, this approach is robust to outliers.\n\n    !!! info\n        This implementation uses\n        [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html).\n\n    Parameters\n    ----------\n    alpha : float, default=0.0\n        Constant that multiplies the penalty terms.\n    l1_ratio : float, default=0.0\n        The ElasticNet mixing parameter, with `0 &lt;= l1_ratio &lt;= 1`:\n\n        - `l1_ratio = 0` is equivalent to an L2 penalty.\n        - `l1_ratio = 1` is equivalent to an L1 penalty.\n        - `0 &lt; l1_ratio &lt; 1` is the combination of L1 and L2.\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n    copy_X : bool, default=True\n        If True, `X` will be copied; else, it may be overwritten.\n    positive : bool, default=False\n        When set to True, forces the coefficients to be positive.\n    method : Literal[\"SLSQP\", \"TNC\", \"L-BFGS-B\"], default=\"SLSQP\"\n        Type of solver to use for optimization.\n    quantile : float, default=0.5\n        The line output by the model will have a share of approximately `quantile` data points under it. It  should\n        be a value between 0 and 1.\n\n        A value of `quantile=1` outputs a line that is above each data point, for example.\n        `quantile=0.5` corresponds to LADRegression.\n\n    Attributes\n    ----------\n    coef_ : np.ndarray of shape (n_features,)\n        Estimated coefficients of the model.\n    intercept_ : float\n        Independent term in the linear model. Set to 0.0 if `fit_intercept = False`.\n    n_features_in_ : int\n        Number of features seen during `fit`.\n\n    Examples\n    --------\n    ```py\n    import numpy as np\n    from sklego.linear_model import QuantileRegression\n\n    np.random.seed(0)\n    X = np.random.randn(100, 4)\n    y = X @ np.array([1, 2, 3, 4])\n\n    model = QuantileRegression().fit(X, y)\n    model.coef_\n    # array([1., 2., 3., 4.])\n\n    y = X @ np.array([-1, 2, -3, 4])\n    model = QuantileRegression(quantile=0.8).fit(X, y)\n    model.coef_\n    # array([-1.,  2., -3.,  4.])\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha=0.0,\n        l1_ratio=0.0,\n        fit_intercept=True,\n        copy_X=True,\n        positive=False,\n        method=\"SLSQP\",\n        quantile=0.5,\n    ):\n        super().__init__(alpha, l1_ratio, fit_intercept, copy_X, positive, method)\n        self.quantile = quantile\n\n    def _get_objective(self, X, y, sample_weight):\n        def quantile_loss(params):\n            return np.mean(\n                sample_weight * np.where(X @ params &lt; y, self.quantile, 1 - self.quantile) * np.abs(y - X @ params)\n            ) + self._regularized_loss(params)\n\n        def grad_quantile_loss(params):\n            return (\n                -(sample_weight * np.where(X @ params &lt; y, self.quantile, 1 - self.quantile) * np.sign(y - X @ params))\n                @ X\n                / X.shape[0]\n            ) + self._regularized_grad_loss(params)\n\n        return quantile_loss, grad_quantile_loss\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimator on training data `X` and `y` by minimizing the quantile loss function.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n        sample_weight : array-like of shape (n_samples,) | None, default=None\n            Individual weights for each sample.\n\n        Returns\n        -------\n        self : QuantileRegression\n            The fitted estimator.\n\n        Raises\n        ------\n        ValueError\n            If `quantile` is not between 0 and 1.\n        \"\"\"\n        if 0 &lt;= self.quantile &lt;= 1:\n            super().fit(X, y, sample_weight)\n        else:\n            raise ValueError(\"Parameter `quantile` should be between zero and one.\")\n\n        return self\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.QuantileRegression.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit the estimator on training data <code>X</code> and <code>y</code> by minimizing the quantile loss function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <code>sample_weight</code> <code>array-like of shape (n_samples,) | None</code> <p>Individual weights for each sample.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>QuantileRegression</code> <p>The fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>quantile</code> is not between 0 and 1.</p> Source code in <code>sklego/linear_model.py</code> <pre><code>def fit(self, X, y, sample_weight=None):\n    \"\"\"Fit the estimator on training data `X` and `y` by minimizing the quantile loss function.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n    sample_weight : array-like of shape (n_samples,) | None, default=None\n        Individual weights for each sample.\n\n    Returns\n    -------\n    self : QuantileRegression\n        The fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        If `quantile` is not between 0 and 1.\n    \"\"\"\n    if 0 &lt;= self.quantile &lt;= 1:\n        super().fit(X, y, sample_weight)\n    else:\n        raise ValueError(\"Parameter `quantile` should be between zero and one.\")\n\n    return self\n</code></pre>"},{"location":"api/linear-model/#sklego.linear_model.LADRegression","title":"<code>sklego.linear_model.LADRegression</code>","text":"<p>             Bases: <code>QuantileRegression</code></p> <p>Least absolute deviation Regression.</p> <p><code>LADRegression</code> fits a linear model to minimize the residual sum of absolute deviations between the observed targets in the dataset, and the targets predicted by the linear approximation, i.e.</p> \\[\\frac{1}{N}\\|y - Xw \\|_1 + \\alpha \\cdot l_1 \\cdot\\|w\\|_1 + \\frac{\\alpha}{2} \\cdot (1-l_1)\\cdot \\|w\\|^2_2\\] <p>Compared to linear regression, this approach is robust to outliers. You can even optimize for the lowest MAPE (Mean Average Percentage Error), by providing <code>sample_weight=np.abs(1/y_train)</code> when fitting the regressor.</p> <p>Info</p> <p>This implementation uses scipy.optimize.minimize.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Constant that multiplies the penalty terms.</p> <code>0.0</code> <code>l1_ratio</code> <code>float</code> <p>The ElasticNet mixing parameter, with <code>0 &lt;= l1_ratio &lt;= 1</code>:</p> <ul> <li><code>l1_ratio = 0</code> is equivalent to an L2 penalty.</li> <li><code>l1_ratio = 1</code> is equivalent to an L1 penalty.</li> <li><code>0 &lt; l1_ratio &lt; 1</code> is the combination of L1 and L2.</li> </ul> <code>0.0</code> <code>fit_intercept</code> <code>bool</code> <p>Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).</p> <code>True</code> <code>copy_X</code> <code>bool</code> <p>If True, <code>X</code> will be copied; else, it may be overwritten.</p> <code>True</code> <code>positive</code> <code>bool</code> <p>When set to True, forces the coefficients to be positive.</p> <code>False</code> <code>method</code> <code>Literal[SLSQP, TNC, L - BFGS - B]</code> <p>Type of solver to use for optimization.</p> <code>\"SLSQP\"</code> <code>quantile</code> <code>float</code> <p>The line output by the model will have a share of approximately <code>quantile</code> data points under it. It  should be a value between 0 and 1.</p> <p>A value of <code>quantile=1</code> outputs a line that is above each data point, for example. <code>quantile=0.5</code> corresponds to LADRegression.</p> <code>0.5</code> <p>Attributes:</p> Name Type Description <code>coef_</code> <code>np.ndarray of shape (n_features,)</code> <p>Estimated coefficients of the model.</p> <code>intercept_</code> <code>float</code> <p>Independent term in the linear model. Set to 0.0 if <code>fit_intercept = False</code>.</p> <code>n_features_in_</code> <code>int</code> <p>Number of features seen during <code>fit</code>.</p> <p>Examples:</p> <pre><code>import numpy as np\nfrom sklego.linear_model import LADRegression\n\nnp.random.seed(0)\nX = np.random.randn(100, 4)\ny = X @ np.array([1, 2, 3, 4])\n\nmodel = LADRegression().fit(X, y)\nmodel.coef_\n# array([1., 2., 3., 4.])\n\ny = X @ np.array([-1, 2, -3, 4])\nmodel = LADRegression(positive=True).fit(X, y)\nmodel.coef_\n# array([7.39575926e-18, 1.42423304e+00, 2.80467827e-17, 4.29789588e+00])\n</code></pre> Source code in <code>sklego/linear_model.py</code> <pre><code>class LADRegression(QuantileRegression):\n    r\"\"\"Least absolute deviation Regression.\n\n    `LADRegression` fits a linear model to minimize the residual sum of absolute deviations between the observed targets\n    in the dataset, and the targets predicted by the linear approximation, i.e.\n\n    $$\\frac{1}{N}\\|y - Xw \\|_1 + \\alpha \\cdot l_1 \\cdot\\|w\\|_1 + \\frac{\\alpha}{2} \\cdot (1-l_1)\\cdot \\|w\\|^2_2$$\n\n    Compared to linear regression, this approach is robust to outliers. You can even optimize for the lowest MAPE\n    (Mean Average Percentage Error), by providing `sample_weight=np.abs(1/y_train)` when fitting the regressor.\n\n    !!! info\n        This implementation uses\n        [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html).\n\n    Parameters\n    ----------\n    alpha : float, default=0.0\n        Constant that multiplies the penalty terms.\n    l1_ratio : float, default=0.0\n        The ElasticNet mixing parameter, with `0 &lt;= l1_ratio &lt;= 1`:\n\n        - `l1_ratio = 0` is equivalent to an L2 penalty.\n        - `l1_ratio = 1` is equivalent to an L1 penalty.\n        - `0 &lt; l1_ratio &lt; 1` is the combination of L1 and L2.\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n    copy_X : bool, default=True\n        If True, `X` will be copied; else, it may be overwritten.\n    positive : bool, default=False\n        When set to True, forces the coefficients to be positive.\n    method : Literal[\"SLSQP\", \"TNC\", \"L-BFGS-B\"], default=\"SLSQP\"\n        Type of solver to use for optimization.\n    quantile : float, default=0.5\n        The line output by the model will have a share of approximately `quantile` data points under it. It  should\n        be a value between 0 and 1.\n\n        A value of `quantile=1` outputs a line that is above each data point, for example.\n        `quantile=0.5` corresponds to LADRegression.\n\n    Attributes\n    ----------\n    coef_ : np.ndarray of shape (n_features,)\n        Estimated coefficients of the model.\n    intercept_ : float\n        Independent term in the linear model. Set to 0.0 if `fit_intercept = False`.\n    n_features_in_ : int\n        Number of features seen during `fit`.\n\n    Examples\n    --------\n    ```py\n    import numpy as np\n    from sklego.linear_model import LADRegression\n\n    np.random.seed(0)\n    X = np.random.randn(100, 4)\n    y = X @ np.array([1, 2, 3, 4])\n\n    model = LADRegression().fit(X, y)\n    model.coef_\n    # array([1., 2., 3., 4.])\n\n    y = X @ np.array([-1, 2, -3, 4])\n    model = LADRegression(positive=True).fit(X, y)\n    model.coef_\n    # array([7.39575926e-18, 1.42423304e+00, 2.80467827e-17, 4.29789588e+00])\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha=0.0,\n        l1_ratio=0.0,\n        fit_intercept=True,\n        copy_X=True,\n        positive=False,\n        method=\"SLSQP\",\n    ):\n        super().__init__(alpha, l1_ratio, fit_intercept, copy_X, positive, method, quantile=0.5)\n</code></pre>"},{"location":"api/meta/","title":"Meta Models","text":""},{"location":"api/meta/#sklego.meta.confusion_balancer.ConfusionBalancer","title":"<code>sklego.meta.confusion_balancer.ConfusionBalancer</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code>, <code>MetaEstimatorMixin</code></p> <p>The <code>ConfusionBalancer</code> estimator attempts to give it's child estimator a more balanced output by learning from the confusion matrix during training.</p> <p>The idea is that the confusion matrix calculates \\(P(C_i | M_i)\\) where \\(C_i\\) is the actual class and \\(M_i\\) is the class that the underlying model gives. We use these probabilities to attempt a more balanced prediction by averaging the correction from the confusion matrix with the original probabilities.</p> \\[P(\\text{class}_j) = \\alpha P(\\text{model}_j) + (1-\\alpha) P(\\text{class}_j | \\text{model}_j) P(\\text{model}_j)\\] <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>scikit-learn compatible classifier</code> <p>The estimator to be wrapped, it must implement a <code>predict_proba</code> method.</p> required <code>alpha</code> <code>float</code> <p>Hyperparameter which determines how much smoothing to apply. Must be between 0 and 1.</p> <code>0.5</code> <code>cfm_smooth</code> <code>float</code> <p>Smoothing parameter for the confusion matrices to ensure zeros don't exist.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>classes_</code> <code>array-like of shape (n_classes,)</code> <p>The target class labels.</p> <code>cfm_</code> <code>array-like of shape (n_classes, n_classes)</code> <p>The confusion matrix used for the correction.</p> Source code in <code>sklego/meta/confusion_balancer.py</code> <pre><code>class ConfusionBalancer(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):\n    r\"\"\"The `ConfusionBalancer` estimator attempts to give it's child estimator a more balanced output by learning from\n    the confusion matrix during training.\n\n    The idea is that the confusion matrix calculates $P(C_i | M_i)$ where $C_i$ is the actual class and $M_i$ is the\n    class that the underlying model gives. We use these probabilities to attempt a more balanced prediction by averaging\n    the correction from the confusion matrix with the original probabilities.\n\n    $$P(\\text{class}_j) = \\alpha P(\\text{model}_j) + (1-\\alpha) P(\\text{class}_j | \\text{model}_j) P(\\text{model}_j)$$\n\n    Parameters\n    ----------\n    estimator : scikit-learn compatible classifier\n        The estimator to be wrapped, it must implement a `predict_proba` method.\n    alpha : float, default=0.5\n        Hyperparameter which determines how much smoothing to apply. Must be between 0 and 1.\n    cfm_smooth : float, default=0\n        Smoothing parameter for the confusion matrices to ensure zeros don't exist.\n\n    Attributes\n    ----------\n    classes_ : array-like of shape (n_classes,)\n        The target class labels.\n    cfm_ : array-like of shape (n_classes, n_classes)\n        The confusion matrix used for the correction.\n    \"\"\"\n\n    def __init__(self, estimator, alpha: float = 0.5, cfm_smooth=0):\n        self.estimator = estimator\n        self.alpha = alpha\n        self.cfm_smooth = cfm_smooth\n\n    def fit(self, X, y):\n        \"\"\"Fit the underlying estimator on the training data `X` and `y`, it calculates the confusion matrix,\n        normalizes it and stores it for later use.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : ConfusionBalancer\n            The fitted estimator.\n\n        Raises\n        ------\n        ValueError\n            If the underlying estimator does not have a `predict_proba` method.\n        \"\"\"\n\n        X, y = check_X_y(X, y, estimator=self.estimator, dtype=FLOAT_DTYPES)\n        if not isinstance(self.estimator, ProbabilisticClassifier):\n            raise ValueError(\n                \"The ConfusionBalancer meta model only works on classification models with .predict_proba.\"\n            )\n        self.estimator.fit(X, y)\n        self.classes_ = unique_labels(y)\n        cfm = confusion_matrix(y, self.estimator.predict(X)).T + self.cfm_smooth\n        self.cfm_ = cfm / cfm.sum(axis=1).reshape(-1, 1)\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Predict probabilities for new data `X` using the underlying estimator and then applying the confusion matrix\n        correction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_classes)\n            The predicted values.\n        \"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        preds = self.estimator.predict_proba(X)\n        return (1 - self.alpha) * preds + self.alpha * preds @ self.cfm_\n\n    def predict(self, X):\n        \"\"\"Predict most likely class for new data `X` using the underlying estimator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        check_is_fitted(self, [\"cfm_\", \"classes_\"])\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        return self.classes_[self.predict_proba(X).argmax(axis=1)]\n</code></pre>"},{"location":"api/meta/#sklego.meta.confusion_balancer.ConfusionBalancer.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the underlying estimator on the training data <code>X</code> and <code>y</code>, it calculates the confusion matrix, normalizes it and stores it for later use.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>ConfusionBalancer</code> <p>The fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the underlying estimator does not have a <code>predict_proba</code> method.</p> Source code in <code>sklego/meta/confusion_balancer.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the underlying estimator on the training data `X` and `y`, it calculates the confusion matrix,\n    normalizes it and stores it for later use.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training data.\n    y : array-like of shape (n_samples,)\n        Target values.\n\n    Returns\n    -------\n    self : ConfusionBalancer\n        The fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        If the underlying estimator does not have a `predict_proba` method.\n    \"\"\"\n\n    X, y = check_X_y(X, y, estimator=self.estimator, dtype=FLOAT_DTYPES)\n    if not isinstance(self.estimator, ProbabilisticClassifier):\n        raise ValueError(\n            \"The ConfusionBalancer meta model only works on classification models with .predict_proba.\"\n        )\n    self.estimator.fit(X, y)\n    self.classes_ = unique_labels(y)\n    cfm = confusion_matrix(y, self.estimator.predict(X)).T + self.cfm_smooth\n    self.cfm_ = cfm / cfm.sum(axis=1).reshape(-1, 1)\n    return self\n</code></pre>"},{"location":"api/meta/#sklego.meta.confusion_balancer.ConfusionBalancer.predict","title":"<code>predict(X)</code>","text":"<p>Predict most likely class for new data <code>X</code> using the underlying estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted values.</p> Source code in <code>sklego/meta/confusion_balancer.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict most likely class for new data `X` using the underlying estimator.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted values.\n    \"\"\"\n    check_is_fitted(self, [\"cfm_\", \"classes_\"])\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    return self.classes_[self.predict_proba(X).argmax(axis=1)]\n</code></pre>"},{"location":"api/meta/#sklego.meta.confusion_balancer.ConfusionBalancer.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict probabilities for new data <code>X</code> using the underlying estimator and then applying the confusion matrix correction.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_classes)</code> <p>The predicted values.</p> Source code in <code>sklego/meta/confusion_balancer.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict probabilities for new data `X` using the underlying estimator and then applying the confusion matrix\n    correction.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_classes)\n        The predicted values.\n    \"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    preds = self.estimator.predict_proba(X)\n    return (1 - self.alpha) * preds + self.alpha * preds @ self.cfm_\n</code></pre>"},{"location":"api/meta/#sklego.meta.decay_estimator.DecayEstimator","title":"<code>sklego.meta.decay_estimator.DecayEstimator</code>","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>Morphs an estimator such that the training weights can be adapted to ensure that points that are far away have less weight.</p> <p>This meta estimator will only work for estimators that allow a <code>sample_weights</code> argument in their <code>.fit()</code> method. The meta estimator <code>.fit()</code> method computes the weights to pass to the estimator's <code>.fit()</code> method.</p> <p>Warning</p> <p>It is up to the user to sort the dataset appropriately.</p> <p>Warning</p> <p>By default all the checks on the inputs <code>X</code> and <code>y</code> are delegated to the wrapped estimator. To change such behaviour, set <code>check_input</code> to <code>True</code>. Remark that if the check is skipped, then <code>y</code> should have a <code>shape</code> attribute, which is used to extract the number of samples in training data, and compute the weights.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>scikit-learn compatible estimator</code> <p>The estimator to be wrapped.</p> required <code>decay_func</code> <code>Literal[linear, exponential, stepwise, sigmoid] | Callable[[ndarray, ndarray, ...], ndarray]</code> <p>The decay function to use. Available built-in decay functions are:</p> <ul> <li><code>\"linear\"</code>: linear decay from <code>max_value</code> to <code>min_value</code>.</li> <li><code>\"exponential\"</code>: exponential decay with decay rate <code>decay_rate</code>.</li> <li><code>\"stepwise\"</code>: stepwise decay from <code>max_value</code> to <code>min_value</code>, with <code>n_steps</code> steps or step size <code>step_size</code>.</li> <li><code>\"sigmoid\"</code>: sigmoid decay from <code>max_value</code> to <code>min_value</code> with decay rate <code>growth_rate</code>.</li> </ul> <p>Otherwise a callable can be passed and it should accept <code>X</code>, <code>y</code> as first two positional arguments and any other keyword argument passed along from <code>decay_kwargs</code> (if any). It should compute the weights and return an array of shape <code>(n_samples,)</code>.</p> <code>\"exponential\"</code> <code>check_input</code> <code>bool</code> <p>Whether or not to check the input data. If False, the checks are delegated to the wrapped estimator.</p> <code>False</code> <code>decay_kwargs</code> <code>dict | None</code> <p>Keyword arguments to the decay function.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>estimator_</code> <code>scikit-learn compatible estimator</code> <p>The fitted estimator.</p> <code>weights_</code> <code>array-like of shape (n_samples,)</code> <p>The weights used to train the estimator.</p> <code>classes_</code> <code>array-like of shape (n_classes,)</code> <p>The classes labels. Only present if the wrapped estimator is a classifier.</p> <p>Examples:</p> <pre><code>from sklearn.linear_model import LinearRegression\nfrom sklego.meta import DecayEstimator\n\ndecay_estimator = DecayEstimator(\n    model=LinearRegression(),\n    decay_func=\"linear\",\n    min_value=0.1,\n    max_value=0.9\n    )\n\nX, y = ...\n\n# Fit the DecayEstimator on the data, this will compute the weights\n# and pass them to the wrapped estimator\n_ = decay_estimator.fit(X, y)\n\n# At prediction time, the weights are not used\npredictions = decay_estimator.predict(X)\n\n# The weights are stored in the `weights_` attribute\nweights = decay_estimator.weights_\n</code></pre> Source code in <code>sklego/meta/decay_estimator.py</code> <pre><code>class DecayEstimator(BaseEstimator):\n    \"\"\"Morphs an estimator such that the training weights can be adapted to ensure that points that are far away have\n    less weight.\n\n    This meta estimator will only work for estimators that allow a `sample_weights` argument in their `.fit()` method.\n    The meta estimator `.fit()` method computes the weights to pass to the estimator's `.fit()` method.\n\n    !!! warning\n        It is up to the user to sort the dataset appropriately.\n\n    !!! warning\n        By default all the checks on the inputs `X` and `y` are delegated to the wrapped estimator.\n        To change such behaviour, set `check_input` to `True`.\n        Remark that if the check is skipped, then `y` should have a `shape` attribute, which is\n        used to extract the number of samples in training data, and compute the weights.\n\n    Parameters\n    ----------\n    model : scikit-learn compatible estimator\n        The estimator to be wrapped.\n    decay_func : Literal[\"linear\", \"exponential\", \"stepwise\", \"sigmoid\"] | \\\n            Callable[[np.ndarray, np.ndarray, ...], np.ndarray], default=\"exponential\"\n        The decay function to use. Available built-in decay functions are:\n\n        - `\"linear\"`: linear decay from `max_value` to `min_value`.\n        - `\"exponential\"`: exponential decay with decay rate `decay_rate`.\n        - `\"stepwise\"`: stepwise decay from `max_value` to `min_value`, with `n_steps` steps or step size `step_size`.\n        - `\"sigmoid\"`: sigmoid decay from `max_value` to `min_value` with decay rate `growth_rate`.\n\n        Otherwise a callable can be passed and it should accept `X`, `y` as first two positional arguments and any other\n        keyword argument passed along from `decay_kwargs` (if any). It should compute the weights and return an array\n        of shape `(n_samples,)`.\n    check_input : bool, default=False\n        Whether or not to check the input data. If False, the checks are delegated to the wrapped estimator.\n    decay_kwargs : dict | None, default=None\n        Keyword arguments to the decay function.\n\n    Attributes\n    ----------\n    estimator_ : scikit-learn compatible estimator\n        The fitted estimator.\n    weights_ : array-like of shape (n_samples,)\n        The weights used to train the estimator.\n    classes_ : array-like of shape (n_classes,)\n        The classes labels. Only present if the wrapped estimator is a classifier.\n\n    Examples\n    --------\n    ```py\n    from sklearn.linear_model import LinearRegression\n    from sklego.meta import DecayEstimator\n\n    decay_estimator = DecayEstimator(\n        model=LinearRegression(),\n        decay_func=\"linear\",\n        min_value=0.1,\n        max_value=0.9\n        )\n\n    X, y = ...\n\n    # Fit the DecayEstimator on the data, this will compute the weights\n    # and pass them to the wrapped estimator\n    _ = decay_estimator.fit(X, y)\n\n    # At prediction time, the weights are not used\n    predictions = decay_estimator.predict(X)\n\n    # The weights are stored in the `weights_` attribute\n    weights = decay_estimator.weights_\n    ```\n    \"\"\"\n\n    _ALLOWED_DECAYS = {\n        \"linear\": linear_decay,\n        \"exponential\": exponential_decay,\n        \"stepwise\": stepwise_decay,\n        \"sigmoid\": sigmoid_decay,\n    }\n\n    def __init__(self, model, decay_func=\"exponential\", check_input=False, **decay_kwargs):\n        self.model = model\n        self.decay_func = decay_func\n        self.check_input = check_input\n        self.decay_kwargs = decay_kwargs\n\n    def _is_classifier(self):\n        \"\"\"Checks if the wrapped estimator is a classifier.\"\"\"\n        return any([\"ClassifierMixin\" in p.__name__ for p in type(self.model).__bases__])\n\n    @property\n    def _estimator_type(self):\n        \"\"\"Computes `_estimator_type` dynamically from the wrapped model.\"\"\"\n        return self.model._estimator_type\n\n    def fit(self, X, y):\n        \"\"\"Fit the underlying estimator on the training data `X` and `y` using the calculated sample weights.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : DecayEstimator\n            The fitted estimator.\n        \"\"\"\n\n        if self.check_input:\n            X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES, ensure_min_features=0)\n\n        if self.decay_func in self._ALLOWED_DECAYS.keys():\n            self.decay_func_ = self._ALLOWED_DECAYS[self.decay_func]\n        elif callable(self.decay_func):\n            self.decay_func_ = self.decay_func\n        else:\n            raise ValueError(f\"`decay_func` should be one of {self._ALLOWED_DECAYS.keys()} or a callable\")\n\n        self.weights_ = self.decay_func_(X, y, **self.decay_kwargs)\n        self.estimator_ = clone(self.model)\n\n        try:\n            self.estimator_.fit(X, y, sample_weight=self.weights_)\n        except TypeError as e:\n            if \"sample_weight\" in str(e):\n                raise TypeError(f\"Model {type(self.model).__name__}.fit() does not have 'sample_weight'\")\n\n        if self._is_classifier():\n            self.classes_ = self.estimator_.classes_\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict target values for `X` using trained underlying estimator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        if self._is_classifier():\n            check_is_fitted(self, [\"classes_\"])\n\n        check_is_fitted(self, [\"weights_\", \"estimator_\"])\n        return self.estimator_.predict(X)\n\n    def score(self, X, y):\n        \"\"\"Alias for `.score()` method of the underlying estimator.\"\"\"\n        return self.estimator_.score(X, y)\n</code></pre>"},{"location":"api/meta/#sklego.meta.decay_estimator.DecayEstimator.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the underlying estimator on the training data <code>X</code> and <code>y</code> using the calculated sample weights.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>DecayEstimator</code> <p>The fitted estimator.</p> Source code in <code>sklego/meta/decay_estimator.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the underlying estimator on the training data `X` and `y` using the calculated sample weights.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training data.\n    y : array-like of shape (n_samples,)\n        Target values.\n\n    Returns\n    -------\n    self : DecayEstimator\n        The fitted estimator.\n    \"\"\"\n\n    if self.check_input:\n        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES, ensure_min_features=0)\n\n    if self.decay_func in self._ALLOWED_DECAYS.keys():\n        self.decay_func_ = self._ALLOWED_DECAYS[self.decay_func]\n    elif callable(self.decay_func):\n        self.decay_func_ = self.decay_func\n    else:\n        raise ValueError(f\"`decay_func` should be one of {self._ALLOWED_DECAYS.keys()} or a callable\")\n\n    self.weights_ = self.decay_func_(X, y, **self.decay_kwargs)\n    self.estimator_ = clone(self.model)\n\n    try:\n        self.estimator_.fit(X, y, sample_weight=self.weights_)\n    except TypeError as e:\n        if \"sample_weight\" in str(e):\n            raise TypeError(f\"Model {type(self.model).__name__}.fit() does not have 'sample_weight'\")\n\n    if self._is_classifier():\n        self.classes_ = self.estimator_.classes_\n    return self\n</code></pre>"},{"location":"api/meta/#sklego.meta.decay_estimator.DecayEstimator.predict","title":"<code>predict(X)</code>","text":"<p>Predict target values for <code>X</code> using trained underlying estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted values.</p> Source code in <code>sklego/meta/decay_estimator.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict target values for `X` using trained underlying estimator.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted values.\n    \"\"\"\n    if self._is_classifier():\n        check_is_fitted(self, [\"classes_\"])\n\n    check_is_fitted(self, [\"weights_\", \"estimator_\"])\n    return self.estimator_.predict(X)\n</code></pre>"},{"location":"api/meta/#sklego.meta.decay_estimator.DecayEstimator.score","title":"<code>score(X, y)</code>","text":"<p>Alias for <code>.score()</code> method of the underlying estimator.</p> Source code in <code>sklego/meta/decay_estimator.py</code> <pre><code>def score(self, X, y):\n    \"\"\"Alias for `.score()` method of the underlying estimator.\"\"\"\n    return self.estimator_.score(X, y)\n</code></pre>"},{"location":"api/meta/#sklego.meta.estimator_transformer.EstimatorTransformer","title":"<code>sklego.meta.estimator_transformer.EstimatorTransformer</code>","text":"<p>             Bases: <code>TransformerMixin</code>, <code>MetaEstimatorMixin</code>, <code>BaseEstimator</code></p> <p>Allow using an estimator as a transformer in an earlier step of a pipeline.</p> <p>Warning</p> <p>By default all the checks on the inputs <code>X</code> and <code>y</code> are delegated to the wrapped estimator.</p> <p>To change such behaviour, set <code>check_input</code> to <code>True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>scikit-learn compatible estimator</code> <p>The estimator to be applied to the data, used as transformer.</p> required <code>predict_func</code> <code>str</code> <p>The method called on the estimator when transforming e.g. (<code>\"predict\"</code>, <code>\"predict_proba\"</code>).</p> <code>\"predict\"</code> <code>check_input</code> <code>bool</code> <p>Whether or not to check the input data. If False, the checks are delegated to the wrapped estimator.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>estimator_</code> <code>scikit-learn compatible estimator</code> <p>The fitted underlying estimator.</p> <code>multi_output_</code> <code>bool</code> <p>Whether or not the estimator is multi output.</p> Source code in <code>sklego/meta/estimator_transformer.py</code> <pre><code>class EstimatorTransformer(TransformerMixin, MetaEstimatorMixin, BaseEstimator):\n    \"\"\"Allow using an estimator as a transformer in an earlier step of a pipeline.\n\n    !!! warning\n        By default all the checks on the inputs `X` and `y` are delegated to the wrapped estimator.\n\n        To change such behaviour, set `check_input` to `True`.\n\n    Parameters\n    ----------\n    estimator : scikit-learn compatible estimator\n        The estimator to be applied to the data, used as transformer.\n    predict_func : str, default=\"predict\"\n        The method called on the estimator when transforming e.g. (`\"predict\"`, `\"predict_proba\"`).\n    check_input : bool, default=False\n        Whether or not to check the input data. If False, the checks are delegated to the wrapped estimator.\n\n    Attributes\n    ----------\n    estimator_ : scikit-learn compatible estimator\n        The fitted underlying estimator.\n    multi_output_ : bool\n        Whether or not the estimator is multi output.\n    \"\"\"\n\n    def __init__(self, estimator, predict_func=\"predict\", check_input=False):\n        self.estimator = estimator\n        self.predict_func = predict_func\n        self.check_input = check_input\n\n    def fit(self, X, y, **kwargs):\n        \"\"\"Fit the underlying estimator on training data `X` and `y`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,)\n            Target values.\n        **kwargs : dict\n            Additional keyword arguments passed to the `fit` method of the underlying estimator.\n\n        Returns\n        -------\n        self : EstimatorTransformer\n            The fitted transformer.\n        \"\"\"\n\n        if self.check_input:\n            X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES, multi_output=True)\n\n        self.multi_output_ = len(y.shape) &gt; 1\n        self.estimator_ = clone(self.estimator)\n        self.estimator_.fit(X, y, **kwargs)\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform the data by applying the `predict_func` of the fitted estimator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to be transformed.\n\n        Returns\n        -------\n        output : array-like of shape (n_samples,) | (n_samples, n_outputs)\n            The transformed data. Array will be of shape `(X.shape[0], )` if estimator is not multi output.\n            For multi output estimators an array of shape `(X.shape[0], y.shape[1])` is returned.\n        \"\"\"\n\n        check_is_fitted(self, \"estimator_\")\n        output = getattr(self.estimator_, self.predict_func)(X)\n        return output if self.multi_output_ else output.reshape(-1, 1)\n</code></pre>"},{"location":"api/meta/#sklego.meta.estimator_transformer.EstimatorTransformer.fit","title":"<code>fit(X, y, **kwargs)</code>","text":"<p>Fit the underlying estimator on training data <code>X</code> and <code>y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target values.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to the <code>fit</code> method of the underlying estimator.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>EstimatorTransformer</code> <p>The fitted transformer.</p> Source code in <code>sklego/meta/estimator_transformer.py</code> <pre><code>def fit(self, X, y, **kwargs):\n    \"\"\"Fit the underlying estimator on training data `X` and `y`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training data.\n    y : array-like of shape (n_samples,)\n        Target values.\n    **kwargs : dict\n        Additional keyword arguments passed to the `fit` method of the underlying estimator.\n\n    Returns\n    -------\n    self : EstimatorTransformer\n        The fitted transformer.\n    \"\"\"\n\n    if self.check_input:\n        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES, multi_output=True)\n\n    self.multi_output_ = len(y.shape) &gt; 1\n    self.estimator_ = clone(self.estimator)\n    self.estimator_.fit(X, y, **kwargs)\n    return self\n</code></pre>"},{"location":"api/meta/#sklego.meta.estimator_transformer.EstimatorTransformer.transform","title":"<code>transform(X)</code>","text":"<p>Transform the data by applying the <code>predict_func</code> of the fitted estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to be transformed.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>array-like of shape (n_samples,) | (n_samples, n_outputs)</code> <p>The transformed data. Array will be of shape <code>(X.shape[0], )</code> if estimator is not multi output. For multi output estimators an array of shape <code>(X.shape[0], y.shape[1])</code> is returned.</p> Source code in <code>sklego/meta/estimator_transformer.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform the data by applying the `predict_func` of the fitted estimator.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to be transformed.\n\n    Returns\n    -------\n    output : array-like of shape (n_samples,) | (n_samples, n_outputs)\n        The transformed data. Array will be of shape `(X.shape[0], )` if estimator is not multi output.\n        For multi output estimators an array of shape `(X.shape[0], y.shape[1])` is returned.\n    \"\"\"\n\n    check_is_fitted(self, \"estimator_\")\n    output = getattr(self.estimator_, self.predict_func)(X)\n    return output if self.multi_output_ else output.reshape(-1, 1)\n</code></pre>"},{"location":"api/meta/#sklego.meta.grouped_predictor.GroupedPredictor","title":"<code>sklego.meta.grouped_predictor.GroupedPredictor</code>","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>Construct an estimator per data group. Splits data by values of a single column and fits one estimator per such column.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>scikit-learn compatible estimator/pipeline</code> <p>The estimator/pipeline to be applied per group.</p> required <code>groups</code> <code>int | str | List[int] | List[str]</code> <p>The column(s) of the array/dataframe to select as a grouping parameter set.</p> required <code>shrinkage</code> <code>Literal[constant, min_n_obs, relative] | Callable | None</code> <p>How to perform shrinkage:</p> <ul> <li><code>None</code>: No shrinkage (default)</li> <li><code>\"constant\"</code>: shrunk prediction for a level is weighted average of its prediction and its parents prediction</li> <li><code>\"min_n_obs\"</code>: shrunk prediction is the prediction for the smallest group with at least n observations in it</li> <li><code>\"relative\"</code>: each group-level is weight according to its size</li> <li><code>Callable</code>: a function that takes a list of group lengths and returns an array of the same size with the     weights for each group</li> </ul> <code>None</code> <code>use_global_model</code> <code>bool</code> <ul> <li>With shrinkage: whether to have a model over the entire input as first group</li> <li>Without shrinkage: whether or not to fall back to a general model in case the group parameter is not found     during <code>.predict()</code></li> </ul> <code>True</code> <code>check_X</code> <code>bool</code> <p>Whether to validate <code>X</code> to be non-empty 2D array of finite values and attempt to cast <code>X</code> to float. If disabled, the model/pipeline is expected to handle e.g. missing, non-numeric, or non-finite values.</p> <code>True</code> <code>**shrinkage_kwargs</code> <code>dict</code> <p>Keyword arguments to the shrinkage function</p> <code>{}</code> Source code in <code>sklego/meta/grouped_predictor.py</code> <pre><code>class GroupedPredictor(BaseEstimator):\n    \"\"\"Construct an estimator per data group. Splits data by values of a single column and fits one estimator per such\n    column.\n\n    Parameters\n    ----------\n    estimator : scikit-learn compatible estimator/pipeline\n        The estimator/pipeline to be applied per group.\n    groups : int | str | List[int] | List[str]\n        The column(s) of the array/dataframe to select as a grouping parameter set.\n    shrinkage : Literal[\"constant\", \"min_n_obs\", \"relative\"] | Callable | None, default=None\n        How to perform shrinkage:\n\n        - `None`: No shrinkage (default)\n        - `\"constant\"`: shrunk prediction for a level is weighted average of its prediction and its parents prediction\n        - `\"min_n_obs\"`: shrunk prediction is the prediction for the smallest group with at least n observations in it\n        - `\"relative\"`: each group-level is weight according to its size\n        - `Callable`: a function that takes a list of group lengths and returns an array of the same size with the\n            weights for each group\n    use_global_model : bool, default=True\n\n        - With shrinkage: whether to have a model over the entire input as first group\n        - Without shrinkage: whether or not to fall back to a general model in case the group parameter is not found\n            during `.predict()`\n    check_X : bool, default=True\n        Whether to validate `X` to be non-empty 2D array of finite values and attempt to cast `X` to float.\n        If disabled, the model/pipeline is expected to handle e.g. missing, non-numeric, or non-finite values.\n    **shrinkage_kwargs : dict\n        Keyword arguments to the shrinkage function\n    \"\"\"\n\n    # Number of features in value df can be 0, e.g. for dummy models\n    _check_kwargs = {\"ensure_min_features\": 0, \"accept_large_sparse\": False}\n    _global_col_name = \"a-column-that-is-constant-for-all-data\"\n    _global_col_value = \"global\"\n\n    def __init__(\n        self,\n        estimator,\n        groups,\n        shrinkage=None,\n        use_global_model=True,\n        check_X=True,\n        **shrinkage_kwargs,\n    ):\n        self.estimator = estimator\n        self.groups = groups\n        self.shrinkage = shrinkage\n        self.use_global_model = use_global_model\n        self.shrinkage_kwargs = shrinkage_kwargs\n        self.check_X = check_X\n\n    def __set_shrinkage_function(self):\n        if self.shrinkage and len(as_list(self.groups)) == 1 and not self.use_global_model:\n            raise ValueError(\"Cannot do shrinkage with a single group if use_global_model is False\")\n\n        if isinstance(self.shrinkage, str):\n            # Predefined shrinkage functions\n            shrink_options = {\n                \"constant\": constant_shrinkage,\n                \"relative\": relative_shrinkage,\n                \"min_n_obs\": min_n_obs_shrinkage,\n            }\n\n            try:\n                self.shrinkage_function_ = shrink_options[self.shrinkage]\n            except KeyError:\n                raise ValueError(\n                    f\"The specified shrinkage function {self.shrinkage} is not valid, \"\n                    f\"choose from {list(shrink_options.keys())} or supply a callable.\"\n                )\n        elif callable(self.shrinkage):\n            self.__check_shrinkage_func()\n            self.shrinkage_function_ = self.shrinkage\n        else:\n            raise ValueError(\"Invalid shrinkage specified. Should be either None (no shrinkage), str or callable.\")\n\n    def __check_shrinkage_func(self):\n        \"\"\"Validate the shrinkage function if a function is specified\"\"\"\n        group_lengths = [10, 5, 2]\n        expected_shape = np.array(group_lengths).shape\n        try:\n            result = self.shrinkage(group_lengths)\n        except Exception as e:\n            raise ValueError(f\"Caught an exception while checking the shrinkage function: {str(e)}\") from e\n        else:\n            if not isinstance(result, np.ndarray):\n                raise ValueError(f\"shrinkage_function({group_lengths}) should return an np.ndarray\")\n            if result.shape != expected_shape:\n                raise ValueError(f\"shrinkage_function({group_lengths}).shape should be {expected_shape}\")\n\n    def __get_shrinkage_factor(self, X_group):\n        \"\"\"Get for all complete groups an array of shrinkages\"\"\"\n        group_colnames = X_group.columns.to_list()\n        counts = X_group.groupby(group_colnames).size()\n\n        # Groups that are split on all\n        most_granular_groups = [grp for grp in self.groups_ if len(as_list(grp)) == len(group_colnames)]\n\n        # For each hierarchy level in each most granular group, get the number of observations\n        hierarchical_counts = {\n            granular_group: [counts[tuple(subgroup)].sum() for subgroup in expanding_list(granular_group, tuple)]\n            for granular_group in most_granular_groups\n        }\n\n        # For each hierarchy level in each most granular group, get the shrinkage factor\n        shrinkage_factors = {\n            group: self.shrinkage_function_(counts, **self.shrinkage_kwargs)\n            for group, counts in hierarchical_counts.items()\n        }\n\n        # Make sure that the factors sum to one\n        shrinkage_factors = {group: value / value.sum() for group, value in shrinkage_factors.items()}\n\n        return shrinkage_factors\n\n    def __fit_single_group(self, group, X, y=None):\n        \"\"\"Fit estimator to the given group.\"\"\"\n        try:\n            return clone(self.estimator).fit(X, y)\n        except Exception as e:\n            raise type(e)(f\"Exception for group {group}: {e}\")\n\n    def __fit_grouped_estimator(self, X_group, X_value, y=None, columns=None):\n        \"\"\"Fit an estimator to each group\"\"\"\n        # Reset indices such that they are the same in X and y\n        if not columns:\n            columns = X_group.columns.tolist()\n\n        # Make the groups based on the groups dataframe, use the indices on the values array\n        try:\n            group_indices = X_group.groupby(columns).indices\n        except TypeError:\n            # This one is needed because of line #918 of sklearn/utils/estimator_checks\n            raise TypeError(\"argument must be a string, date or number\")\n\n        if y is not None:\n            if isinstance(y, pd.Series):\n                y.index = X_group.index\n\n            grouped_estimators = {\n                # Fit a clone of the transformer to each group\n                group: self.__fit_single_group(group, X_value[indices, :], y[indices])\n                for group, indices in group_indices.items()\n            }\n        else:\n            grouped_estimators = {\n                group: self.__fit_single_group(group, X_value[indices, :]) for group, indices in group_indices.items()\n            }\n\n        return grouped_estimators\n\n    def __fit_shrinkage_groups(self, X_group, X_value, y):\n        estimators = {}\n\n        for grouping_colnames in self.group_colnames_hierarchical_:\n            # Fit a grouped estimator to each (sub)group hierarchically\n            estimators.update(self.__fit_grouped_estimator(X_group, X_value, y, columns=grouping_colnames))\n\n        return estimators\n\n    def __add_shrinkage_column(self, X_group):\n        \"\"\"Add global group as first column if needed for shrinkage\"\"\"\n\n        if self.shrinkage is not None and self.use_global_model:\n            return pd.concat(\n                [\n                    pd.Series(\n                        [self._global_col_value] * len(X_group),\n                        name=self._global_col_name,\n                    ),\n                    X_group,\n                ],\n                axis=1,\n            )\n\n        return X_group\n\n    def fit(self, X, y=None):\n        \"\"\"Fit one estimator for each group of training data `X` and `y`.\n\n        Will also learn the groups that exist within the dataset.\n\n        If `use_global_model=True` a fallback estimator will be fitted on the entire dataset in case a group is not\n        found during `.predict()`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,), default=None\n            Target values.\n\n        Returns\n        -------\n        self : GroupedPredictor\n            The fitted estimator.\n        \"\"\"\n        X_group, X_value = _split_groups_and_values(\n            X, self.groups, min_value_cols=0, check_X=self.check_X, **self._check_kwargs\n        )\n\n        X_group = self.__add_shrinkage_column(X_group)\n\n        if y is not None:\n            y = check_array(y, ensure_2d=False)\n\n        if self.shrinkage is not None:\n            self.__set_shrinkage_function()\n\n        # List of all hierarchical subsets of columns\n        self.group_colnames_hierarchical_ = expanding_list(X_group.columns, list)\n\n        self.fallback_ = None\n\n        if self.shrinkage is None and self.use_global_model:\n            self.fallback_ = clone(self.estimator).fit(X_value, y)\n\n        if self.shrinkage is not None:\n            self.estimators_ = self.__fit_shrinkage_groups(X_group, X_value, y)\n        else:\n            self.estimators_ = self.__fit_grouped_estimator(X_group, X_value, y)\n\n        self.groups_ = as_list(self.estimators_.keys())\n\n        if self.shrinkage is not None:\n            self.shrinkage_factors_ = self.__get_shrinkage_factor(X_group)\n\n        return self\n\n    def __predict_shrinkage_groups(self, X_group, X_value, method=\"predict\"):\n        \"\"\"Make predictions for all shrinkage groups\"\"\"\n        # DataFrame with predictions for each hierarchy level, per row. Missing groups errors are thrown here.\n        hierarchical_predictions = pd.concat(\n            [\n                pd.Series(self.__predict_groups(X_group, X_value, level_columns, method=method))\n                for level_columns in self.group_colnames_hierarchical_\n            ],\n            axis=1,\n        )\n\n        # This is a Series with values the tuples of hierarchical grouping\n        prediction_groups = pd.Series([tuple(_) for _ in X_group.itertuples(index=False)])\n\n        # This is a Series of arrays\n        shrinkage_factors = prediction_groups.map(self.shrinkage_factors_)\n\n        # Convert the Series of arrays it to a DataFrame\n        shrinkage_factors = pd.DataFrame.from_dict(shrinkage_factors.to_dict()).T\n        return (hierarchical_predictions * shrinkage_factors).sum(axis=1)\n\n    def __predict_single_group(self, group, X, method=\"predict\"):\n        \"\"\"Predict a single group by getting its estimator from the fitted dict\"\"\"\n        # Keep track of the original index such that we can sort in __predict_groups\n        index = X.index\n\n        try:\n            group_predictor = self.estimators_[group]\n        except KeyError:\n            if self.fallback_:\n                group_predictor = self.fallback_\n            else:\n                raise ValueError(f\"Found new group {group} during predict with use_global_model = False\")\n\n        is_predict_proba = is_classifier(group_predictor) and method == \"predict_proba\"\n        # Ensure to provide pd.DataFrame with the correct label name\n        extra_kwargs = {\"columns\": group_predictor.classes_} if is_predict_proba else {}\n\n        # getattr(group_predictor, method) returns the predict method of the fitted model\n        # if the method argument is \"predict\" and the predict_proba method if method argument is \"predict_proba\"\n        return pd.DataFrame(getattr(group_predictor, method)(X), **extra_kwargs).set_index(index)\n\n    def __predict_groups(\n        self,\n        X_group: pd.DataFrame,\n        X_value: np.array,\n        group_cols=None,\n        method=\"predict\",\n    ):\n        \"\"\"Predict for all groups\"\"\"\n        # Reset indices such that they are the same in X_group (reset in __check_grouping_columns),\n        # this way we can track the order of the result\n        X_value = pd.DataFrame(X_value).reset_index(drop=True)\n\n        if group_cols is None:\n            group_cols = X_group.columns.tolist()\n\n        # Make the groups based on the groups dataframe, use the indices on the values array\n        group_indices = X_group.groupby(group_cols).indices\n\n        return (\n            pd.concat(\n                [\n                    self.__predict_single_group(group, X_value.loc[indices, :], method=method)\n                    for group, indices in group_indices.items()\n                ],\n                axis=0,\n            )\n            # Fill with prob = 0 for impossible labels in predict_proba\n            .fillna(0)\n            .sort_index()\n            .values.squeeze()\n        )\n\n    def predict(self, X):\n        \"\"\"Predict target values on new data `X` by predicting on each group. If a group is not found during\n        `.predict()` and `use_global_model=True` the fallback estimator will be used. If `use_global_model=False` a\n        `ValueError` will be raised.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            Predicted target values.\n        \"\"\"\n        check_is_fitted(self, [\"estimators_\", \"groups_\", \"fallback_\"])\n\n        X_group, X_value = _split_groups_and_values(\n            X, self.groups, min_value_cols=0, check_X=self.check_X, **self._check_kwargs\n        )\n\n        X_group = self.__add_shrinkage_column(X_group)\n\n        if self.shrinkage is None:\n            return self.__predict_groups(X_group, X_value, method=\"predict\")\n        else:\n            return self.__predict_shrinkage_groups(X_group, X_value, method=\"predict\")\n\n    # This ensures that the meta-estimator only has the predict_proba method if the estimator has it\n    @available_if(lambda self: hasattr(self.estimator, \"predict_proba\"))\n    def predict_proba(self, X):\n        \"\"\"Predict probabilities on new data `X`.\n\n        !!! warning\n            Available only if the underlying estimator implements `.predict_proba()` method.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_classes)\n            Predicted probabilities per class.\n        \"\"\"\n        check_is_fitted(self, [\"estimators_\", \"groups_\", \"fallback_\"])\n\n        X_group, X_value = _split_groups_and_values(\n            X, self.groups, min_value_cols=0, check_X=self.check_X, **self._check_kwargs\n        )\n\n        X_group = self.__add_shrinkage_column(X_group)\n\n        if self.shrinkage is None:\n            return self.__predict_groups(X_group, X_value, method=\"predict_proba\")\n        else:\n            return self.__predict_shrinkage_groups(X_group, X_value, method=\"predict_proba\")\n\n    # This ensures that the meta-estimator only has the predict_proba method if the estimator has it\n    @available_if(lambda self: hasattr(self.estimator, \"decision_function\"))\n    def decision_function(self, X):\n        \"\"\"Predict confidence scores for samples in `X`.\n\n        !!! warning\n            Available only if the underlying estimator implements `.decision_function()` method.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,) or (n_samples, n_classes)\n            Confidence scores per (n_samples, n_classes) combination.\n            In the binary case, confidence score for self.classes_[1] where &gt; 0 means this class would be\n            predicted.\n        \"\"\"\n        check_is_fitted(self, [\"estimators_\", \"groups_\", \"fallback_\"])\n\n        X_group, X_value = _split_groups_and_values(\n            X, self.groups, min_value_cols=0, check_X=self.check_X, **self._check_kwargs\n        )\n\n        X_group = self.__add_shrinkage_column(X_group)\n\n        if self.shrinkage is None:\n            return self.__predict_groups(X_group, X_value, method=\"decision_function\")\n        else:\n            return self.__predict_shrinkage_groups(X_group, X_value, method=\"decision_function\")\n</code></pre>"},{"location":"api/meta/#sklego.meta.grouped_predictor.GroupedPredictor.decision_function","title":"<code>decision_function(X)</code>","text":"<p>Predict confidence scores for samples in <code>X</code>.</p> <p>Warning</p> <p>Available only if the underlying estimator implements <code>.decision_function()</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,) or (n_samples, n_classes)</code> <p>Confidence scores per (n_samples, n_classes) combination. In the binary case, confidence score for self.classes_[1] where &gt; 0 means this class would be predicted.</p> Source code in <code>sklego/meta/grouped_predictor.py</code> <pre><code>@available_if(lambda self: hasattr(self.estimator, \"decision_function\"))\ndef decision_function(self, X):\n    \"\"\"Predict confidence scores for samples in `X`.\n\n    !!! warning\n        Available only if the underlying estimator implements `.decision_function()` method.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,) or (n_samples, n_classes)\n        Confidence scores per (n_samples, n_classes) combination.\n        In the binary case, confidence score for self.classes_[1] where &gt; 0 means this class would be\n        predicted.\n    \"\"\"\n    check_is_fitted(self, [\"estimators_\", \"groups_\", \"fallback_\"])\n\n    X_group, X_value = _split_groups_and_values(\n        X, self.groups, min_value_cols=0, check_X=self.check_X, **self._check_kwargs\n    )\n\n    X_group = self.__add_shrinkage_column(X_group)\n\n    if self.shrinkage is None:\n        return self.__predict_groups(X_group, X_value, method=\"decision_function\")\n    else:\n        return self.__predict_shrinkage_groups(X_group, X_value, method=\"decision_function\")\n</code></pre>"},{"location":"api/meta/#sklego.meta.grouped_predictor.GroupedPredictor.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit one estimator for each group of training data <code>X</code> and <code>y</code>.</p> <p>Will also learn the groups that exist within the dataset.</p> <p>If <code>use_global_model=True</code> a fallback estimator will be fitted on the entire dataset in case a group is not found during <code>.predict()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target values.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>GroupedPredictor</code> <p>The fitted estimator.</p> Source code in <code>sklego/meta/grouped_predictor.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit one estimator for each group of training data `X` and `y`.\n\n    Will also learn the groups that exist within the dataset.\n\n    If `use_global_model=True` a fallback estimator will be fitted on the entire dataset in case a group is not\n    found during `.predict()`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training data.\n    y : array-like of shape (n_samples,), default=None\n        Target values.\n\n    Returns\n    -------\n    self : GroupedPredictor\n        The fitted estimator.\n    \"\"\"\n    X_group, X_value = _split_groups_and_values(\n        X, self.groups, min_value_cols=0, check_X=self.check_X, **self._check_kwargs\n    )\n\n    X_group = self.__add_shrinkage_column(X_group)\n\n    if y is not None:\n        y = check_array(y, ensure_2d=False)\n\n    if self.shrinkage is not None:\n        self.__set_shrinkage_function()\n\n    # List of all hierarchical subsets of columns\n    self.group_colnames_hierarchical_ = expanding_list(X_group.columns, list)\n\n    self.fallback_ = None\n\n    if self.shrinkage is None and self.use_global_model:\n        self.fallback_ = clone(self.estimator).fit(X_value, y)\n\n    if self.shrinkage is not None:\n        self.estimators_ = self.__fit_shrinkage_groups(X_group, X_value, y)\n    else:\n        self.estimators_ = self.__fit_grouped_estimator(X_group, X_value, y)\n\n    self.groups_ = as_list(self.estimators_.keys())\n\n    if self.shrinkage is not None:\n        self.shrinkage_factors_ = self.__get_shrinkage_factor(X_group)\n\n    return self\n</code></pre>"},{"location":"api/meta/#sklego.meta.grouped_predictor.GroupedPredictor.predict","title":"<code>predict(X)</code>","text":"<p>Predict target values on new data <code>X</code> by predicting on each group. If a group is not found during <code>.predict()</code> and <code>use_global_model=True</code> the fallback estimator will be used. If <code>use_global_model=False</code> a <code>ValueError</code> will be raised.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>Predicted target values.</p> Source code in <code>sklego/meta/grouped_predictor.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict target values on new data `X` by predicting on each group. If a group is not found during\n    `.predict()` and `use_global_model=True` the fallback estimator will be used. If `use_global_model=False` a\n    `ValueError` will be raised.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        Predicted target values.\n    \"\"\"\n    check_is_fitted(self, [\"estimators_\", \"groups_\", \"fallback_\"])\n\n    X_group, X_value = _split_groups_and_values(\n        X, self.groups, min_value_cols=0, check_X=self.check_X, **self._check_kwargs\n    )\n\n    X_group = self.__add_shrinkage_column(X_group)\n\n    if self.shrinkage is None:\n        return self.__predict_groups(X_group, X_value, method=\"predict\")\n    else:\n        return self.__predict_shrinkage_groups(X_group, X_value, method=\"predict\")\n</code></pre>"},{"location":"api/meta/#sklego.meta.grouped_predictor.GroupedPredictor.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict probabilities on new data <code>X</code>.</p> <p>Warning</p> <p>Available only if the underlying estimator implements <code>.predict_proba()</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_classes)</code> <p>Predicted probabilities per class.</p> Source code in <code>sklego/meta/grouped_predictor.py</code> <pre><code>@available_if(lambda self: hasattr(self.estimator, \"predict_proba\"))\ndef predict_proba(self, X):\n    \"\"\"Predict probabilities on new data `X`.\n\n    !!! warning\n        Available only if the underlying estimator implements `.predict_proba()` method.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_classes)\n        Predicted probabilities per class.\n    \"\"\"\n    check_is_fitted(self, [\"estimators_\", \"groups_\", \"fallback_\"])\n\n    X_group, X_value = _split_groups_and_values(\n        X, self.groups, min_value_cols=0, check_X=self.check_X, **self._check_kwargs\n    )\n\n    X_group = self.__add_shrinkage_column(X_group)\n\n    if self.shrinkage is None:\n        return self.__predict_groups(X_group, X_value, method=\"predict_proba\")\n    else:\n        return self.__predict_shrinkage_groups(X_group, X_value, method=\"predict_proba\")\n</code></pre>"},{"location":"api/meta/#sklego.meta.grouped_transformer.GroupedTransformer","title":"<code>sklego.meta.grouped_transformer.GroupedTransformer</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Construct a transformer per data group. Splits data by groups from single or multiple columns and transforms remaining columns using the transformers corresponding to the groups.</p> <p>Parameters:</p> Name Type Description Default <code>transformer</code> <code>scikit-learn compatible transformer</code> <p>The transformer to be applied per group.</p> required <code>groups</code> <code>int | str | List[int] | List[str] | None</code> <p>The column(s) of the array/dataframe to select as a grouping parameter set. If <code>None</code>, the transformer will be applied to the entire input without grouping.</p> required <code>use_global_model</code> <code>bool</code> <p>Whether or not to fall back to a general transformation in case a group is not found during <code>.transform()</code>.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>transformers_</code> <code>scikit-learn compatible transformer | dict[..., scikit-learn compatible transformer]</code> <p>The fitted transformers per group or a single fitted transformer if <code>groups</code> is <code>None</code>.</p> <code>fallback_</code> <code>scikit-learn compatible transformer | None</code> <p>The fitted transformer to fall back to in case a group is not found during <code>.transform()</code>. Only present if <code>use_global_model</code> is <code>True</code>.</p> Source code in <code>sklego/meta/grouped_transformer.py</code> <pre><code>class GroupedTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Construct a transformer per data group. Splits data by groups from single or multiple columns and transforms\n    remaining columns using the transformers corresponding to the groups.\n\n    Parameters\n    ----------\n    transformer : scikit-learn compatible transformer\n        The transformer to be applied per group.\n    groups : int | str | List[int] | List[str] | None\n        The column(s) of the array/dataframe to select as a grouping parameter set. If `None`, the transformer will be\n        applied to the entire input without grouping.\n    use_global_model : bool, default=True\n        Whether or not to fall back to a general transformation in case a group is not found during `.transform()`.\n\n    Attributes\n    ----------\n    transformers_ : scikit-learn compatible transformer | dict[..., scikit-learn compatible transformer]\n        The fitted transformers per group or a single fitted transformer if `groups` is `None`.\n    fallback_ : scikit-learn compatible transformer | None\n        The fitted transformer to fall back to in case a group is not found during `.transform()`. Only present if\n        `use_global_model` is `True`.\n    \"\"\"\n\n    _check_kwargs = {\"accept_large_sparse\": False}\n\n    def __init__(self, transformer, groups, use_global_model=True):\n        self.transformer = transformer\n        self.groups = groups\n        self.use_global_model = use_global_model\n\n    def __fit_single_group(self, group, X, y=None):\n        \"\"\"Fit transformer to the given group.\n\n        Parameters\n        ----------\n        group : tuple\n            The group to fit the transformer to.\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,), default=None\n            Target values.\n\n        Returns\n        -------\n        transformer : scikit-learn compatible transformer\n            The fitted transformer for the group.\n        \"\"\"\n        try:\n            return clone(self.transformer).fit(X, y)\n        except Exception as e:\n            raise type(e)(f\"Exception for group {group}: {e}\")\n\n    def __fit_grouped_transformer(self, X_group: pd.DataFrame, X_value: np.ndarray, y=None):\n        \"\"\"Fit a transformer to each group\"\"\"\n        # Make the groups based on the groups dataframe, use the indices on the values array\n        try:\n            group_indices = X_group.groupby(X_group.columns.tolist()).indices\n        except TypeError:\n            # This one is needed because of line #918 of sklearn/utils/estimator_checks\n            raise TypeError(\"argument must be a string, date or number\")\n\n        if y is not None:\n            if isinstance(y, pd.Series):\n                y.index = X_group.index\n\n            grouped_transformers = {\n                # Fit a clone of the transformer to each group\n                group: self.__fit_single_group(group, X_value[indices, :], y[indices])\n                for group, indices in group_indices.items()\n            }\n        else:\n            grouped_transformers = {\n                group: self.__fit_single_group(group, X_value[indices, :]) for group, indices in group_indices.items()\n            }\n\n        return grouped_transformers\n\n    def __check_transformer(self):\n        \"\"\"Check if the supplied transformer has a `transform` method and raise a `ValueError` if not.\"\"\"\n        if not hasattr(self.transformer, \"transform\"):\n            raise ValueError(\"The supplied transformer should have a 'transform' method\")\n\n    def fit(self, X, y=None):\n        \"\"\"Fit one transformer for each group of training data `X`.\n\n        Will also learn the groups that exist within the dataset.\n\n        If `use_global_model=True` a fallback transformer will be fitted on the entire dataset in case a group is not\n        found during `.transform()`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data. If `groups` is not `None`, X should have at least two columns, of which at least one\n            corresponds to groups defined in `groups`, and the remaining columns represent the values to transform.\n        y : array-like of shape (n_samples,), default=None\n            Target values.\n\n        Returns\n        -------\n        self : GroupedTransformer\n            The fitted transformer.\n        \"\"\"\n        self.__check_transformer()\n\n        self.fallback_ = None\n\n        if self.groups is None:\n            self.transformers_ = clone(self.transformer).fit(X, y)\n            return self\n\n        X_group, X_value = _split_groups_and_values(X, self.groups, **self._check_kwargs)\n        self.transformers_ = self.__fit_grouped_transformer(X_group, X_value, y)\n\n        if self.use_global_model:\n            self.fallback_ = clone(self.transformer).fit(X_value)\n\n        return self\n\n    def __transform_single_group(self, group, X):\n        \"\"\"Transform a single group by getting its transformer from the fitted dict\"\"\"\n        # Keep track of the original index such that we can sort in __transform_groups\n        index = X.index\n        try:\n            group_transformer = self.transformers_[group]\n        except KeyError:\n            if self.fallback_:\n                group_transformer = self.fallback_\n            else:\n                raise ValueError(f\"Found new group {group} during transform with use_global_model = False\")\n\n        return pd.DataFrame(group_transformer.transform(X)).set_index(index)\n\n    def __transform_groups(self, X_group: pd.DataFrame, X_value: np.ndarray):\n        \"\"\"Transform all groups\"\"\"\n        # Reset indices such that they are the same in X_group (reset in __check_grouping_columns),\n        # this way we can track the order of the result\n        X_value = pd.DataFrame(X_value).reset_index(drop=True)\n\n        # Make the groups based on the groups dataframe, use the indices on the values array\n        group_indices = X_group.groupby(X_group.columns.tolist()).indices\n\n        return (\n            pd.concat(\n                [\n                    self.__transform_single_group(group, X_value.loc[indices, :])\n                    for group, indices in group_indices.items()\n                ],\n                axis=0,\n            )\n            .sort_index()\n            .values\n        )\n\n    def transform(self, X):\n        \"\"\"Transform new data `X` by transforming on each group. If a group is not found during `.transform()` and\n        `use_global_model=True` the fallback transformer will be used. If `use_global_model=False` a `ValueError` will\n        be raised.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data to transform.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_features)\n            Data transformed per group.\n        \"\"\"\n        check_is_fitted(self, [\"fallback_\", \"transformers_\"])\n\n        if self.groups is None:\n            return self.transformers_.transform(X)\n\n        X_group, X_value = _split_groups_and_values(X, self.groups, **self._check_kwargs)\n\n        return self.__transform_groups(X_group, X_value)\n</code></pre>"},{"location":"api/meta/#sklego.meta.grouped_transformer.GroupedTransformer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit one transformer for each group of training data <code>X</code>.</p> <p>Will also learn the groups that exist within the dataset.</p> <p>If <code>use_global_model=True</code> a fallback transformer will be fitted on the entire dataset in case a group is not found during <code>.transform()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Training data. If <code>groups</code> is not <code>None</code>, X should have at least two columns, of which at least one corresponds to groups defined in <code>groups</code>, and the remaining columns represent the values to transform.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target values.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>GroupedTransformer</code> <p>The fitted transformer.</p> Source code in <code>sklego/meta/grouped_transformer.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit one transformer for each group of training data `X`.\n\n    Will also learn the groups that exist within the dataset.\n\n    If `use_global_model=True` a fallback transformer will be fitted on the entire dataset in case a group is not\n    found during `.transform()`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training data. If `groups` is not `None`, X should have at least two columns, of which at least one\n        corresponds to groups defined in `groups`, and the remaining columns represent the values to transform.\n    y : array-like of shape (n_samples,), default=None\n        Target values.\n\n    Returns\n    -------\n    self : GroupedTransformer\n        The fitted transformer.\n    \"\"\"\n    self.__check_transformer()\n\n    self.fallback_ = None\n\n    if self.groups is None:\n        self.transformers_ = clone(self.transformer).fit(X, y)\n        return self\n\n    X_group, X_value = _split_groups_and_values(X, self.groups, **self._check_kwargs)\n    self.transformers_ = self.__fit_grouped_transformer(X_group, X_value, y)\n\n    if self.use_global_model:\n        self.fallback_ = clone(self.transformer).fit(X_value)\n\n    return self\n</code></pre>"},{"location":"api/meta/#sklego.meta.grouped_transformer.GroupedTransformer.transform","title":"<code>transform(X)</code>","text":"<p>Transform new data <code>X</code> by transforming on each group. If a group is not found during <code>.transform()</code> and <code>use_global_model=True</code> the fallback transformer will be used. If <code>use_global_model=False</code> a <code>ValueError</code> will be raised.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Data to transform.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_features)</code> <p>Data transformed per group.</p> Source code in <code>sklego/meta/grouped_transformer.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform new data `X` by transforming on each group. If a group is not found during `.transform()` and\n    `use_global_model=True` the fallback transformer will be used. If `use_global_model=False` a `ValueError` will\n    be raised.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data to transform.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_features)\n        Data transformed per group.\n    \"\"\"\n    check_is_fitted(self, [\"fallback_\", \"transformers_\"])\n\n    if self.groups is None:\n        return self.transformers_.transform(X)\n\n    X_group, X_value = _split_groups_and_values(X, self.groups, **self._check_kwargs)\n\n    return self.__transform_groups(X_group, X_value)\n</code></pre>"},{"location":"api/meta/#sklego.meta.ordinal_classification.OrdinalClassifier","title":"<code>sklego.meta.ordinal_classification.OrdinalClassifier</code>","text":"<p>             Bases: <code>MultiOutputMixin</code>, <code>ClassifierMixin</code>, <code>MetaEstimatorMixin</code>, <code>BaseEstimator</code></p> <p>The <code>OrdinalClassifier</code> allows to use a binary classifier to address an ordinal classification problem.</p> <p>Suppose we have N ordinal classes to predict, then the original binary classifier is fitted on N-1 by training sets, each of which represents the samples where <code>y &lt;= y_label</code> for each <code>y_label</code> in <code>y</code> except <code>y.max()</code> (as every sample is smaller than the maximum value).</p> <p>The binary classifiers are then used to predict the probability of each sample to be in each new class <code>y &lt;= y_label</code>, and finally the probability of each sample is the difference between two consecutive classes is computed:</p> \\[ P(y = \\text{class}_i) = P(\\text{class}_{i-1} &lt; y \\leq \\text{class}_i) = P(y \\leq \\text{class}_i) - P(y \\leq \\text{class}_{i-1}) \\] <p>About scikit-learn <code>predict_proba</code>s</p> <p>As you can see from the formula above, it is of utmost importance to use proper probabilities to compute the results. However, not every scikit-learn classifier <code>.predict_proba()</code> method outputs probabilities (they are more like a confidence score).</p> <p>We recommend to use <code>CalibratedClassifierCV</code> to calibrate the probabilities of the binary classifiers.</p> <p>You can enable this by setting <code>use_calibration=True</code> and passing an uncalibrated classifier to the <code>OrdinalClassifier</code> or by passing a calibrated classifier to the <code>OrdinalClassifier</code> constructor.</p> <p>More on this topic can be found in the scikit-learn documentation.</p> <p>Computation time</p> <p>The <code>OrdinalClassifier</code> is a meta-estimator that fits N-1 binary classifiers. This can be computationally expensive, especially when using a large number of samples and/or features or a complex classifier.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>scikit-learn compatible classifier</code> <p>The estimator to be applied to the data, used as binary classifier.</p> required <code>n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel. The same convention of <code>joblib.Parallel</code> holds:</p> <ul> <li><code>n_jobs = None</code>: interpreted as n_jobs=1.</li> <li><code>n_jobs &gt; 0</code>: n_cpus=n_jobs are used.</li> <li><code>n_jobs &lt; 0</code>: (n_cpus + 1 + n_jobs) are used.</li> </ul> <code>None</code> <code>use_calibration</code> <code>bool</code> <p>Whether or not to calibrate the binary classifiers using <code>CalibratedClassifierCV</code>.</p> <code>False</code> <code>calibrarion_kwargs</code> <code>dict | None</code> <p>Keyword arguments to the <code>CalibratedClassifierCV</code> class, used only if <code>use_calibration=True</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>estimators_</code> <code>dict[int, scikit-learn compatible classifier]</code> <p>The fitted underlying binary classifiers.</p> <code>classes_</code> <code>np.ndarray of shape (n_classes,)</code> <p>The classes seen during <code>fit</code>.</p> <code>n_features_in_</code> <code>int</code> <p>The number of features seen during <code>fit</code>.</p> <p>Examples:</p> <pre><code>import pandas as pd\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklego.meta import OrdinalClassifier\n\nurl = \"https://stats.idre.ucla.edu/stat/data/ologit.dta\"\ndf = pd.read_stata(url).assign(apply_codes = lambda t: t[\"apply\"].cat.codes)\n\ntarget = \"apply_codes\"\nfeatures = [c for c in df.columns if c not in {target, \"apply\"}]\n\nX, y = df[features].to_numpy(), df[target].to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclf = OrdinalClassifier(LogisticRegression(), n_jobs=-1)\n_ = clf.fit(X_train, y_train)\nclf.predict_proba(X_test)\n</code></pre>"},{"location":"api/meta/#sklego.meta.ordinal_classification.OrdinalClassifier--notes","title":"Notes","text":"<p>The implementation is based on the paper A simple approach to ordinal classification by Eibe Frank and Mark Hall.</p> Source code in <code>sklego/meta/ordinal_classification.py</code> <pre><code>class OrdinalClassifier(MultiOutputMixin, ClassifierMixin, MetaEstimatorMixin, BaseEstimator):\n    r\"\"\"The `OrdinalClassifier` allows to use a binary classifier to address an ordinal classification problem.\n\n    Suppose we have N ordinal classes to predict, then the original binary classifier is fitted on N-1 by training sets,\n    each of which represents the samples where `y &lt;= y_label` for each `y_label` in `y` except `y.max()` (as every\n    sample is smaller than the maximum value).\n\n    The binary classifiers are then used to predict the probability of each sample to be in each _new_ class\n    `y &lt;= y_label`, and finally the probability of each sample is the difference between two consecutive classes is\n    computed:\n\n    $$ P(y = \\text{class}_i) = P(\\text{class}_{i-1} &lt; y \\leq \\text{class}_i) = P(y \\leq \\text{class}_i) - P(y \\leq \\text{class}_{i-1}) $$\n\n    !!! warning \"About scikit-learn `predict_proba`s\"\n\n        As you can see from the formula above, it is of utmost importance to use _proper_ probabilities to compute the\n        results. However, not every scikit-learn classifier `.predict_proba()` method outputs probabilities (they are\n        more like a confidence score).\n\n        We recommend to use `CalibratedClassifierCV` to calibrate the probabilities of the binary classifiers.\n\n        You can enable this by setting `use_calibration=True` and passing an uncalibrated classifier to the\n        `OrdinalClassifier` or by passing a calibrated classifier to the `OrdinalClassifier` constructor.\n\n        More on this topic can be found in the [scikit-learn documentation](https://scikit-learn.org/stable/modules/calibration.html).\n\n    !!! warning \"Computation time\"\n\n        The `OrdinalClassifier` is a meta-estimator that fits N-1 binary classifiers. This can be computationally\n        expensive, especially when using a large number of samples and/or features or a complex classifier.\n\n    Parameters\n    ----------\n    estimator : scikit-learn compatible classifier\n        The estimator to be applied to the data, used as binary classifier.\n    n_jobs : int, default=None\n        The number of jobs to run in parallel. The same convention of [`joblib.Parallel`](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html)\n        holds:\n\n        - `n_jobs = None`: interpreted as n_jobs=1.\n        - `n_jobs &gt; 0`: n_cpus=n_jobs are used.\n        - `n_jobs &lt; 0`: (n_cpus + 1 + n_jobs) are used.\n    use_calibration : bool, default=False\n        Whether or not to calibrate the binary classifiers using `CalibratedClassifierCV`.\n    calibrarion_kwargs : dict | None, default=None\n        Keyword arguments to the `CalibratedClassifierCV` class, used only if `use_calibration=True`.\n\n    Attributes\n    ----------\n    estimators_ : dict[int, scikit-learn compatible classifier]\n        The fitted underlying binary classifiers.\n    classes_ : np.ndarray of shape (n_classes,)\n        The classes seen during `fit`.\n    n_features_in_ : int\n        The number of features seen during `fit`.\n\n    Examples\n    --------\n    ```py\n    import pandas as pd\n\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import train_test_split\n\n    from sklego.meta import OrdinalClassifier\n\n    url = \"https://stats.idre.ucla.edu/stat/data/ologit.dta\"\n    df = pd.read_stata(url).assign(apply_codes = lambda t: t[\"apply\"].cat.codes)\n\n    target = \"apply_codes\"\n    features = [c for c in df.columns if c not in {target, \"apply\"}]\n\n    X, y = df[features].to_numpy(), df[target].to_numpy()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    clf = OrdinalClassifier(LogisticRegression(), n_jobs=-1)\n    _ = clf.fit(X_train, y_train)\n    clf.predict_proba(X_test)\n    ```\n\n    Notes\n    -----\n    The implementation is based on the paper [A simple approach to ordinal classification](https://www.cs.waikato.ac.nz/~eibe/pubs/ordinal_tech_report.pdf)\n    by Eibe Frank and Mark Hall.\n\n    \"\"\"\n\n    def __init__(self, estimator, *, n_jobs=None, use_calibration=False, **calibrarion_kwargs):\n        self.estimator = estimator\n        self.n_jobs = n_jobs\n        self.use_calibration = use_calibration\n        self.calibrarion_kwargs = calibrarion_kwargs\n\n    def fit(self, X, y):\n        \"\"\"Fit the `OrdinalClassifier` model on training data `X` and `y` by fitting its underlying estimators on\n        N-1 datasets `X` and `y` for each class `y_label` in `y` except `y.max()`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n\n        Returns\n        -------\n        self : OrdinalClassifier\n            Fitted model.\n\n        Raises\n        ------\n        ValueError\n            If the estimator is not a classifier or if it does not implement `.predict_proba()`.\n        \"\"\"\n\n        if not is_classifier(self.estimator):\n            raise ValueError(\"The estimator must be a classifier.\")\n\n        if not hasattr(self.estimator, \"predict_proba\"):\n            raise ValueError(\"The estimator must implement `.predict_proba()` method.\")\n\n        X, y = check_X_y(X, y, estimator=self, ensure_min_samples=2)\n\n        self.classes_ = np.sort(np.unique(y))\n        self.n_features_in_ = X.shape[1]\n\n        if self.n_classes_ &lt; 3:\n            raise ValueError(\"`OrdinalClassifier` can't train when less than 3 classes are present.\")\n\n        if self.n_jobs is None or self.n_jobs == 1:\n            self.estimators_ = {y_label: self._fit_binary_estimator(X, y, y_label) for y_label in self.classes_[:-1]}\n        else:\n            self.estimators_ = dict(\n                zip(\n                    self.classes_[:-1],\n                    Parallel(n_jobs=self.n_jobs)(\n                        delayed(self._fit_binary_estimator)(X, y, y_label) for y_label in self.classes_[:-1]\n                    ),\n                )\n            )\n\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for samples in `X`. The class probabilities of a sample are computed as the\n        difference between the probability of the sample to be in class `y_label` and the probability of the sample to\n        be in class `y_label - 1`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_classes)\n            The predicted class probabilities.\n\n        Raises\n        ------\n        ValueError\n            If `X` has a different number of features than the one seen during `fit`.\n        \"\"\"\n        check_is_fitted(self, [\"estimators_\", \"classes_\"])\n        X = check_array(X, ensure_2d=True, estimator=self)\n\n        if X.shape[1] != self.n_features_in_:\n            raise ValueError(f\"X has {X.shape[1]} features, expected {self.n_features_in_} features.\")\n\n        raw_proba = np.array([estimator.predict_proba(X)[:, 1] for estimator in self.estimators_.values()]).T\n        p_y_le = np.column_stack((np.zeros(X.shape[0]), raw_proba, np.ones(X.shape[0])))\n\n        # Equivalent to (p_y_le[:, 1:] - p_y_le[:, :-1])\n        return np.diff(p_y_le, n=1, axis=1)\n\n    def predict(self, X):\n        \"\"\"Predict class labels for samples in `X` as the class with the highest probability.\"\"\"\n        check_is_fitted(self, [\"estimators_\", \"classes_\"])\n        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n\n    def _fit_binary_estimator(self, X, y, y_label):\n        \"\"\"Utility method to fit a binary classifier on the dataset where `y &lt;= y_label`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n        y_label : int\n            The label of the class to predict.\n\n        Returns\n        -------\n        fitted_model : scikit-learn compatible classifier\n            The fitted binary classifier.\n        \"\"\"\n        y_bin = (y &lt;= y_label).astype(int)\n        if self.use_calibration:\n            return CalibratedClassifierCV(estimator=clone(self.estimator), **self.calibrarion_kwargs).fit(X, y_bin)\n        else:\n            return clone(self.estimator).fit(X, y_bin)\n\n    @property\n    def n_classes_(self):\n        \"\"\"Number of classes.\"\"\"\n        return len(self.classes_)\n</code></pre>"},{"location":"api/meta/#sklego.meta.ordinal_classification.OrdinalClassifier.n_classes_","title":"<code>n_classes_</code>  <code>property</code>","text":"<p>Number of classes.</p>"},{"location":"api/meta/#sklego.meta.ordinal_classification.OrdinalClassifier.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the <code>OrdinalClassifier</code> model on training data <code>X</code> and <code>y</code> by fitting its underlying estimators on N-1 datasets <code>X</code> and <code>y</code> for each class <code>y_label</code> in <code>y</code> except <code>y.max()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>OrdinalClassifier</code> <p>Fitted model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the estimator is not a classifier or if it does not implement <code>.predict_proba()</code>.</p> Source code in <code>sklego/meta/ordinal_classification.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the `OrdinalClassifier` model on training data `X` and `y` by fitting its underlying estimators on\n    N-1 datasets `X` and `y` for each class `y_label` in `y` except `y.max()`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n\n    Returns\n    -------\n    self : OrdinalClassifier\n        Fitted model.\n\n    Raises\n    ------\n    ValueError\n        If the estimator is not a classifier or if it does not implement `.predict_proba()`.\n    \"\"\"\n\n    if not is_classifier(self.estimator):\n        raise ValueError(\"The estimator must be a classifier.\")\n\n    if not hasattr(self.estimator, \"predict_proba\"):\n        raise ValueError(\"The estimator must implement `.predict_proba()` method.\")\n\n    X, y = check_X_y(X, y, estimator=self, ensure_min_samples=2)\n\n    self.classes_ = np.sort(np.unique(y))\n    self.n_features_in_ = X.shape[1]\n\n    if self.n_classes_ &lt; 3:\n        raise ValueError(\"`OrdinalClassifier` can't train when less than 3 classes are present.\")\n\n    if self.n_jobs is None or self.n_jobs == 1:\n        self.estimators_ = {y_label: self._fit_binary_estimator(X, y, y_label) for y_label in self.classes_[:-1]}\n    else:\n        self.estimators_ = dict(\n            zip(\n                self.classes_[:-1],\n                Parallel(n_jobs=self.n_jobs)(\n                    delayed(self._fit_binary_estimator)(X, y, y_label) for y_label in self.classes_[:-1]\n                ),\n            )\n        )\n\n    return self\n</code></pre>"},{"location":"api/meta/#sklego.meta.ordinal_classification.OrdinalClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict class labels for samples in <code>X</code> as the class with the highest probability.</p> Source code in <code>sklego/meta/ordinal_classification.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict class labels for samples in `X` as the class with the highest probability.\"\"\"\n    check_is_fitted(self, [\"estimators_\", \"classes_\"])\n    return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n</code></pre>"},{"location":"api/meta/#sklego.meta.ordinal_classification.OrdinalClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict class probabilities for samples in <code>X</code>. The class probabilities of a sample are computed as the difference between the probability of the sample to be in class <code>y_label</code> and the probability of the sample to be in class <code>y_label - 1</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_classes)</code> <p>The predicted class probabilities.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>X</code> has a different number of features than the one seen during <code>fit</code>.</p> Source code in <code>sklego/meta/ordinal_classification.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict class probabilities for samples in `X`. The class probabilities of a sample are computed as the\n    difference between the probability of the sample to be in class `y_label` and the probability of the sample to\n    be in class `y_label - 1`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_classes)\n        The predicted class probabilities.\n\n    Raises\n    ------\n    ValueError\n        If `X` has a different number of features than the one seen during `fit`.\n    \"\"\"\n    check_is_fitted(self, [\"estimators_\", \"classes_\"])\n    X = check_array(X, ensure_2d=True, estimator=self)\n\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(f\"X has {X.shape[1]} features, expected {self.n_features_in_} features.\")\n\n    raw_proba = np.array([estimator.predict_proba(X)[:, 1] for estimator in self.estimators_.values()]).T\n    p_y_le = np.column_stack((np.zeros(X.shape[0]), raw_proba, np.ones(X.shape[0])))\n\n    # Equivalent to (p_y_le[:, 1:] - p_y_le[:, :-1])\n    return np.diff(p_y_le, n=1, axis=1)\n</code></pre>"},{"location":"api/meta/#sklego.meta.outlier_classifier.OutlierClassifier","title":"<code>sklego.meta.outlier_classifier.OutlierClassifier</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Morphs an outlier detection model into a classifier.</p> <p>When an outlier is detected it will output 1 and 0 otherwise. This way you can use familiar metrics again and this allows you to consider outlier models as a fraud detector.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>scikit-learn compatible outlier detection model</code> <p>An outlier detection model that will be used for prediction.</p> required <p>Attributes:</p> Name Type Description <code>estimator_</code> <code>scikit-learn compatible outlier detection model</code> <p>The fitted underlying outlier detection model.</p> <code>classes_</code> <code>array-like of shape (2,)</code> <p>Classes used for prediction (0 or 1)</p> Source code in <code>sklego/meta/outlier_classifier.py</code> <pre><code>class OutlierClassifier(BaseEstimator, ClassifierMixin):\n    \"\"\"Morphs an outlier detection model into a classifier.\n\n    When an outlier is detected it will output 1 and 0 otherwise. This way you can use familiar metrics again and\n    this allows you to consider outlier models as a fraud detector.\n\n    Parameters\n    ----------\n    model : scikit-learn compatible outlier detection model\n        An outlier detection model that will be used for prediction.\n\n    Attributes\n    ----------\n    estimator_ : scikit-learn compatible outlier detection model\n        The fitted underlying outlier detection model.\n    classes_ : array-like of shape (2,)\n        Classes used for prediction (0 or 1)\n    \"\"\"\n\n    def __init__(self, model):\n        self.model = model\n\n    def _is_outlier_model(self):\n        \"\"\"Check if the underlying model is an outlier detection model.\"\"\"\n        return isinstance(self.model, OutlierModel)\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the underlying estimator to the training data `X` and `y`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,), default=None\n            Target values.\n\n        Returns\n        -------\n        self : OutlierClassifier\n            The fitted estimator.\n\n        Raises\n        ------\n        ValueError\n            - If the underlying model is not an outlier detection model.\n            - If the underlying model does not have a `decision_function` method.\n        \"\"\"\n        X, y = check_X_y(X, y, estimator=self)\n        if not self._is_outlier_model():\n            raise ValueError(\"Passed model does not detect outliers!\")\n        if not hasattr(self.model, \"decision_function\"):\n            raise ValueError(\n                f\"Passed model {self.model} does not have a `decision_function` \"\n                f\"method. This is required for `predict_proba` estimation.\"\n            )\n        self.estimator_ = self.model.fit(X, y)\n        self.classes_ = np.array([0, 1])\n\n        # fit sigmoid function for `predict_proba`\n        decision_function_scores = self.estimator_.decision_function(X)\n        self._predict_proba_sigmoid = _SigmoidCalibration().fit(decision_function_scores, y)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict new data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        np.ndarray of shape (n_samples,)\n            The predicted values. 0 for inliers, 1 for outliers.\n        \"\"\"\n        check_is_fitted(self, [\"estimator_\", \"classes_\"])\n        preds = self.estimator_.predict(X)\n        result = np.zeros(preds.shape)\n        result[preds == -1] = 1\n        return result\n\n    def predict_proba(self, X):\n        \"\"\"Predict probability estimates for new data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        np.ndarray of shape (n_samples,)\n            The predicted probabilities.\n        \"\"\"\n        check_is_fitted(self, [\"estimator_\", \"classes_\"])\n        decision_function_scores = self.estimator_.decision_function(X)\n        probabilities = self._predict_proba_sigmoid.predict(decision_function_scores).reshape(-1, 1)\n        complement = np.ones_like(probabilities) - probabilities\n        return np.hstack((complement, probabilities))\n</code></pre>"},{"location":"api/meta/#sklego.meta.outlier_classifier.OutlierClassifier.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the underlying estimator to the training data <code>X</code> and <code>y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target values.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>OutlierClassifier</code> <p>The fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the underlying model is not an outlier detection model.</li> <li>If the underlying model does not have a <code>decision_function</code> method.</li> </ul> Source code in <code>sklego/meta/outlier_classifier.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the underlying estimator to the training data `X` and `y`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training data.\n    y : array-like of shape (n_samples,), default=None\n        Target values.\n\n    Returns\n    -------\n    self : OutlierClassifier\n        The fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        - If the underlying model is not an outlier detection model.\n        - If the underlying model does not have a `decision_function` method.\n    \"\"\"\n    X, y = check_X_y(X, y, estimator=self)\n    if not self._is_outlier_model():\n        raise ValueError(\"Passed model does not detect outliers!\")\n    if not hasattr(self.model, \"decision_function\"):\n        raise ValueError(\n            f\"Passed model {self.model} does not have a `decision_function` \"\n            f\"method. This is required for `predict_proba` estimation.\"\n        )\n    self.estimator_ = self.model.fit(X, y)\n    self.classes_ = np.array([0, 1])\n\n    # fit sigmoid function for `predict_proba`\n    decision_function_scores = self.estimator_.decision_function(X)\n    self._predict_proba_sigmoid = _SigmoidCalibration().fit(decision_function_scores, y)\n\n    return self\n</code></pre>"},{"location":"api/meta/#sklego.meta.outlier_classifier.OutlierClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict new data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>np.ndarray of shape (n_samples,)</code> <p>The predicted values. 0 for inliers, 1 for outliers.</p> Source code in <code>sklego/meta/outlier_classifier.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict new data.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    np.ndarray of shape (n_samples,)\n        The predicted values. 0 for inliers, 1 for outliers.\n    \"\"\"\n    check_is_fitted(self, [\"estimator_\", \"classes_\"])\n    preds = self.estimator_.predict(X)\n    result = np.zeros(preds.shape)\n    result[preds == -1] = 1\n    return result\n</code></pre>"},{"location":"api/meta/#sklego.meta.outlier_classifier.OutlierClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict probability estimates for new data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>np.ndarray of shape (n_samples,)</code> <p>The predicted probabilities.</p> Source code in <code>sklego/meta/outlier_classifier.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict probability estimates for new data.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    np.ndarray of shape (n_samples,)\n        The predicted probabilities.\n    \"\"\"\n    check_is_fitted(self, [\"estimator_\", \"classes_\"])\n    decision_function_scores = self.estimator_.decision_function(X)\n    probabilities = self._predict_proba_sigmoid.predict(decision_function_scores).reshape(-1, 1)\n    complement = np.ones_like(probabilities) - probabilities\n    return np.hstack((complement, probabilities))\n</code></pre>"},{"location":"api/meta/#sklego.meta.regression_outlier_detector.RegressionOutlierDetector","title":"<code>sklego.meta.regression_outlier_detector.RegressionOutlierDetector</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>OutlierMixin</code></p> <p>Morphs a regression estimator into one that can detect outliers. We will try to predict <code>column</code> in X.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>scikit-learn compatible regression model</code> <p>A regression model that will be used for prediction.</p> required <code>column</code> <code>int</code> <p>The index of the target column to predict in the input data.</p> required <code>lower</code> <code>float</code> <p>Lower threshold for outlier detection. The method used for detection depends on the <code>method</code> parameter.</p> <code>2.0</code> <code>upper</code> <code>float</code> <p>Upper threshold for outlier detection. The method used for detection depends on the <code>method</code> parameter.</p> <code>2.0</code> <code>method</code> <code>Literal[sd, relative, absolute]</code> <p>The method to use for outlier detection.</p> <ul> <li><code>\"sd\"</code> uses standard deviation</li> <li><code>\"relative\"</code> uses relative difference</li> <li><code>\"absolute\"</code> uses absolute difference</li> </ul> <code>\"sd\"</code> <p>Attributes:</p> Name Type Description <code>estimator_</code> <code>scikit-learn compatible regression model</code> <p>The fitted underlying regression model.</p> <code>sd_</code> <code>float</code> <p>The standard deviation of the differences between true and predicted values.</p> <code>idx_</code> <code>int</code> <p>The index of the target column in the input data.</p> Source code in <code>sklego/meta/regression_outlier_detector.py</code> <pre><code>class RegressionOutlierDetector(BaseEstimator, OutlierMixin):\n    \"\"\"Morphs a regression estimator into one that can detect outliers. We will try to predict `column` in X.\n\n    Parameters\n    ----------\n    model : scikit-learn compatible regression model\n        A regression model that will be used for prediction.\n    column : int\n        The index of the target column to predict in the input data.\n    lower : float, default=2.0\n        Lower threshold for outlier detection. The method used for detection depends on the `method` parameter.\n    upper : float, default=2.0\n        Upper threshold for outlier detection. The method used for detection depends on the `method` parameter.\n    method : Literal[\"sd\", \"relative\", \"absolute\"], default=\"sd\"\n        The method to use for outlier detection.\n\n        - `\"sd\"` uses standard deviation\n        - `\"relative\"` uses relative difference\n        - `\"absolute\"` uses absolute difference\n\n    Attributes\n    ----------\n    estimator_ : scikit-learn compatible regression model\n        The fitted underlying regression model.\n    sd_ : float\n        The standard deviation of the differences between true and predicted values.\n    idx_ : int\n        The index of the target column in the input data.\n    \"\"\"\n\n    def __init__(self, model, column, lower=2, upper=2, method=\"sd\"):\n        self.model = model\n        self.column = column\n        self.lower = lower\n        self.upper = upper\n        self.method = method\n\n    def _is_regression_model(self):\n        \"\"\"Check if the underlying model is a regression model.\"\"\"\n        return any([\"RegressorMixin\" in p.__name__ for p in type(self.model).__bases__])\n\n    def _handle_thresholds(self, y_true, y_pred):\n        \"\"\"Compute if a sample is an outlier based on the `method` parameter.\"\"\"\n        difference = y_true - y_pred\n        results = np.ones(difference.shape, dtype=int)\n        allowed_methods = [\"sd\", \"relative\", \"absolute\"]\n        if self.method not in allowed_methods:\n            ValueError(f\"`method` must be in {allowed_methods} got: {self.method}\")\n        if self.method == \"sd\":\n            lower_limit_hit = -self.lower * self.sd_ &gt; difference\n            upper_limit_hit = self.upper * self.sd_ &lt; difference\n        if self.method == \"relative\":\n            lower_limit_hit = -self.lower &gt; difference / y_true\n            upper_limit_hit = self.upper &lt; difference / y_true\n        if self.method == \"absolute\":\n            lower_limit_hit = -self.lower &gt; difference\n            upper_limit_hit = self.upper &lt; difference\n        results[lower_limit_hit] = -1\n        results[upper_limit_hit] = -1\n        return results\n\n    def to_x_y(self, X):\n        \"\"\"Split `X` into two arrays `X_to_use` and `y`.\n        `y` is the column we want to predict (specified in the `column` parameter) and `X_to_use` is the rest of the\n        data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data to split.\n\n        Returns\n        -------\n        X_to_use : array-like of shape (n_samples, n_features-1)\n            Data to use for prediction.\n        y : array-like of shape (n_samples,)\n            The target column.\n        \"\"\"\n        y = X[:, self.idx_]\n        cols_to_use = [i for i in range(X.shape[1]) if i != self.column]\n        X_to_use = X[:, cols_to_use]\n        if len(X_to_use.shape) == 1:\n            X_to_use = X_to_use.reshape(-1, 1)\n        return X_to_use, y\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the underlying model on `X_to_use` and `y` where:\n\n        - `y` is the column we want to predict (`X[:, self.column]`)\n        - `X_to_use` is the rest of the data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,), default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : RegressionOutlierDetector\n            The fitted estimator.\n\n        Raises\n        ------\n        ValueError\n            If the `model` is not a regression estimator.\n        \"\"\"\n        self.idx_ = np.argmax([i == self.column for i in X.columns]) if isinstance(X, pd.DataFrame) else self.column\n        X = check_array(X, estimator=self)\n        if not self._is_regression_model():\n            raise ValueError(\"Passed model must be regression!\")\n        X, y = self.to_x_y(X)\n        self.estimator_ = self.model.fit(X, y)\n        self.sd_ = np.std(self.estimator_.predict(X) - y)\n        return self\n\n    def predict(self, X, y=None):\n        \"\"\"Predict which samples of `X` are outliers using the underlying estimator and given `method`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n        y : array-like of shape (n_samples,), default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        np.ndarray of shape (n_samples,)\n            The predicted values. 1 for inliers, -1 for outliers.\n        \"\"\"\n        check_is_fitted(self, [\"estimator_\", \"sd_\", \"idx_\"])\n        X = check_array(X, estimator=self)\n        X, y = self.to_x_y(X)\n        preds = self.estimator_.predict(X)\n        return self._handle_thresholds(y, preds)\n\n    def score_samples(self, X, y=None):\n        \"\"\"Calculate the outlier scores for the given data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data for which outlier scores are calculated.\n        y : array-like of shape shape=(n_samples,), default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        np.ndarray of shape (n_samples,)\n            The outlier scores for the input data.\n\n        Raises\n        ------\n        ValueError\n            If `method` is not one of \"sd\", \"relative\", or \"absolute\".\n        \"\"\"\n        check_is_fitted(self, [\"estimator_\", \"sd_\", \"idx_\"])\n        X = check_array(X, estimator=self)\n        X, y_true = self.to_x_y(X)\n        y_pred = self.estimator_.predict(X)\n        difference = y_true - y_pred\n        allowed_methods = [\"sd\", \"relative\", \"absolute\"]\n        if self.method not in allowed_methods:\n            ValueError(f\"`method` must be in {allowed_methods} got: {self.method}\")\n        if self.method == \"sd\":\n            return difference\n        if self.method == \"relative\":\n            return difference / y_true\n        if self.method == \"absolute\":\n            return difference\n</code></pre>"},{"location":"api/meta/#sklego.meta.regression_outlier_detector.RegressionOutlierDetector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the underlying model on <code>X_to_use</code> and <code>y</code> where:</p> <ul> <li><code>y</code> is the column we want to predict (<code>X[:, self.column]</code>)</li> <li><code>X_to_use</code> is the rest of the data.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>RegressionOutlierDetector</code> <p>The fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>model</code> is not a regression estimator.</p> Source code in <code>sklego/meta/regression_outlier_detector.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the underlying model on `X_to_use` and `y` where:\n\n    - `y` is the column we want to predict (`X[:, self.column]`)\n    - `X_to_use` is the rest of the data.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training data.\n    y : array-like of shape (n_samples,), default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : RegressionOutlierDetector\n        The fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        If the `model` is not a regression estimator.\n    \"\"\"\n    self.idx_ = np.argmax([i == self.column for i in X.columns]) if isinstance(X, pd.DataFrame) else self.column\n    X = check_array(X, estimator=self)\n    if not self._is_regression_model():\n        raise ValueError(\"Passed model must be regression!\")\n    X, y = self.to_x_y(X)\n    self.estimator_ = self.model.fit(X, y)\n    self.sd_ = np.std(self.estimator_.predict(X) - y)\n    return self\n</code></pre>"},{"location":"api/meta/#sklego.meta.regression_outlier_detector.RegressionOutlierDetector.predict","title":"<code>predict(X, y=None)</code>","text":"<p>Predict which samples of <code>X</code> are outliers using the underlying estimator and given <code>method</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray of shape (n_samples,)</code> <p>The predicted values. 1 for inliers, -1 for outliers.</p> Source code in <code>sklego/meta/regression_outlier_detector.py</code> <pre><code>def predict(self, X, y=None):\n    \"\"\"Predict which samples of `X` are outliers using the underlying estimator and given `method`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n    y : array-like of shape (n_samples,), default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    np.ndarray of shape (n_samples,)\n        The predicted values. 1 for inliers, -1 for outliers.\n    \"\"\"\n    check_is_fitted(self, [\"estimator_\", \"sd_\", \"idx_\"])\n    X = check_array(X, estimator=self)\n    X, y = self.to_x_y(X)\n    preds = self.estimator_.predict(X)\n    return self._handle_thresholds(y, preds)\n</code></pre>"},{"location":"api/meta/#sklego.meta.regression_outlier_detector.RegressionOutlierDetector.score_samples","title":"<code>score_samples(X, y=None)</code>","text":"<p>Calculate the outlier scores for the given data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Data for which outlier scores are calculated.</p> required <code>y</code> <code>array-like of shape shape=(n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray of shape (n_samples,)</code> <p>The outlier scores for the input data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>method</code> is not one of \"sd\", \"relative\", or \"absolute\".</p> Source code in <code>sklego/meta/regression_outlier_detector.py</code> <pre><code>def score_samples(self, X, y=None):\n    \"\"\"Calculate the outlier scores for the given data.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data for which outlier scores are calculated.\n    y : array-like of shape shape=(n_samples,), default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    np.ndarray of shape (n_samples,)\n        The outlier scores for the input data.\n\n    Raises\n    ------\n    ValueError\n        If `method` is not one of \"sd\", \"relative\", or \"absolute\".\n    \"\"\"\n    check_is_fitted(self, [\"estimator_\", \"sd_\", \"idx_\"])\n    X = check_array(X, estimator=self)\n    X, y_true = self.to_x_y(X)\n    y_pred = self.estimator_.predict(X)\n    difference = y_true - y_pred\n    allowed_methods = [\"sd\", \"relative\", \"absolute\"]\n    if self.method not in allowed_methods:\n        ValueError(f\"`method` must be in {allowed_methods} got: {self.method}\")\n    if self.method == \"sd\":\n        return difference\n    if self.method == \"relative\":\n        return difference / y_true\n    if self.method == \"absolute\":\n        return difference\n</code></pre>"},{"location":"api/meta/#sklego.meta.regression_outlier_detector.RegressionOutlierDetector.to_x_y","title":"<code>to_x_y(X)</code>","text":"<p>Split <code>X</code> into two arrays <code>X_to_use</code> and <code>y</code>. <code>y</code> is the column we want to predict (specified in the <code>column</code> parameter) and <code>X_to_use</code> is the rest of the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Data to split.</p> required <p>Returns:</p> Name Type Description <code>X_to_use</code> <code>array-like of shape (n_samples, n_features-1)</code> <p>Data to use for prediction.</p> <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target column.</p> Source code in <code>sklego/meta/regression_outlier_detector.py</code> <pre><code>def to_x_y(self, X):\n    \"\"\"Split `X` into two arrays `X_to_use` and `y`.\n    `y` is the column we want to predict (specified in the `column` parameter) and `X_to_use` is the rest of the\n    data.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data to split.\n\n    Returns\n    -------\n    X_to_use : array-like of shape (n_samples, n_features-1)\n        Data to use for prediction.\n    y : array-like of shape (n_samples,)\n        The target column.\n    \"\"\"\n    y = X[:, self.idx_]\n    cols_to_use = [i for i in range(X.shape[1]) if i != self.column]\n    X_to_use = X[:, cols_to_use]\n    if len(X_to_use.shape) == 1:\n        X_to_use = X_to_use.reshape(-1, 1)\n    return X_to_use, y\n</code></pre>"},{"location":"api/meta/#sklego.meta.subjective_classifier.SubjectiveClassifier","title":"<code>sklego.meta.subjective_classifier.SubjectiveClassifier</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code>, <code>MetaEstimatorMixin</code></p> <p>Corrects predictions of the inner classifier by taking into account a (subjective) prior distribution of the classes.</p> <p>This can be useful when there is a difference in class distribution between the training data set and the real world. Using the confusion matrix of the inner classifier and the prior, the posterior probability for a class, given the prediction of the inner classifier, can be computed.</p> <p>The background for this posterior estimation is given in this article.</p> <p>Based on the <code>evidence</code> attribute, this meta estimator's predictions are based on simple weighing of the inner estimator's <code>predict_proba()</code> results, the posterior probabilities based on the confusion matrix, or a combination of the two approaches.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>scikit-learn compatible classifier</code> <p>Classifier that will be wrapped with SubjectiveClassifier. It should implement <code>predict_proba</code> method.</p> required <code>prior</code> <code>dict[int, float]</code> <p>A dictionary mapping <code>class -&gt; frequency</code> representing the prior (a.k.a. subjective real-world) class distribution. The class frequencies should sum to 1.</p> required <code>evidence</code> <code>Literal[predict_proba, confusion_matrix, both]</code> <p>A string indicating which evidence should be used to correct the inner estimator's predictions.</p> <ul> <li>If <code>\"both\"</code> the the inner estimator's <code>predict_proba()</code> results are multiplied by the posterior probabilities.</li> <li>If <code>\"predict_proba\"</code>, the inner estimator's <code>predict_proba()</code> results are multiplied by the prior     distribution.</li> <li>If <code>\"confusion_matrix\"</code>, the inner estimator's discrete predictions are converted to posterior probabilities     using the prior and the inner estimator's confusion matrix (obtained from the train data used in <code>fit()</code>).</li> </ul> <code>\"both\"</code> <p>Attributes:</p> Name Type Description <code>estimator_</code> <code>scikit-learn compatible classifier</code> <p>The fitted classifier.</p> <code>classes_</code> <code>array-like, shape=(n_classes,)</code> <p>The classes labels.</p> <code>posterior_matrix_</code> <code>array-like, shape=(n_classes, n_classes)</code> <p>The posterior probabilities for each class, given the prediction of the inner classifier.</p> Source code in <code>sklego/meta/subjective_classifier.py</code> <pre><code>class SubjectiveClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):\n    \"\"\"Corrects predictions of the inner classifier by taking into account a (subjective) prior distribution of the\n    classes.\n\n    This can be useful when there is a difference in class distribution between the training data set and the real\n    world. Using the confusion matrix of the inner classifier and the prior, the posterior probability for a class,\n    given the prediction of the inner classifier, can be computed.\n\n    The background for this posterior estimation is given in\n    [this article](https://lucdemortier.github.io/articles/16/PerformanceMetrics).\n\n    Based on the `evidence` attribute, this meta estimator's predictions are based on simple weighing of the inner\n    estimator's `predict_proba()` results, the posterior probabilities based on the confusion matrix, or a combination\n    of the two approaches.\n\n    Parameters\n    ----------\n    estimator : scikit-learn compatible classifier\n        Classifier that will be wrapped with SubjectiveClassifier. It should implement `predict_proba` method.\n    prior : dict[int, float]\n        A dictionary mapping `class -&gt; frequency` representing the prior (a.k.a. subjective real-world) class\n        distribution. The class frequencies should sum to 1.\n    evidence : Literal[\"predict_proba\", \"confusion_matrix\", \"both\"], default=\"both\"\n        A string indicating which evidence should be used to correct the inner estimator's predictions.\n\n        - If `\"both\"` the the inner estimator's `predict_proba()` results are multiplied by the posterior probabilities.\n        - If `\"predict_proba\"`, the inner estimator's `predict_proba()` results are multiplied by the prior\n            distribution.\n        - If `\"confusion_matrix\"`, the inner estimator's discrete predictions are converted to posterior probabilities\n            using the prior and the inner estimator's confusion matrix (obtained from the train data used in `fit()`).\n\n    Attributes\n    ----------\n    estimator_ : scikit-learn compatible classifier\n        The fitted classifier.\n    classes_ : array-like, shape=(n_classes,)\n        The classes labels.\n    posterior_matrix_ : array-like, shape=(n_classes, n_classes)\n        The posterior probabilities for each class, given the prediction of the inner classifier.\n    \"\"\"\n\n    def __init__(self, estimator, prior, evidence=\"both\"):\n        self.estimator = estimator\n        self.prior = prior\n        self.evidence = evidence\n\n    def _likelihood(self, predicted_class, given_class, cfm):\n        return cfm[given_class, predicted_class] / cfm[given_class, :].sum()\n\n    def _evidence(self, predicted_class, cfm):\n        return sum(\n            [\n                self._likelihood(predicted_class, given_class, cfm) * self.prior[self.classes_[given_class]]\n                for given_class in range(cfm.shape[0])\n            ]\n        )\n\n    def _posterior(self, y, y_hat, cfm):\n        y_hat_evidence = self._evidence(y_hat, cfm)\n        return (\n            (self._likelihood(y_hat, y, cfm) * self.prior[self.classes_[y]] / y_hat_evidence)\n            if y_hat_evidence &gt; 0\n            else self.prior[y]  # in case confusion matrix has all-zero column for y_hat\n        )\n\n    def fit(self, X, y):\n        \"\"\"Fit the inner classfier using `X` and `y` as training data by fitting the underlying estimator and computing\n        the posterior probabilities.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n\n        Returns\n        -------\n        self : SubjectiveClassifier\n            The fitted estimator.\n\n        Raises\n        ------\n        ValueError\n            - If `estimator` is not a classifier.\n            - If `y` contains classes that are not specified in the `prior`\n            - If `prior` is not a valid probability distribution (i.e. does not sum to 1).\n            - If `evidence` is not one of \"predict_proba\", \"confusion_matrix\", or \"both\".\n        \"\"\"\n        if not isinstance(self.estimator, ClassifierMixin):\n            raise ValueError(\n                \"Invalid inner estimator: the SubjectiveClassifier meta model only works on classification models\"\n            )\n\n        if not np.isclose(sum(self.prior.values()), 1):\n            raise ValueError(\"Invalid prior: the prior probabilities of all classes should sum to 1\")\n\n        valid_evidence_types = [\"predict_proba\", \"confusion_matrix\", \"both\"]\n        if self.evidence not in valid_evidence_types:\n            raise ValueError(f\"Invalid evidence: the provided evidence should be one of {valid_evidence_types}\")\n\n        X, y = check_X_y(X, y, estimator=self.estimator, dtype=FLOAT_DTYPES)\n        if set(y) - set(self.prior.keys()):\n            raise ValueError(\n                f\"Training data is inconsistent with prior: no prior defined for classes \"\n                f\"{set(y) - set(self.prior.keys())}\"\n            )\n        self.estimator.fit(X, y)\n        cfm = confusion_matrix(y, self.estimator.predict(X))\n        self.posterior_matrix_ = np.array(\n            [[self._posterior(y, y_hat, cfm) for y_hat in range(cfm.shape[0])] for y in range(cfm.shape[0])]\n        )\n        return self\n\n    @staticmethod\n    def _weighted_proba(weights, y_hat_probas):\n        return normalize(weights * y_hat_probas, norm=\"l1\")\n\n    @staticmethod\n    def _to_discrete(y_hat_probas):\n        y_hat_discrete = np.zeros(y_hat_probas.shape)\n        y_hat_discrete[np.arange(y_hat_probas.shape[0]), y_hat_probas.argmax(axis=1)] = 1\n        return y_hat_discrete\n\n    def predict_proba(self, X):\n        \"\"\"Predict probability distribution of the class, based on the provided data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_classes)\n            The predicted probabilities.\n        \"\"\"\n        check_is_fitted(self, [\"posterior_matrix_\"])\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        y_hats = self.estimator.predict_proba(X)  # these are ignorant of the prior\n\n        if self.evidence == \"predict_proba\":\n            prior_weights = np.array([self.prior[klass] for klass in self.classes_])\n            return self._weighted_proba(prior_weights, y_hats)\n        else:\n            posterior_probas = self._to_discrete(y_hats) @ self.posterior_matrix_.T\n            return self._weighted_proba(posterior_probas, y_hats) if self.evidence == \"both\" else posterior_probas\n\n    def predict(self, X):\n        \"\"\"Predict target values for `X` using fitted estimator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples, )\n            The predicted class.\n        \"\"\"\n        check_is_fitted(self, [\"posterior_matrix_\"])\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        return self.classes_[self.predict_proba(X).argmax(axis=1)]\n\n    @property\n    def classes_(self):\n        \"\"\"Alias for `.classes_` attribute of the underlying estimator.\"\"\"\n        return self.estimator.classes_\n</code></pre>"},{"location":"api/meta/#sklego.meta.subjective_classifier.SubjectiveClassifier.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>Alias for <code>.classes_</code> attribute of the underlying estimator.</p>"},{"location":"api/meta/#sklego.meta.subjective_classifier.SubjectiveClassifier.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the inner classfier using <code>X</code> and <code>y</code> as training data by fitting the underlying estimator and computing the posterior probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>SubjectiveClassifier</code> <p>The fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If <code>estimator</code> is not a classifier.</li> <li>If <code>y</code> contains classes that are not specified in the <code>prior</code></li> <li>If <code>prior</code> is not a valid probability distribution (i.e. does not sum to 1).</li> <li>If <code>evidence</code> is not one of \"predict_proba\", \"confusion_matrix\", or \"both\".</li> </ul> Source code in <code>sklego/meta/subjective_classifier.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the inner classfier using `X` and `y` as training data by fitting the underlying estimator and computing\n    the posterior probabilities.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n\n    Returns\n    -------\n    self : SubjectiveClassifier\n        The fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        - If `estimator` is not a classifier.\n        - If `y` contains classes that are not specified in the `prior`\n        - If `prior` is not a valid probability distribution (i.e. does not sum to 1).\n        - If `evidence` is not one of \"predict_proba\", \"confusion_matrix\", or \"both\".\n    \"\"\"\n    if not isinstance(self.estimator, ClassifierMixin):\n        raise ValueError(\n            \"Invalid inner estimator: the SubjectiveClassifier meta model only works on classification models\"\n        )\n\n    if not np.isclose(sum(self.prior.values()), 1):\n        raise ValueError(\"Invalid prior: the prior probabilities of all classes should sum to 1\")\n\n    valid_evidence_types = [\"predict_proba\", \"confusion_matrix\", \"both\"]\n    if self.evidence not in valid_evidence_types:\n        raise ValueError(f\"Invalid evidence: the provided evidence should be one of {valid_evidence_types}\")\n\n    X, y = check_X_y(X, y, estimator=self.estimator, dtype=FLOAT_DTYPES)\n    if set(y) - set(self.prior.keys()):\n        raise ValueError(\n            f\"Training data is inconsistent with prior: no prior defined for classes \"\n            f\"{set(y) - set(self.prior.keys())}\"\n        )\n    self.estimator.fit(X, y)\n    cfm = confusion_matrix(y, self.estimator.predict(X))\n    self.posterior_matrix_ = np.array(\n        [[self._posterior(y, y_hat, cfm) for y_hat in range(cfm.shape[0])] for y in range(cfm.shape[0])]\n    )\n    return self\n</code></pre>"},{"location":"api/meta/#sklego.meta.subjective_classifier.SubjectiveClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict target values for <code>X</code> using fitted estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, )</code> <p>The predicted class.</p> Source code in <code>sklego/meta/subjective_classifier.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict target values for `X` using fitted estimator.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples, )\n        The predicted class.\n    \"\"\"\n    check_is_fitted(self, [\"posterior_matrix_\"])\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    return self.classes_[self.predict_proba(X).argmax(axis=1)]\n</code></pre>"},{"location":"api/meta/#sklego.meta.subjective_classifier.SubjectiveClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict probability distribution of the class, based on the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_classes)</code> <p>The predicted probabilities.</p> Source code in <code>sklego/meta/subjective_classifier.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict probability distribution of the class, based on the provided data.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_classes)\n        The predicted probabilities.\n    \"\"\"\n    check_is_fitted(self, [\"posterior_matrix_\"])\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    y_hats = self.estimator.predict_proba(X)  # these are ignorant of the prior\n\n    if self.evidence == \"predict_proba\":\n        prior_weights = np.array([self.prior[klass] for klass in self.classes_])\n        return self._weighted_proba(prior_weights, y_hats)\n    else:\n        posterior_probas = self._to_discrete(y_hats) @ self.posterior_matrix_.T\n        return self._weighted_proba(posterior_probas, y_hats) if self.evidence == \"both\" else posterior_probas\n</code></pre>"},{"location":"api/meta/#sklego.meta.thresholder.Thresholder","title":"<code>sklego.meta.thresholder.Thresholder</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Takes a binary classifier and moves the threshold. This way you might design the algorithm to only accept a certain class if the probability for it is larger than, say, 90% instead of 50%.</p> <p>Info</p> <p>Please note that this only works for binary classification problems.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>scikit-learn compatible classifier</code> <p>Classifier that will be wrapped with Thresholder. It should implement <code>predict_proba</code> method.</p> required <code>threshold</code> <code>float</code> <p>The threshold value to use.</p> required <code>refit</code> <code>bool</code> <ul> <li>If True, we will always retrain the model even if it is already fitted.</li> <li>If False we only refit if the original model isn't fitted.</li> </ul> <code>False</code> <p>Attributes:</p> Name Type Description <code>estimator_</code> <code>scikit-learn compatible classifier</code> <p>The fitted classifier.</p> <code>classes_</code> <code>array-like, shape=(2,)</code> <p>The classes labels.</p> Source code in <code>sklego/meta/thresholder.py</code> <pre><code>class Thresholder(BaseEstimator, ClassifierMixin):\n    \"\"\"Takes a binary classifier and moves the threshold. This way you might design the algorithm to only accept a\n    certain class if the probability for it is larger than, say, 90% instead of 50%.\n\n    !!! info\n        Please note that this only works for binary classification problems.\n\n    Parameters\n    ----------\n    model : scikit-learn compatible classifier\n        Classifier that will be wrapped with Thresholder. It should implement `predict_proba` method.\n    threshold : float\n        The threshold value to use.\n    refit : bool, default=False\n\n        - If True, we will always retrain the model even if it is already fitted.\n        - If False we only refit if the original model isn't fitted.\n\n    Attributes\n    ----------\n    estimator_ : scikit-learn compatible classifier\n        The fitted classifier.\n    classes_ : array-like, shape=(2,)\n        The classes labels.\n    \"\"\"\n\n    def __init__(self, model, threshold: float, refit=False):\n        self.model = model\n        self.threshold = threshold\n        self.refit = refit\n\n    def _handle_refit(self, X, y, sample_weight=None):\n        \"\"\"Only refit when we need to, unless `refit=True` is present.\"\"\"\n        if self.refit:\n            self.estimator_ = clone(self.model)\n            self.estimator_.fit(X, y, sample_weight=sample_weight)\n        else:\n            try:\n                _ = self.estimator_.predict(X[:1])\n            except NotFittedError:\n                self.estimator_.fit(X, y, sample_weight=sample_weight)\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the underlying estimator using `X` and `y` as training data. If `refit=True` we will always retrain\n        (a copy of) the estimator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n        sample_weight : array-like of shape (n_samples, ), default=None\n            Individual weights for each sample.\n\n        Returns\n        -------\n        self : Thresholder\n            The fitted estimator.\n\n        Raises\n        ------\n        ValueError\n            - If `model` is not a classifier or it does not implement `predict_proba` method.\n            - If `model` does not have two classes.\n        \"\"\"\n        self.estimator_ = self.model\n        if not isinstance(self.estimator_, ProbabilisticClassifier):\n            raise ValueError(\"The Thresholder meta model only works on classification models with .predict_proba.\")\n        self._handle_refit(X, y, sample_weight)\n        self.classes_ = self.estimator_.classes_\n        if len(self.classes_) != 2:\n            raise ValueError(\"The `Thresholder` meta model only works on models with two classes.\")\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict target values for `X` using fitted estimator and the given `threshold`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        check_is_fitted(self, [\"classes_\", \"estimator_\"])\n        predicate = self.estimator_.predict_proba(X)[:, 1] &gt; self.threshold\n        return np.where(predicate, self.classes_[1], self.classes_[0])\n\n    def predict_proba(self, X):\n        \"\"\"Alias for `.predict_proba()` method of the underlying estimator.\"\"\"\n        check_is_fitted(self, [\"classes_\", \"estimator_\"])\n        return self.estimator_.predict_proba(X)\n\n    def score(self, X, y):\n        \"\"\"Alias for `.score()` method of the underlying estimator.\"\"\"\n        return self.estimator_.score(X, y)\n</code></pre>"},{"location":"api/meta/#sklego.meta.thresholder.Thresholder.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit the underlying estimator using <code>X</code> and <code>y</code> as training data. If <code>refit=True</code> we will always retrain (a copy of) the estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <code>sample_weight</code> <code>array-like of shape (n_samples, )</code> <p>Individual weights for each sample.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>Thresholder</code> <p>The fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If <code>model</code> is not a classifier or it does not implement <code>predict_proba</code> method.</li> <li>If <code>model</code> does not have two classes.</li> </ul> Source code in <code>sklego/meta/thresholder.py</code> <pre><code>def fit(self, X, y, sample_weight=None):\n    \"\"\"Fit the underlying estimator using `X` and `y` as training data. If `refit=True` we will always retrain\n    (a copy of) the estimator.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n    sample_weight : array-like of shape (n_samples, ), default=None\n        Individual weights for each sample.\n\n    Returns\n    -------\n    self : Thresholder\n        The fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        - If `model` is not a classifier or it does not implement `predict_proba` method.\n        - If `model` does not have two classes.\n    \"\"\"\n    self.estimator_ = self.model\n    if not isinstance(self.estimator_, ProbabilisticClassifier):\n        raise ValueError(\"The Thresholder meta model only works on classification models with .predict_proba.\")\n    self._handle_refit(X, y, sample_weight)\n    self.classes_ = self.estimator_.classes_\n    if len(self.classes_) != 2:\n        raise ValueError(\"The `Thresholder` meta model only works on models with two classes.\")\n    return self\n</code></pre>"},{"location":"api/meta/#sklego.meta.thresholder.Thresholder.predict","title":"<code>predict(X)</code>","text":"<p>Predict target values for <code>X</code> using fitted estimator and the given <code>threshold</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted values.</p> Source code in <code>sklego/meta/thresholder.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict target values for `X` using fitted estimator and the given `threshold`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted values.\n    \"\"\"\n    check_is_fitted(self, [\"classes_\", \"estimator_\"])\n    predicate = self.estimator_.predict_proba(X)[:, 1] &gt; self.threshold\n    return np.where(predicate, self.classes_[1], self.classes_[0])\n</code></pre>"},{"location":"api/meta/#sklego.meta.thresholder.Thresholder.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Alias for <code>.predict_proba()</code> method of the underlying estimator.</p> Source code in <code>sklego/meta/thresholder.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Alias for `.predict_proba()` method of the underlying estimator.\"\"\"\n    check_is_fitted(self, [\"classes_\", \"estimator_\"])\n    return self.estimator_.predict_proba(X)\n</code></pre>"},{"location":"api/meta/#sklego.meta.thresholder.Thresholder.score","title":"<code>score(X, y)</code>","text":"<p>Alias for <code>.score()</code> method of the underlying estimator.</p> Source code in <code>sklego/meta/thresholder.py</code> <pre><code>def score(self, X, y):\n    \"\"\"Alias for `.score()` method of the underlying estimator.\"\"\"\n    return self.estimator_.score(X, y)\n</code></pre>"},{"location":"api/meta/#sklego.meta.zero_inflated_regressor.ZeroInflatedRegressor","title":"<code>sklego.meta.zero_inflated_regressor.ZeroInflatedRegressor</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>A meta regressor for zero-inflated datasets, i.e. the targets contain a lot of zeroes.</p> <p><code>ZeroInflatedRegressor</code> consists of a classifier and a regressor.</p> <pre><code>- The classifier's task is to find of if the target is zero or not.\n- The regressor's task is to output a (usually positive) prediction whenever the classifier indicates that the\nthere should be a non-zero prediction.\n</code></pre> <p>The regressor is only trained on examples where the target is non-zero, which makes it easier for it to focus.</p> <p>At prediction time, the classifier is first asked if the output should be zero. If yes, output zero. Otherwise, ask the regressor for its prediction and output it.</p> <p>Parameters:</p> Name Type Description Default <code>classifier</code> <code>scikit-learn compatible classifier</code> <p>A classifier that answers the question \"Should the output be zero?\".</p> required <code>regressor</code> <code>scikit-learn compatible regressor</code> <p>A regressor for predicting the target. Its prediction is only used if <code>classifier</code> says that the output is non-zero.</p> required <p>Attributes:</p> Name Type Description <code>classifier_</code> <code>scikit-learn compatible classifier</code> <p>The fitted classifier.</p> <code>regressor_</code> <code>scikit-learn compatible regressor</code> <p>The fitted regressor.</p> <p>Examples:</p> <pre><code>import numpy as np\nfrom sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\nfrom sklego.meta import ZeroInflatedRegressor\n\nnp.random.seed(0)\nX = np.random.randn(10000, 4)\ny = ((X[:, 0]&gt;0) &amp; (X[:, 1]&gt;0)) * np.abs(X[:, 2] * X[:, 3]**2)\nmodel = ZeroInflatedRegressor(\n    classifier=ExtraTreesClassifier(random_state=0),\n    regressor=ExtraTreesRegressor(random_state=0)\n    )\n\nmodel.fit(X, y)\n# ZeroInflatedRegressor(classifier=ExtraTreesClassifier(random_state=0),\n#                       regressor=ExtraTreesRegressor(random_state=0))\n\nmodel.predict(X)[:5]\n# array([4.91483294, 0.        , 0.        , 0.04941909, 0.        ])\n</code></pre> Source code in <code>sklego/meta/zero_inflated_regressor.py</code> <pre><code>class ZeroInflatedRegressor(BaseEstimator, RegressorMixin):\n    \"\"\"A meta regressor for zero-inflated datasets, i.e. the targets contain a lot of zeroes.\n\n    `ZeroInflatedRegressor` consists of a classifier and a regressor.\n\n        - The classifier's task is to find of if the target is zero or not.\n        - The regressor's task is to output a (usually positive) prediction whenever the classifier indicates that the\n        there should be a non-zero prediction.\n\n    The regressor is only trained on examples where the target is non-zero, which makes it easier for it to focus.\n\n    At prediction time, the classifier is first asked if the output should be zero. If yes, output zero.\n    Otherwise, ask the regressor for its prediction and output it.\n\n    Parameters\n    ----------\n    classifier : scikit-learn compatible classifier\n        A classifier that answers the question \"Should the output be zero?\".\n    regressor : scikit-learn compatible regressor\n        A regressor for predicting the target. Its prediction is only used if `classifier` says that the output is\n        non-zero.\n\n    Attributes\n    ----------\n    classifier_ : scikit-learn compatible classifier\n        The fitted classifier.\n    regressor_ : scikit-learn compatible regressor\n        The fitted regressor.\n\n    Examples\n    --------\n    ```py\n    import numpy as np\n    from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n    from sklego.meta import ZeroInflatedRegressor\n\n    np.random.seed(0)\n    X = np.random.randn(10000, 4)\n    y = ((X[:, 0]&gt;0) &amp; (X[:, 1]&gt;0)) * np.abs(X[:, 2] * X[:, 3]**2)\n    model = ZeroInflatedRegressor(\n        classifier=ExtraTreesClassifier(random_state=0),\n        regressor=ExtraTreesRegressor(random_state=0)\n        )\n\n    model.fit(X, y)\n    # ZeroInflatedRegressor(classifier=ExtraTreesClassifier(random_state=0),\n    #                       regressor=ExtraTreesRegressor(random_state=0))\n\n    model.predict(X)[:5]\n    # array([4.91483294, 0.        , 0.        , 0.04941909, 0.        ])\n    ```\n    \"\"\"\n\n    def __init__(self, classifier, regressor) -&gt; None:\n        self.classifier = classifier\n        self.regressor = regressor\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the underlying classifier and regressor using `X` and `y` as training data. The regressor is only trained\n        on examples where the target is non-zero.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n        sample_weight : array-like of shape (n_samples, ), default=None\n            Individual weights for each sample.\n\n        Returns\n        -------\n        self : ZeroInflatedRegressor\n            The fitted estimator.\n\n        Raises\n        ------\n        ValueError\n            If `classifier` is not a classifier or `regressor` is not a regressor.\n        \"\"\"\n        X, y = check_X_y(X, y)\n        self._check_n_features(X, reset=True)\n        if not is_classifier(self.classifier):\n            raise ValueError(\n                f\"`classifier` has to be a classifier. Received instance of {type(self.classifier)} instead.\"\n            )\n        if not is_regressor(self.regressor):\n            raise ValueError(f\"`regressor` has to be a regressor. Received instance of {type(self.regressor)} instead.\")\n\n        try:\n            check_is_fitted(self.classifier)\n            self.classifier_ = self.classifier\n        except NotFittedError:\n            self.classifier_ = clone(self.classifier)\n\n            if \"sample_weight\" in signature(self.classifier_.fit).parameters:\n                self.classifier_.fit(X, y != 0, sample_weight=sample_weight)\n            else:\n                logging.warning(\"Classifier ignores sample_weight.\")\n                self.classifier_.fit(X, y != 0)\n\n        non_zero_indices = np.where(self.classifier_.predict(X) == 1)[0]\n\n        if non_zero_indices.size &gt; 0:\n            try:\n                check_is_fitted(self.regressor)\n                self.regressor_ = self.regressor\n            except NotFittedError:\n                self.regressor_ = clone(self.regressor)\n\n                if \"sample_weight\" in signature(self.regressor_.fit).parameters:\n                    self.regressor_.fit(\n                        X[non_zero_indices],\n                        y[non_zero_indices],\n                        sample_weight=sample_weight[non_zero_indices] if sample_weight is not None else None,\n                    )\n                else:\n                    logging.warning(\"Regressor ignores sample_weight.\")\n                    self.regressor_.fit(\n                        X[non_zero_indices],\n                        y[non_zero_indices],\n                    )\n        else:\n            raise ValueError(\n                \"\"\"The predicted training labels are all zero, making the regressor obsolete. Change the classifier\n                or use a plain regressor instead.\"\"\"\n            )\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict target values for `X` using fitted estimator by first asking the classifier if the output should be\n        zero. If yes, output zero. Otherwise, ask the regressor for its prediction and output it.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X)\n        self._check_n_features(X, reset=False)\n\n        output = np.zeros(len(X))\n        non_zero_indices = np.where(self.classifier_.predict(X))[0]\n\n        if non_zero_indices.size &gt; 0:\n            output[non_zero_indices] = self.regressor_.predict(X[non_zero_indices])\n\n        return output\n</code></pre>"},{"location":"api/meta/#sklego.meta.zero_inflated_regressor.ZeroInflatedRegressor.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit the underlying classifier and regressor using <code>X</code> and <code>y</code> as training data. The regressor is only trained on examples where the target is non-zero.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <code>sample_weight</code> <code>array-like of shape (n_samples, )</code> <p>Individual weights for each sample.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>ZeroInflatedRegressor</code> <p>The fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>classifier</code> is not a classifier or <code>regressor</code> is not a regressor.</p> Source code in <code>sklego/meta/zero_inflated_regressor.py</code> <pre><code>def fit(self, X, y, sample_weight=None):\n    \"\"\"Fit the underlying classifier and regressor using `X` and `y` as training data. The regressor is only trained\n    on examples where the target is non-zero.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n    sample_weight : array-like of shape (n_samples, ), default=None\n        Individual weights for each sample.\n\n    Returns\n    -------\n    self : ZeroInflatedRegressor\n        The fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        If `classifier` is not a classifier or `regressor` is not a regressor.\n    \"\"\"\n    X, y = check_X_y(X, y)\n    self._check_n_features(X, reset=True)\n    if not is_classifier(self.classifier):\n        raise ValueError(\n            f\"`classifier` has to be a classifier. Received instance of {type(self.classifier)} instead.\"\n        )\n    if not is_regressor(self.regressor):\n        raise ValueError(f\"`regressor` has to be a regressor. Received instance of {type(self.regressor)} instead.\")\n\n    try:\n        check_is_fitted(self.classifier)\n        self.classifier_ = self.classifier\n    except NotFittedError:\n        self.classifier_ = clone(self.classifier)\n\n        if \"sample_weight\" in signature(self.classifier_.fit).parameters:\n            self.classifier_.fit(X, y != 0, sample_weight=sample_weight)\n        else:\n            logging.warning(\"Classifier ignores sample_weight.\")\n            self.classifier_.fit(X, y != 0)\n\n    non_zero_indices = np.where(self.classifier_.predict(X) == 1)[0]\n\n    if non_zero_indices.size &gt; 0:\n        try:\n            check_is_fitted(self.regressor)\n            self.regressor_ = self.regressor\n        except NotFittedError:\n            self.regressor_ = clone(self.regressor)\n\n            if \"sample_weight\" in signature(self.regressor_.fit).parameters:\n                self.regressor_.fit(\n                    X[non_zero_indices],\n                    y[non_zero_indices],\n                    sample_weight=sample_weight[non_zero_indices] if sample_weight is not None else None,\n                )\n            else:\n                logging.warning(\"Regressor ignores sample_weight.\")\n                self.regressor_.fit(\n                    X[non_zero_indices],\n                    y[non_zero_indices],\n                )\n    else:\n        raise ValueError(\n            \"\"\"The predicted training labels are all zero, making the regressor obsolete. Change the classifier\n            or use a plain regressor instead.\"\"\"\n        )\n\n    return self\n</code></pre>"},{"location":"api/meta/#sklego.meta.zero_inflated_regressor.ZeroInflatedRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predict target values for <code>X</code> using fitted estimator by first asking the classifier if the output should be zero. If yes, output zero. Otherwise, ask the regressor for its prediction and output it.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted values.</p> Source code in <code>sklego/meta/zero_inflated_regressor.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict target values for `X` using fitted estimator by first asking the classifier if the output should be\n    zero. If yes, output zero. Otherwise, ask the regressor for its prediction and output it.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted values.\n    \"\"\"\n    check_is_fitted(self)\n    X = check_array(X)\n    self._check_n_features(X, reset=False)\n\n    output = np.zeros(len(X))\n    non_zero_indices = np.where(self.classifier_.predict(X))[0]\n\n    if non_zero_indices.size &gt; 0:\n        output[non_zero_indices] = self.regressor_.predict(X[non_zero_indices])\n\n    return output\n</code></pre>"},{"location":"api/metrics/","title":"Metrics","text":""},{"location":"api/metrics/#sklego.metrics.correlation_score","title":"<code>sklego.metrics.correlation_score(column)</code>","text":"<p>The correlation score can score how well the estimator predictions correlate with a given column.</p> <p>This is especially useful to use in situations where \"fairness\" is a theme.</p> <p><code>correlation_score</code> takes a column on which to calculate the correlation and returns a metric function.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>str | int</code> <p>Name of the column (when X is a dataframe) or the index of the column (when X is a numpy array) to score against.</p> required <p>Returns:</p> Type Description <code>Callable[..., float]</code> <p>A function which calculates the negative correlation between <code>estimator.predict(X)</code> and <code>X[column]</code> (in gridsearch, larger is better and we want to typically punish correlation).</p> <p>Examples:</p> <pre><code>from sklego.metrics import correlation_score\n...\ncorrelation_score('gender')(clf, X, y)\n</code></pre> Source code in <code>sklego/metrics.py</code> <pre><code>def correlation_score(column):\n    \"\"\"The correlation score can score how well the estimator predictions correlate with a given column.\n\n    This is especially useful to use in situations where \"fairness\" is a theme.\n\n    `correlation_score` takes a column on which to calculate the correlation and returns a metric function.\n\n    Parameters\n    ----------\n    column : str | int\n        Name of the column (when X is a dataframe) or the index of the column (when X is a numpy array) to score\n        against.\n\n    Returns\n    -------\n    Callable[..., float]\n        A function which calculates the _negative_ correlation between `estimator.predict(X)` and `X[column]`\n        (in gridsearch, larger is better and we want to typically punish correlation).\n\n    Examples\n    --------\n    ```py\n    from sklego.metrics import correlation_score\n    ...\n    correlation_score('gender')(clf, X, y)\n    ```\n    \"\"\"\n\n    def correlation_metric(estimator, X, y_true=None):\n        \"\"\"Remember: X is the thing going *in* to your pipeline.\"\"\"\n        sensitive_col = X[:, column] if isinstance(X, np.ndarray) else X[column]\n        return -np.abs(np.corrcoef(estimator.predict(X), sensitive_col)[1, 0])\n\n    return correlation_metric\n</code></pre>"},{"location":"api/metrics/#sklego.metrics.equal_opportunity_score","title":"<code>sklego.metrics.equal_opportunity_score(sensitive_column, positive_target=1)</code>","text":"<p>The equality opportunity score calculates the ratio between the probability of a true positive outcome given the sensitive attribute (column) being true and the same probability given the sensitive attribute being false.</p> \\[\\min \\left(\\frac{P(\\hat{y}=1 | z=1, y=1)}{P(\\hat{y}=1 | z=0, y=1)}, \\frac{P(\\hat{y}=1 | z=0, y=1)}{P(\\hat{y}=1 | z=1, y=1)}\\right)\\] <p>This is especially useful to use in situations where \"fairness\" is a theme.</p> <p>Parameters:</p> Name Type Description Default <code>sensitive_column</code> <code>str | int</code> <p>Name of the column containing the binary sensitive attribute (when X is a dataframe) or the index of the column (when X is a numpy array).</p> required <code>positive_target</code> <p>The name of the class which is associated with a positive outcome</p> <code>1</code> <p>Returns:</p> Type Description <code>Callable[..., float]</code> <p>A function which calculates the equal opportunity score for z = column</p> <p>Examples:</p> <pre><code>from sklego.metrics import equal_opportunity_score\n...\nequal_opportunity_score('gender')(clf, X, y)\n</code></pre>"},{"location":"api/metrics/#sklego.metrics.equal_opportunity_score--source","title":"Source","text":"<p>M. Hardt, E. Price and N. Srebro (2016), Equality of Opportunity in Supervised Learning</p> Source code in <code>sklego/metrics.py</code> <pre><code>def equal_opportunity_score(sensitive_column, positive_target=1):\n    r\"\"\"The equality opportunity score calculates the ratio between the probability of a **true positive** outcome\n    given the sensitive attribute (column) being true and the same probability given the sensitive attribute being\n    false.\n\n    $$\\min \\left(\\frac{P(\\hat{y}=1 | z=1, y=1)}{P(\\hat{y}=1 | z=0, y=1)},\n    \\frac{P(\\hat{y}=1 | z=0, y=1)}{P(\\hat{y}=1 | z=1, y=1)}\\right)$$\n\n    This is especially useful to use in situations where \"fairness\" is a theme.\n\n    Parameters\n    ----------\n    sensitive_column : str | int\n        Name of the column containing the binary sensitive attribute (when X is a dataframe) or the index of the column\n        (when X is a numpy array).\n    positive_target: int, default=1\n        The name of the class which is associated with a positive outcome\n\n    Returns\n    -------\n    Callable[..., float]\n        A function which calculates the equal opportunity score for z = column\n\n    Examples\n    --------\n    ```py\n    from sklego.metrics import equal_opportunity_score\n    ...\n    equal_opportunity_score('gender')(clf, X, y)\n    ```\n\n    Source\n    ------\n    M. Hardt, E. Price and N. Srebro (2016), Equality of Opportunity in Supervised Learning\n    \"\"\"\n\n    def impl(estimator, X, y_true):\n        \"\"\"Remember: X is the thing going *in* to your pipeline.\"\"\"\n        sensitive_col = X[:, sensitive_column] if isinstance(X, np.ndarray) else X[sensitive_column]\n\n        if not np.all((sensitive_col == 0) | (sensitive_col == 1)):\n            raise ValueError(\n                f\"equal_opportunity_score only supports binary indicator columns for `column`. \"\n                f\"Found values {np.unique(sensitive_col)}\"\n            )\n\n        y_hat = estimator.predict(X)\n        y_given_z1_y1 = y_hat[(sensitive_col == 1) &amp; (y_true == positive_target)]\n        y_given_z0_y1 = y_hat[(sensitive_col == 0) &amp; (y_true == positive_target)]\n\n        # If we never predict a positive target for one of the subgroups, the model is by definition not\n        # fair so we return 0\n        if len(y_given_z1_y1) == 0:\n            warnings.warn(\n                f\"No samples with y_hat == {positive_target} for {sensitive_column} == 1, returning 0\",\n                RuntimeWarning,\n            )\n            return 0\n\n        if len(y_given_z0_y1) == 0:\n            warnings.warn(\n                f\"No samples with y_hat == {positive_target} for {sensitive_column} == 0, returning 0\",\n                RuntimeWarning,\n            )\n            return 0\n\n        p_y1_z1 = np.mean(y_given_z1_y1 == positive_target)\n        p_y1_z0 = np.mean(y_given_z0_y1 == positive_target)\n        score = np.minimum(p_y1_z1 / p_y1_z0, p_y1_z0 / p_y1_z1)\n        return score if not np.isnan(score) else 1\n\n    return impl\n</code></pre>"},{"location":"api/metrics/#sklego.metrics.p_percent_score","title":"<code>sklego.metrics.p_percent_score(sensitive_column, positive_target=1)</code>","text":"<p>The p_percent score calculates the ratio between the probability of a positive outcome given the sensitive attribute (column) being true and the same probability given the sensitive attribute being false.</p> \\[\\min \\left(\\frac{P(\\hat{y}=1 | z=1)}{P(\\hat{y}=1 | z=0)}, \\frac{P(\\hat{y}=1 | z=0)}{P(\\hat{y}=1 | z=1)}\\right)\\] <p>This is especially useful to use in situations where \"fairness\" is a theme.</p> <p>Parameters:</p> Name Type Description Default <code>sensitive_column</code> <code>str | int</code> <p>Name of the column containing the binary sensitive attribute (when X is a dataframe) or the index of the column (when X is a numpy array).</p> required <code>positive_target</code> <code>int</code> <p>The name of the class which is associated with a positive outcome</p> <code>1</code> <p>Returns:</p> Type Description <code>Callable[..., float]</code> <p>A function which calculates the p percent score for z = column</p> <p>Examples:</p> <pre><code>from sklego.metrics import p_percent_score\n...\np_percent_score('gender')(clf, X, y)\n</code></pre>"},{"location":"api/metrics/#sklego.metrics.p_percent_score--source","title":"Source","text":"<p>M. Zafar et al. (2017), Fairness Constraints: Mechanisms for Fair Classification</p> Source code in <code>sklego/metrics.py</code> <pre><code>def p_percent_score(sensitive_column, positive_target=1):\n    r\"\"\"The p_percent score calculates the ratio between the probability of a positive outcome given the sensitive\n    attribute (column) being true and the same probability given the sensitive attribute being false.\n\n    $$\\min \\left(\\frac{P(\\hat{y}=1 | z=1)}{P(\\hat{y}=1 | z=0)}, \\frac{P(\\hat{y}=1 | z=0)}{P(\\hat{y}=1 | z=1)}\\right)$$\n\n    This is especially useful to use in situations where \"fairness\" is a theme.\n\n    Parameters\n    ----------\n    sensitive_column : str | int\n        Name of the column containing the binary sensitive attribute (when X is a dataframe) or the index of the column\n        (when X is a numpy array).\n    positive_target : int, default=1\n        The name of the class which is associated with a positive outcome\n\n    Returns\n    -------\n    Callable[..., float]\n        A function which calculates the p percent score for z = column\n\n    Examples\n    --------\n    ```py\n    from sklego.metrics import p_percent_score\n    ...\n    p_percent_score('gender')(clf, X, y)\n    ```\n\n    Source\n    ------\n    M. Zafar et al. (2017), Fairness Constraints: Mechanisms for Fair Classification\n    \"\"\"\n\n    def impl(estimator, X, y_true=None):\n        \"\"\"Remember: X is the thing going *in* to your pipeline.\"\"\"\n        sensitive_col = X[:, sensitive_column] if isinstance(X, np.ndarray) else X[sensitive_column]\n\n        if not np.all((sensitive_col == 0) | (sensitive_col == 1)):\n            raise ValueError(\n                f\"p_percent_score only supports binary indicator columns for `column`. \"\n                f\"Found values {np.unique(sensitive_col)}\"\n            )\n\n        y_hat = estimator.predict(X)\n        y_given_z1 = y_hat[sensitive_col == 1]\n        y_given_z0 = y_hat[sensitive_col == 0]\n        p_y1_z1 = np.mean(y_given_z1 == positive_target)\n        p_y1_z0 = np.mean(y_given_z0 == positive_target)\n\n        # If we never predict a positive target for one of the subgroups, the model is by definition not\n        # fair so we return 0\n        if p_y1_z1 == 0:\n            warnings.warn(\n                f\"No samples with y_hat == {positive_target} for {sensitive_column} == 1, returning 0\",\n                RuntimeWarning,\n            )\n            return 0\n\n        if p_y1_z0 == 0:\n            warnings.warn(\n                f\"No samples with y_hat == {positive_target} for {sensitive_column} == 0, returning 0\",\n                RuntimeWarning,\n            )\n            return 0\n\n        p_percent = np.minimum(p_y1_z1 / p_y1_z0, p_y1_z0 / p_y1_z1)\n        return p_percent if not np.isnan(p_percent) else 1\n\n    return impl\n</code></pre>"},{"location":"api/metrics/#sklego.metrics.subset_score","title":"<code>sklego.metrics.subset_score(subset_picker, score, **kwargs)</code>","text":"<p>Return a method that applies the passed score only to a specific subset.</p> <p>The subset picker is a method that is passed the corresponding <code>X</code> and <code>y_true</code> and returns a one-dimensional boolean vector where every element corresponds to a row in the data.</p> <p>Only the elements with a True value are taken into account for the passed score, representing a filter.</p> <p>This allows users to have an easy approach to measuring metrics over different slices of the population which can give insights into the model performance, either specifically for fairness or in general.</p> <p>Parameters:</p> Name Type Description Default <code>subset_picker</code> <code>Callable</code> <p>Method that returns a boolean mask that is used for slicing the samples</p> required <code>score</code> <code>Callable[..., T]</code> <p>The score that needs to be applied to the subset</p> required <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to score</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable[..., T]</code> <p>A function which calculates the passed score for the subset</p> <p>Examples:</p> <pre><code>from sklego.metrics import subset_score\n...\nsubset_score(lambda X, y_true: X['column'] == 'A', accuracy_score)(clf, X, y)\n</code></pre> Source code in <code>sklego/metrics.py</code> <pre><code>def subset_score(subset_picker: Callable, score: Callable, **kwargs):\n    \"\"\"Return a method that applies the passed score only to a specific subset.\n\n    The subset picker is a method that is passed the corresponding `X` and `y_true` and returns a one-dimensional\n    boolean vector where every element corresponds to a row in the data.\n\n    Only the elements with a True value are taken into account for the passed score, representing a filter.\n\n    This allows users to have an easy approach to measuring metrics over different slices of the population which can\n    give insights into the model performance, either specifically for fairness or in general.\n\n    Parameters\n    ----------\n    subset_picker : Callable\n        Method that returns a boolean mask that is used for slicing the samples\n    score : Callable[..., T]\n        The score that needs to be applied to the subset\n    kwargs : dict\n        Additional keyword arguments to pass to score\n\n    Returns\n    -------\n    Callable[..., T]\n        A function which calculates the passed score for the subset\n\n    Examples\n    --------\n    ```py\n    from sklego.metrics import subset_score\n    ...\n    subset_score(lambda X, y_true: X['column'] == 'A', accuracy_score)(clf, X, y)\n    ```\n    \"\"\"\n\n    def sliced_metric(estimator, X, y_true=None):\n        mask = subset_picker(X, y_true)\n        if isinstance(mask, np.ndarray):\n            if len(mask.shape) &gt; 1:\n                raise ValueError(\n                    \"`subset_picker` should return 1-dimensional numpy array or Pandas\"\n                    + \" series, returned {} instead\".format(len(mask.shape))\n                )\n        if np.sum(mask) == 0:\n            warnings.warn(\"No samples in subset, returning NaN\", RuntimeWarning)\n            return np.nan\n        X = X[mask]\n        y_pred = estimator.predict(X)\n        return score(y_true[mask], y_pred, **kwargs)\n\n    return sliced_metric\n</code></pre>"},{"location":"api/mixture/","title":"Mixture Models","text":""},{"location":"api/mixture/#sklego.mixture.bayesian_gmm_classifier.BayesianGMMClassifier","title":"<code>sklego.mixture.bayesian_gmm_classifier.BayesianGMMClassifier</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>The <code>BayesianGMMClassifier</code> trains a Gaussian Mixture Model for each class in <code>y</code> on a dataset <code>X</code>. Once a density is trained for each class we can evaluate the likelihood scores to see which class is more likely.</p> <p>Note</p> <p>All the parameters are an exact copy of those of sklearn.mixture.BayesianGaussianMixture.</p> <p>Attributes:</p> Name Type Description <code>gmms_</code> <code>dict[int, BayesianGaussianMixture]</code> <p>A dictionary of Bayesian Gaussian Mixture Models, one for each class.</p> <code>classes_</code> <code>np.ndarray of shape (n_classes,)</code> <p>The classes seen during <code>fit</code>.</p> Source code in <code>sklego/mixture/bayesian_gmm_classifier.py</code> <pre><code>class BayesianGMMClassifier(BaseEstimator, ClassifierMixin):\n    \"\"\"The `BayesianGMMClassifier` trains a Gaussian Mixture Model for each class in `y` on a dataset `X`.\n    Once a density is trained for each class we can evaluate the likelihood scores to see which class is more likely.\n\n    !!! note\n        All the parameters are an exact copy of those of\n        [sklearn.mixture.BayesianGaussianMixture]( https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html).\n\n\n    Attributes\n    ----------\n    gmms_ : dict[int, BayesianGaussianMixture]\n        A dictionary of Bayesian Gaussian Mixture Models, one for each class.\n    classes_ : np.ndarray of shape (n_classes,)\n        The classes seen during `fit`.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components=1,\n        covariance_type=\"full\",\n        tol=0.001,\n        reg_covar=1e-06,\n        max_iter=100,\n        n_init=1,\n        init_params=\"kmeans\",\n        weight_concentration_prior_type=\"dirichlet_process\",\n        weight_concentration_prior=None,\n        mean_precision_prior=None,\n        mean_prior=None,\n        degrees_of_freedom_prior=None,\n        covariance_prior=None,\n        random_state=None,\n        warm_start=False,\n        verbose=0,\n        verbose_interval=10,\n    ):\n        self.n_components = n_components\n        self.covariance_type = covariance_type\n        self.tol = tol\n        self.reg_covar = reg_covar\n        self.max_iter = max_iter\n        self.n_init = n_init\n        self.init_params = init_params\n        self.weight_concentration_prior_type = weight_concentration_prior_type\n        self.weight_concentration_prior = weight_concentration_prior\n        self.mean_precision_prior = mean_precision_prior\n        self.mean_prior = mean_prior\n        self.degrees_of_freedom_prior = degrees_of_freedom_prior\n        self.covariance_prior = covariance_prior\n        self.random_state = random_state\n        self.warm_start = warm_start\n        self.verbose = verbose\n        self.verbose_interval = verbose_interval\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"BayesianGMMClassifier\":\n        \"\"\"Fit the `BayesianGMMClassifier` model using `X`, `y` as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n\n        Returns\n        -------\n        self : BayesianGMMClassifier\n            The fitted estimator.\n        \"\"\"\n        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n        if X.ndim == 1:\n            X = np.expand_dims(X, 1)\n\n        self.gmms_ = {}\n        self.classes_ = unique_labels(y)\n        for c in self.classes_:\n            subset_x, subset_y = X[y == c], y[y == c]\n            mixture = BayesianGaussianMixture(\n                n_components=self.n_components,\n                covariance_type=self.covariance_type,\n                tol=self.tol,\n                reg_covar=self.reg_covar,\n                max_iter=self.max_iter,\n                n_init=self.n_init,\n                init_params=self.init_params,\n                weight_concentration_prior_type=self.weight_concentration_prior_type,\n                weight_concentration_prior=self.weight_concentration_prior,\n                mean_precision_prior=self.mean_precision_prior,\n                mean_prior=self.mean_prior,\n                degrees_of_freedom_prior=self.degrees_of_freedom_prior,\n                covariance_prior=self.covariance_prior,\n                random_state=self.random_state,\n                warm_start=self.warm_start,\n                verbose=self.verbose,\n                verbose_interval=self.verbose_interval,\n            )\n            self.gmms_[c] = mixture.fit(subset_x, subset_y)\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict labels for `X` using fitted estimator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        check_is_fitted(self, [\"gmms_\", \"classes_\"])\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        return self.classes_[self.predict_proba(X).argmax(axis=1)]\n\n    def predict_proba(self, X):\n        \"\"\"Predict probabilities for `X` using fitted estimator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_classes)\n            The predicted probabilities.\n        \"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        check_is_fitted(self, [\"gmms_\", \"classes_\"])\n        res = np.zeros((X.shape[0], self.classes_.shape[0]))\n        for idx, c in enumerate(self.classes_):\n            res[:, idx] = self.gmms_[c].score_samples(X)\n        return softmax(res, axis=1)\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.bayesian_gmm_classifier.BayesianGMMClassifier.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the <code>BayesianGMMClassifier</code> model using <code>X</code>, <code>y</code> as training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>BayesianGMMClassifier</code> <p>The fitted estimator.</p> Source code in <code>sklego/mixture/bayesian_gmm_classifier.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"BayesianGMMClassifier\":\n    \"\"\"Fit the `BayesianGMMClassifier` model using `X`, `y` as training data.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n\n    Returns\n    -------\n    self : BayesianGMMClassifier\n        The fitted estimator.\n    \"\"\"\n    X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n    if X.ndim == 1:\n        X = np.expand_dims(X, 1)\n\n    self.gmms_ = {}\n    self.classes_ = unique_labels(y)\n    for c in self.classes_:\n        subset_x, subset_y = X[y == c], y[y == c]\n        mixture = BayesianGaussianMixture(\n            n_components=self.n_components,\n            covariance_type=self.covariance_type,\n            tol=self.tol,\n            reg_covar=self.reg_covar,\n            max_iter=self.max_iter,\n            n_init=self.n_init,\n            init_params=self.init_params,\n            weight_concentration_prior_type=self.weight_concentration_prior_type,\n            weight_concentration_prior=self.weight_concentration_prior,\n            mean_precision_prior=self.mean_precision_prior,\n            mean_prior=self.mean_prior,\n            degrees_of_freedom_prior=self.degrees_of_freedom_prior,\n            covariance_prior=self.covariance_prior,\n            random_state=self.random_state,\n            warm_start=self.warm_start,\n            verbose=self.verbose,\n            verbose_interval=self.verbose_interval,\n        )\n        self.gmms_[c] = mixture.fit(subset_x, subset_y)\n    return self\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.bayesian_gmm_classifier.BayesianGMMClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict labels for <code>X</code> using fitted estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted data.</p> Source code in <code>sklego/mixture/bayesian_gmm_classifier.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for `X` using fitted estimator.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted data.\n    \"\"\"\n    check_is_fitted(self, [\"gmms_\", \"classes_\"])\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    return self.classes_[self.predict_proba(X).argmax(axis=1)]\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.bayesian_gmm_classifier.BayesianGMMClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict probabilities for <code>X</code> using fitted estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_classes)</code> <p>The predicted probabilities.</p> Source code in <code>sklego/mixture/bayesian_gmm_classifier.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict probabilities for `X` using fitted estimator.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_classes)\n        The predicted probabilities.\n    \"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    check_is_fitted(self, [\"gmms_\", \"classes_\"])\n    res = np.zeros((X.shape[0], self.classes_.shape[0]))\n    for idx, c in enumerate(self.classes_):\n        res[:, idx] = self.gmms_[c].score_samples(X)\n    return softmax(res, axis=1)\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.bayesian_gmm_detector.BayesianGMMOutlierDetector","title":"<code>sklego.mixture.bayesian_gmm_detector.BayesianGMMOutlierDetector</code>","text":"<p>             Bases: <code>OutlierMixin</code>, <code>BaseEstimator</code></p> <p>The <code>BayesianGMMOutlierDetector</code> trains a Bayesian Gaussian Mixture model on a dataset <code>X</code>. Once a density is trained we can evaluate the likelihood scores to see if it is deemed likely.</p> <p>By giving a threshold this model might then label outliers if their likelihood score is too low.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The limit at which the model thinks an outlier appears, must be between (0, 1).</p> <code>0.99</code> <code>method</code> <code>Literal[quantile, stddev]</code> <p>The method to use to apply the <code>threshold</code>.</p> <p>Info</p> <p>If you select <code>method=\"quantile\"</code> then the threshold value represents the quantile value to start calling something an outlier.</p> <p>If you select <code>method=\"stddev\"</code> then the threshold value represents the numbers of standard deviations before calling something an outlier.</p> <code>\"quantile\"</code> <p>Note</p> <p>The other parameters are an exact copy of the parameters in sklearn.mixture.BayesianGaussianMixture.</p> <p>Attributes:</p> Name Type Description <code>gmm_</code> <code>BayesianGaussianMixture</code> <p>The trained Bayesian Gaussian Mixture Model.</p> <code>likelihood_threshold_</code> <code>float</code> <p>The threshold value used to determine if something is an outlier.</p> Source code in <code>sklego/mixture/bayesian_gmm_detector.py</code> <pre><code>class BayesianGMMOutlierDetector(OutlierMixin, BaseEstimator):\n    \"\"\"The `BayesianGMMOutlierDetector` trains a Bayesian Gaussian Mixture model on a dataset `X`. Once a density is\n    trained we can evaluate the likelihood scores to see if it is deemed likely.\n\n    By giving a threshold this model might then label outliers if their likelihood score is too low.\n\n    Parameters\n    ----------\n    threshold : float, default=0.99\n        The limit at which the model thinks an outlier appears, must be between (0, 1).\n    method : Literal[\"quantile\", \"stddev\"], default=\"quantile\"\n        The method to use to apply the `threshold`.\n\n        !!! info\n            If you select `method=\"quantile\"` then the threshold value represents the quantile value to start calling\n            something an outlier.\n\n            If you select `method=\"stddev\"` then the threshold value represents the\n            numbers of standard deviations before calling something an outlier.\n\n    !!! note\n        The other parameters are an exact copy of the parameters in\n        [sklearn.mixture.BayesianGaussianMixture]( https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html).\n\n    Attributes\n    ----------\n    gmm_ : BayesianGaussianMixture\n        The trained Bayesian Gaussian Mixture Model.\n    likelihood_threshold_ : float\n        The threshold value used to determine if something is an outlier.\n    \"\"\"\n\n    _ALLOWED_METHODS = (\"quantile\", \"stddev\")\n\n    def __init__(\n        self,\n        threshold=0.99,\n        method=\"quantile\",\n        n_components=1,\n        covariance_type=\"full\",\n        tol=0.001,\n        reg_covar=1e-06,\n        max_iter=100,\n        n_init=1,\n        init_params=\"kmeans\",\n        weight_concentration_prior_type=\"dirichlet_process\",\n        weight_concentration_prior=None,\n        mean_precision_prior=None,\n        mean_prior=None,\n        degrees_of_freedom_prior=None,\n        covariance_prior=None,\n        random_state=None,\n        warm_start=False,\n        verbose=0,\n        verbose_interval=10,\n    ):\n        self.threshold = threshold\n        self.method = method\n\n        self.n_components = n_components\n        self.covariance_type = covariance_type\n        self.tol = tol\n        self.reg_covar = reg_covar\n        self.max_iter = max_iter\n        self.n_init = n_init\n        self.init_params = init_params\n        self.weight_concentration_prior_type = weight_concentration_prior_type\n        self.weight_concentration_prior = weight_concentration_prior\n        self.mean_precision_prior = mean_precision_prior\n        self.mean_prior = mean_prior\n        self.degrees_of_freedom_prior = degrees_of_freedom_prior\n        self.covariance_prior = covariance_prior\n        self.random_state = random_state\n        self.warm_start = warm_start\n        self.verbose = verbose\n        self.verbose_interval = verbose_interval\n\n    def fit(self, X: np.ndarray, y=None) -&gt; \"BayesianGMMOutlierDetector\":\n        \"\"\"Fit the `BayesianGMMOutlierDetector` model using `X`, `y` as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : BayesianGMMOutlierDetector\n            The fitted estimator.\n\n        Raises\n        ------\n        ValueError\n            - If `method=\"quantile\"` and `threshold` is not between (0, 1).\n            - If `method=\"stddev\"` and `threshold` is negative.\n            - If `method` is not in `[\"quantile\", \"stddev\"]`.\n        \"\"\"\n\n        # GMM sometimes throws an error if you don't do this\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X, 1)\n\n        if (self.method == \"quantile\") and ((self.threshold &gt; 1) or (self.threshold &lt; 0)):\n            raise ValueError(f\"Threshold {self.threshold} with method {self.method} needs to be 0 &lt; threshold &lt; 1\")\n        if (self.method == \"stddev\") and (self.threshold &lt; 0):\n            raise ValueError(f\"Threshold {self.threshold} with method {self.method} needs to be 0 &lt; threshold \")\n        if self.method not in self._ALLOWED_METHODS:\n            raise ValueError(f\"Method not recognised. Method must be in {self._ALLOWED_METHODS}\")\n\n        self.gmm_ = BayesianGaussianMixture(\n            n_components=self.n_components,\n            covariance_type=self.covariance_type,\n            tol=self.tol,\n            reg_covar=self.reg_covar,\n            max_iter=self.max_iter,\n            n_init=self.n_init,\n            init_params=self.init_params,\n            weight_concentration_prior_type=self.weight_concentration_prior_type,\n            weight_concentration_prior=self.weight_concentration_prior,\n            mean_precision_prior=self.mean_precision_prior,\n            mean_prior=self.mean_prior,\n            degrees_of_freedom_prior=self.degrees_of_freedom_prior,\n            covariance_prior=self.covariance_prior,\n            random_state=self.random_state,\n            warm_start=self.warm_start,\n            verbose=self.verbose,\n            verbose_interval=self.verbose_interval,\n        )\n        self.gmm_.fit(X)\n        score_samples = self.gmm_.score_samples(X)\n\n        if self.method == \"quantile\":\n            self.likelihood_threshold_ = np.quantile(score_samples, 1 - self.threshold)\n\n        if self.method == \"stddev\":\n            density = gaussian_kde(score_samples)\n            max_x_value = minimize_scalar(lambda x: -density(x)).x\n            mean_likelihood = score_samples.mean()\n            new_likelihoods = score_samples[score_samples &lt; max_x_value]\n            new_likelihoods_std = np.std(new_likelihoods - mean_likelihood)\n            self.likelihood_threshold_ = mean_likelihood - (self.threshold * new_likelihoods_std)\n\n        return self\n\n    def score_samples(self, X):\n        \"\"\"Compute the log likelihood for each sample and return the negative value.\"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        check_is_fitted(self, [\"gmm_\", \"likelihood_threshold_\"])\n        if len(X.shape) == 1:\n            X = np.expand_dims(X, 1)\n\n        return self.gmm_.score_samples(X) * -1\n\n    def decision_function(self, X):\n        # We subtract self.offset_ to make 0 be the threshold value for being an outlier:\n        return self.score_samples(X) + self.likelihood_threshold_\n\n    def predict(self, X):\n        \"\"\"Predict if a point is an outlier or not using the fitted estimator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted data. 1 for inliers, -1 for outliers.\n        \"\"\"\n        predictions = (self.decision_function(X) &gt;= 0).astype(int)\n        predictions[predictions == 1] = -1\n        predictions[predictions == 0] = 1\n        return predictions\n\n    @property\n    def allowed_methods(self):\n        warn(\n            \"Please use `_ALLOWED_METHODS` instead of `allowed_methods`,\"\n            \"`allowed_methods` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self._ALLOWED_METHODS\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.bayesian_gmm_detector.BayesianGMMOutlierDetector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the <code>BayesianGMMOutlierDetector</code> model using <code>X</code>, <code>y</code> as training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>BayesianGMMOutlierDetector</code> <p>The fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If <code>method=\"quantile\"</code> and <code>threshold</code> is not between (0, 1).</li> <li>If <code>method=\"stddev\"</code> and <code>threshold</code> is negative.</li> <li>If <code>method</code> is not in <code>[\"quantile\", \"stddev\"]</code>.</li> </ul> Source code in <code>sklego/mixture/bayesian_gmm_detector.py</code> <pre><code>def fit(self, X: np.ndarray, y=None) -&gt; \"BayesianGMMOutlierDetector\":\n    \"\"\"Fit the `BayesianGMMOutlierDetector` model using `X`, `y` as training data.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : BayesianGMMOutlierDetector\n        The fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        - If `method=\"quantile\"` and `threshold` is not between (0, 1).\n        - If `method=\"stddev\"` and `threshold` is negative.\n        - If `method` is not in `[\"quantile\", \"stddev\"]`.\n    \"\"\"\n\n    # GMM sometimes throws an error if you don't do this\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    if len(X.shape) == 1:\n        X = np.expand_dims(X, 1)\n\n    if (self.method == \"quantile\") and ((self.threshold &gt; 1) or (self.threshold &lt; 0)):\n        raise ValueError(f\"Threshold {self.threshold} with method {self.method} needs to be 0 &lt; threshold &lt; 1\")\n    if (self.method == \"stddev\") and (self.threshold &lt; 0):\n        raise ValueError(f\"Threshold {self.threshold} with method {self.method} needs to be 0 &lt; threshold \")\n    if self.method not in self._ALLOWED_METHODS:\n        raise ValueError(f\"Method not recognised. Method must be in {self._ALLOWED_METHODS}\")\n\n    self.gmm_ = BayesianGaussianMixture(\n        n_components=self.n_components,\n        covariance_type=self.covariance_type,\n        tol=self.tol,\n        reg_covar=self.reg_covar,\n        max_iter=self.max_iter,\n        n_init=self.n_init,\n        init_params=self.init_params,\n        weight_concentration_prior_type=self.weight_concentration_prior_type,\n        weight_concentration_prior=self.weight_concentration_prior,\n        mean_precision_prior=self.mean_precision_prior,\n        mean_prior=self.mean_prior,\n        degrees_of_freedom_prior=self.degrees_of_freedom_prior,\n        covariance_prior=self.covariance_prior,\n        random_state=self.random_state,\n        warm_start=self.warm_start,\n        verbose=self.verbose,\n        verbose_interval=self.verbose_interval,\n    )\n    self.gmm_.fit(X)\n    score_samples = self.gmm_.score_samples(X)\n\n    if self.method == \"quantile\":\n        self.likelihood_threshold_ = np.quantile(score_samples, 1 - self.threshold)\n\n    if self.method == \"stddev\":\n        density = gaussian_kde(score_samples)\n        max_x_value = minimize_scalar(lambda x: -density(x)).x\n        mean_likelihood = score_samples.mean()\n        new_likelihoods = score_samples[score_samples &lt; max_x_value]\n        new_likelihoods_std = np.std(new_likelihoods - mean_likelihood)\n        self.likelihood_threshold_ = mean_likelihood - (self.threshold * new_likelihoods_std)\n\n    return self\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.bayesian_gmm_detector.BayesianGMMOutlierDetector.predict","title":"<code>predict(X)</code>","text":"<p>Predict if a point is an outlier or not using the fitted estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted data. 1 for inliers, -1 for outliers.</p> Source code in <code>sklego/mixture/bayesian_gmm_detector.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict if a point is an outlier or not using the fitted estimator.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted data. 1 for inliers, -1 for outliers.\n    \"\"\"\n    predictions = (self.decision_function(X) &gt;= 0).astype(int)\n    predictions[predictions == 1] = -1\n    predictions[predictions == 0] = 1\n    return predictions\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.bayesian_gmm_detector.BayesianGMMOutlierDetector.score_samples","title":"<code>score_samples(X)</code>","text":"<p>Compute the log likelihood for each sample and return the negative value.</p> Source code in <code>sklego/mixture/bayesian_gmm_detector.py</code> <pre><code>def score_samples(self, X):\n    \"\"\"Compute the log likelihood for each sample and return the negative value.\"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    check_is_fitted(self, [\"gmm_\", \"likelihood_threshold_\"])\n    if len(X.shape) == 1:\n        X = np.expand_dims(X, 1)\n\n    return self.gmm_.score_samples(X) * -1\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.gmm_classifier.GMMClassifier","title":"<code>sklego.mixture.gmm_classifier.GMMClassifier</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>The <code>GMMClassifier</code> trains a Gaussian Mixture Model for each class in <code>y</code> on a dataset <code>X</code>. Once a density is trained for each class we can evaluate the likelihood scores to see which class is more likely.</p> <p>All parameters of the model are an exact copy of the parameters in scikit-learn.</p> <p>Note</p> <p>All the parameters are an exact copy of those of sklearn.mixture.GaussianMixture.</p> <p>Attributes:</p> Name Type Description <code>gmms_</code> <code>dict[int, GaussianMixture]</code> <p>A dictionary of Gaussian Mixture Models, one for each class.</p> <code>classes_</code> <code>np.ndarray of shape (n_classes,)</code> <p>The classes seen during <code>fit</code>.</p> Source code in <code>sklego/mixture/gmm_classifier.py</code> <pre><code>class GMMClassifier(BaseEstimator, ClassifierMixin):\n    \"\"\"The `GMMClassifier` trains a Gaussian Mixture Model for each class in `y` on a dataset `X`. Once a density is\n    trained for each class we can evaluate the likelihood scores to see which class is more likely.\n\n    All parameters of the model are an exact copy of the parameters in scikit-learn.\n\n    !!! note\n        All the parameters are an exact copy of those of\n        [sklearn.mixture.GaussianMixture]( https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html).\n\n    Attributes\n    ----------\n    gmms_ : dict[int, GaussianMixture]\n        A dictionary of Gaussian Mixture Models, one for each class.\n    classes_ : np.ndarray of shape (n_classes,)\n        The classes seen during `fit`.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components=1,\n        covariance_type=\"full\",\n        tol=1e-3,\n        reg_covar=1e-6,\n        max_iter=100,\n        n_init=1,\n        init_params=\"kmeans\",\n        weights_init=None,\n        means_init=None,\n        precisions_init=None,\n        random_state=None,\n        warm_start=False,\n        verbose=0,\n        verbose_interval=10,\n    ):\n        self.n_components = n_components\n        self.covariance_type = covariance_type\n        self.tol = tol\n        self.reg_covar = reg_covar\n        self.max_iter = max_iter\n        self.n_init = n_init\n        self.init_params = init_params\n        self.weights_init = weights_init\n        self.means_init = means_init\n        self.precisions_init = precisions_init\n        self.random_state = random_state\n        self.warm_start = warm_start\n        self.verbose = verbose\n        self.verbose_interval = verbose_interval\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"GMMClassifier\":\n        \"\"\"Fit the `GMMClassifier` model using `X`, `y` as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n\n        Returns\n        -------\n        self : GMMClassifier\n            The fitted estimator.\n        \"\"\"\n        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n        if X.ndim == 1:\n            X = np.expand_dims(X, 1)\n\n        self.gmms_ = {}\n        self.classes_ = unique_labels(y)\n        for c in self.classes_:\n            subset_x, subset_y = X[y == c], y[y == c]\n            mixture = GaussianMixture(\n                n_components=self.n_components,\n                covariance_type=self.covariance_type,\n                tol=self.tol,\n                reg_covar=self.reg_covar,\n                max_iter=self.max_iter,\n                n_init=self.n_init,\n                init_params=self.init_params,\n                weights_init=self.weights_init,\n                means_init=self.means_init,\n                precisions_init=self.precisions_init,\n                random_state=self.random_state,\n                warm_start=self.warm_start,\n                verbose=self.verbose,\n                verbose_interval=self.verbose_interval,\n            )\n            self.gmms_[c] = mixture.fit(subset_x, subset_y)\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict labels for `X` using fitted estimator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        check_is_fitted(self, [\"gmms_\", \"classes_\"])\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        return self.classes_[self.predict_proba(X).argmax(axis=1)]\n\n    def predict_proba(self, X):\n        \"\"\"Predict probabilities for `X` using fitted estimator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_classes)\n            The predicted probabilities.\n        \"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        check_is_fitted(self, [\"gmms_\", \"classes_\"])\n        res = np.zeros((X.shape[0], self.classes_.shape[0]))\n        for idx, c in enumerate(self.classes_):\n            res[:, idx] = self.gmms_[c].score_samples(X)\n        return softmax(res, axis=1)\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.gmm_classifier.GMMClassifier.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the <code>GMMClassifier</code> model using <code>X</code>, <code>y</code> as training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>GMMClassifier</code> <p>The fitted estimator.</p> Source code in <code>sklego/mixture/gmm_classifier.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"GMMClassifier\":\n    \"\"\"Fit the `GMMClassifier` model using `X`, `y` as training data.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n\n    Returns\n    -------\n    self : GMMClassifier\n        The fitted estimator.\n    \"\"\"\n    X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n    if X.ndim == 1:\n        X = np.expand_dims(X, 1)\n\n    self.gmms_ = {}\n    self.classes_ = unique_labels(y)\n    for c in self.classes_:\n        subset_x, subset_y = X[y == c], y[y == c]\n        mixture = GaussianMixture(\n            n_components=self.n_components,\n            covariance_type=self.covariance_type,\n            tol=self.tol,\n            reg_covar=self.reg_covar,\n            max_iter=self.max_iter,\n            n_init=self.n_init,\n            init_params=self.init_params,\n            weights_init=self.weights_init,\n            means_init=self.means_init,\n            precisions_init=self.precisions_init,\n            random_state=self.random_state,\n            warm_start=self.warm_start,\n            verbose=self.verbose,\n            verbose_interval=self.verbose_interval,\n        )\n        self.gmms_[c] = mixture.fit(subset_x, subset_y)\n    return self\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.gmm_classifier.GMMClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict labels for <code>X</code> using fitted estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted data.</p> Source code in <code>sklego/mixture/gmm_classifier.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for `X` using fitted estimator.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted data.\n    \"\"\"\n    check_is_fitted(self, [\"gmms_\", \"classes_\"])\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    return self.classes_[self.predict_proba(X).argmax(axis=1)]\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.gmm_classifier.GMMClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict probabilities for <code>X</code> using fitted estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_classes)</code> <p>The predicted probabilities.</p> Source code in <code>sklego/mixture/gmm_classifier.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict probabilities for `X` using fitted estimator.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_classes)\n        The predicted probabilities.\n    \"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    check_is_fitted(self, [\"gmms_\", \"classes_\"])\n    res = np.zeros((X.shape[0], self.classes_.shape[0]))\n    for idx, c in enumerate(self.classes_):\n        res[:, idx] = self.gmms_[c].score_samples(X)\n    return softmax(res, axis=1)\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.gmm_outlier_detector.GMMOutlierDetector","title":"<code>sklego.mixture.gmm_outlier_detector.GMMOutlierDetector</code>","text":"<p>             Bases: <code>OutlierMixin</code>, <code>BaseEstimator</code></p> <p>The <code>GMMDetector</code> trains a Gaussian Mixture model on a dataset <code>X</code>. Once a density is trained we can evaluate the likelihood scores to see if it is deemed likely.</p> <p>By giving a threshold this model might then label outliers if their likelihood score is too low.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The limit at which the model thinks an outlier appears, must be between (0, 1).</p> <code>0.99</code> <code>method</code> <code>Literal[quantile, stddev]</code> <p>The method to use to apply the <code>threshold</code>.</p> <p>Info</p> <p>If you select <code>method=\"quantile\"</code> then the threshold value represents the quantile value to start calling something an outlier.</p> <p>If you select <code>method=\"stddev\"</code> then the threshold value represents the numbers of standard deviations before calling something an outlier.</p> <code>\"quantile\"</code> <p>Note</p> <p>The other parameters are an exact copy of the parameters in sklearn.mixture.GaussianMixture.</p> <p>Attributes:</p> Name Type Description <code>gmm_</code> <code>GaussianMixture</code> <p>The trained Gaussian Mixture model.</p> <code>likelihood_threshold_</code> <code>float</code> <p>The threshold value used to determine if something is an outlier.</p> Source code in <code>sklego/mixture/gmm_outlier_detector.py</code> <pre><code>class GMMOutlierDetector(OutlierMixin, BaseEstimator):\n    \"\"\"The `GMMDetector` trains a Gaussian Mixture model on a dataset `X`. Once a density is trained we can evaluate the\n    likelihood scores to see if it is deemed likely.\n\n    By giving a threshold this model might then label outliers if their likelihood score is too low.\n\n    Parameters\n    ----------\n    threshold : float, default=0.99\n        The limit at which the model thinks an outlier appears, must be between (0, 1).\n    method : Literal[\"quantile\", \"stddev\"], default=\"quantile\"\n        The method to use to apply the `threshold`.\n\n        !!! info\n            If you select `method=\"quantile\"` then the threshold value represents the quantile value to start calling\n            something an outlier.\n\n            If you select `method=\"stddev\"` then the threshold value represents the\n            numbers of standard deviations before calling something an outlier.\n\n    !!! note\n        The other parameters are an exact copy of the parameters in\n        [sklearn.mixture.GaussianMixture]( https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html).\n\n    Attributes\n    ----------\n    gmm_ : GaussianMixture\n        The trained Gaussian Mixture model.\n    likelihood_threshold_ : float\n        The threshold value used to determine if something is an outlier.\n    \"\"\"\n\n    _ALLOWED_METHODS = (\"quantile\", \"stddev\")\n\n    def __init__(\n        self,\n        threshold=0.99,\n        method=\"quantile\",\n        n_components=1,\n        covariance_type=\"full\",\n        tol=1e-3,\n        reg_covar=1e-6,\n        max_iter=100,\n        n_init=1,\n        init_params=\"kmeans\",\n        weights_init=None,\n        means_init=None,\n        precisions_init=None,\n        random_state=None,\n        warm_start=False,\n        verbose=0,\n        verbose_interval=10,\n    ):\n        self.threshold = threshold\n        self.method = method\n        self.random_state = random_state\n        self.n_components = n_components\n        self.covariance_type = covariance_type\n        self.tol = tol\n        self.reg_covar = reg_covar\n        self.max_iter = max_iter\n        self.n_init = n_init\n        self.init_params = init_params\n        self.weights_init = weights_init\n        self.means_init = means_init\n        self.precisions_init = precisions_init\n        self.random_state = random_state\n        self.warm_start = warm_start\n        self.verbose = verbose\n        self.verbose_interval = verbose_interval\n\n    def fit(self, X: np.ndarray, y=None) -&gt; \"GMMOutlierDetector\":\n        \"\"\"Fit the `BayesianGMMOutlierDetector` model using `X`, `y` as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : GMMOutlierDetector\n            The fitted estimator.\n\n        Raises\n        ------\n        ValueError\n            - If `method=\"quantile\"` and `threshold` is not between (0, 1).\n            - If `method=\"stddev\"` and `threshold` is negative.\n            - If `method` is not in `[\"quantile\", \"stddev\"]`.\n        \"\"\"\n        # GMM sometimes throws an error if you don't do this\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X, 1)\n\n        if (self.method == \"quantile\") and ((self.threshold &gt; 1) or (self.threshold &lt; 0)):\n            raise ValueError(f\"Threshold {self.threshold} with method {self.method} needs to be 0 &lt; threshold &lt; 1\")\n        if (self.method == \"stddev\") and (self.threshold &lt; 0):\n            raise ValueError(f\"Threshold {self.threshold} with method {self.method} needs to be 0 &lt; threshold \")\n        if self.method not in self._ALLOWED_METHODS:\n            raise ValueError(f\"Method not recognised. Method must be in {self._ALLOWED_METHODS}\")\n\n        self.gmm_ = GaussianMixture(\n            n_components=self.n_components,\n            covariance_type=self.covariance_type,\n            tol=self.tol,\n            reg_covar=self.reg_covar,\n            max_iter=self.max_iter,\n            n_init=self.n_init,\n            init_params=self.init_params,\n            weights_init=self.weights_init,\n            means_init=self.means_init,\n            precisions_init=self.precisions_init,\n            random_state=self.random_state,\n            warm_start=self.warm_start,\n            verbose=self.verbose,\n            verbose_interval=self.verbose_interval,\n        )\n        self.gmm_.fit(X)\n        score_samples = self.gmm_.score_samples(X)\n\n        if self.method == \"quantile\":\n            self.likelihood_threshold_ = np.quantile(score_samples, 1 - self.threshold)\n\n        if self.method == \"stddev\":\n            density = gaussian_kde(score_samples)\n            max_x_value = minimize_scalar(lambda x: -density(x)).x\n            mean_likelihood = score_samples.mean()\n            new_likelihoods = score_samples[score_samples &lt; max_x_value]\n            new_likelihoods_std = np.std(new_likelihoods - mean_likelihood)\n            self.likelihood_threshold_ = mean_likelihood - (self.threshold * new_likelihoods_std)\n\n        return self\n\n    def score_samples(self, X):\n        \"\"\"Compute the log likelihood for each sample and return the negative value.\"\"\"\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        check_is_fitted(self, [\"gmm_\", \"likelihood_threshold_\"])\n        if len(X.shape) == 1:\n            X = np.expand_dims(X, 1)\n\n        return -self.gmm_.score_samples(X)\n\n    def decision_function(self, X):\n        # We subtract self.offset_ to make 0 be the threshold value for being an outlier:\n        return self.score_samples(X) + self.likelihood_threshold_\n\n    def predict(self, X):\n        \"\"\"Predict if a point is an outlier or not using the fitted model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted data. 1 for inliers, -1 for outliers.\n        \"\"\"\n        predictions = (self.decision_function(X) &gt;= 0).astype(int)\n        predictions[predictions == 1] = -1\n        predictions[predictions == 0] = 1\n        return predictions\n\n    @property\n    def allowed_methods(self):\n        warn(\n            \"Please use `_ALLOWED_METHODS` instead of `allowed_methods`,\"\n            \"`allowed_methods` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self._ALLOWED_METHODS\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.gmm_outlier_detector.GMMOutlierDetector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the <code>BayesianGMMOutlierDetector</code> model using <code>X</code>, <code>y</code> as training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>GMMOutlierDetector</code> <p>The fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If <code>method=\"quantile\"</code> and <code>threshold</code> is not between (0, 1).</li> <li>If <code>method=\"stddev\"</code> and <code>threshold</code> is negative.</li> <li>If <code>method</code> is not in <code>[\"quantile\", \"stddev\"]</code>.</li> </ul> Source code in <code>sklego/mixture/gmm_outlier_detector.py</code> <pre><code>def fit(self, X: np.ndarray, y=None) -&gt; \"GMMOutlierDetector\":\n    \"\"\"Fit the `BayesianGMMOutlierDetector` model using `X`, `y` as training data.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : GMMOutlierDetector\n        The fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        - If `method=\"quantile\"` and `threshold` is not between (0, 1).\n        - If `method=\"stddev\"` and `threshold` is negative.\n        - If `method` is not in `[\"quantile\", \"stddev\"]`.\n    \"\"\"\n    # GMM sometimes throws an error if you don't do this\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    if len(X.shape) == 1:\n        X = np.expand_dims(X, 1)\n\n    if (self.method == \"quantile\") and ((self.threshold &gt; 1) or (self.threshold &lt; 0)):\n        raise ValueError(f\"Threshold {self.threshold} with method {self.method} needs to be 0 &lt; threshold &lt; 1\")\n    if (self.method == \"stddev\") and (self.threshold &lt; 0):\n        raise ValueError(f\"Threshold {self.threshold} with method {self.method} needs to be 0 &lt; threshold \")\n    if self.method not in self._ALLOWED_METHODS:\n        raise ValueError(f\"Method not recognised. Method must be in {self._ALLOWED_METHODS}\")\n\n    self.gmm_ = GaussianMixture(\n        n_components=self.n_components,\n        covariance_type=self.covariance_type,\n        tol=self.tol,\n        reg_covar=self.reg_covar,\n        max_iter=self.max_iter,\n        n_init=self.n_init,\n        init_params=self.init_params,\n        weights_init=self.weights_init,\n        means_init=self.means_init,\n        precisions_init=self.precisions_init,\n        random_state=self.random_state,\n        warm_start=self.warm_start,\n        verbose=self.verbose,\n        verbose_interval=self.verbose_interval,\n    )\n    self.gmm_.fit(X)\n    score_samples = self.gmm_.score_samples(X)\n\n    if self.method == \"quantile\":\n        self.likelihood_threshold_ = np.quantile(score_samples, 1 - self.threshold)\n\n    if self.method == \"stddev\":\n        density = gaussian_kde(score_samples)\n        max_x_value = minimize_scalar(lambda x: -density(x)).x\n        mean_likelihood = score_samples.mean()\n        new_likelihoods = score_samples[score_samples &lt; max_x_value]\n        new_likelihoods_std = np.std(new_likelihoods - mean_likelihood)\n        self.likelihood_threshold_ = mean_likelihood - (self.threshold * new_likelihoods_std)\n\n    return self\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.gmm_outlier_detector.GMMOutlierDetector.predict","title":"<code>predict(X)</code>","text":"<p>Predict if a point is an outlier or not using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted data. 1 for inliers, -1 for outliers.</p> Source code in <code>sklego/mixture/gmm_outlier_detector.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict if a point is an outlier or not using the fitted model.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted data. 1 for inliers, -1 for outliers.\n    \"\"\"\n    predictions = (self.decision_function(X) &gt;= 0).astype(int)\n    predictions[predictions == 1] = -1\n    predictions[predictions == 0] = 1\n    return predictions\n</code></pre>"},{"location":"api/mixture/#sklego.mixture.gmm_outlier_detector.GMMOutlierDetector.score_samples","title":"<code>score_samples(X)</code>","text":"<p>Compute the log likelihood for each sample and return the negative value.</p> Source code in <code>sklego/mixture/gmm_outlier_detector.py</code> <pre><code>def score_samples(self, X):\n    \"\"\"Compute the log likelihood for each sample and return the negative value.\"\"\"\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    check_is_fitted(self, [\"gmm_\", \"likelihood_threshold_\"])\n    if len(X.shape) == 1:\n        X = np.expand_dims(X, 1)\n\n    return -self.gmm_.score_samples(X)\n</code></pre>"},{"location":"api/model-selection/","title":"Model Selection","text":""},{"location":"api/model-selection/#sklego.model_selection.GroupTimeSeriesSplit","title":"<code>sklego.model_selection.GroupTimeSeriesSplit</code>","text":"<p>             Bases: <code>_BaseKFold</code></p> <p>Sliding window time series split.</p> <p>Create <code>n_splits</code> folds with an as equally possible size through a smart variant of a brute force search. Groups parameter in <code>.split()</code> method should be filled with the time groups (e.g. years)</p> <p>If <code>n_splits</code> is 3 (\"*\" = train, \"x\" = test):</p> <pre><code>* * * x x x - - - - -\n- - - * * * x x x - -\n- - - - - - * * * x x\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_splits</code> <code>int</code> <p>Amount of (train, test) splits to generate.</p> required Source code in <code>sklego/model_selection.py</code> <pre><code>class GroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Sliding window time series split.\n\n    Create `n_splits` folds with an as equally possible size through a smart variant of a brute force search.\n    Groups parameter in `.split()` method should be filled with the time groups (e.g. years)\n\n    If `n_splits` is 3 (\"*\" = train, \"x\" = test):\n\n    ```console\n    * * * x x x - - - - -\n    - - - * * * x x x - -\n    - - - - - - * * * x x\n    ```\n\n    Parameters\n    ----------\n    n_splits : int\n        Amount of (train, test) splits to generate.\n    \"\"\"\n\n    # table above inspired by sktime\n\n    def __init__(self, n_splits):\n        if not isinstance(n_splits, numbers.Integral):\n            raise ValueError(\n                \"The number of folds must be of Integral type. \"\n                \"%s of type %s was passed.\" % (n_splits, type(n_splits))\n            )\n        n_splits = int(n_splits)\n\n        if n_splits &lt;= 1:\n            raise ValueError(\n                \"k-fold cross-validation requires at least one\"\n                \" train/test split by setting n_splits=2 or more,\"\n                \" got n_splits={0}.\".format(n_splits)\n            )\n\n        self.n_splits = n_splits\n\n    def summary(self):\n        \"\"\"Describe all folds in a pd.DataFrame which displays the groups splits and extra statistics about it.\n\n        Can only be run after having applied the `.split()` method to the `GroupTimeSeriesSplit` instance.\n\n        Returns\n        -------\n        pd.DataFrame\n            Summary of all folds.\n        \"\"\"\n        try:\n            return (\n                self._grouped_df.sort_index()\n                .assign(group=lambda df: df[\"group\"].astype(int))\n                .assign(obs_per_group=lambda df: df.groupby(\"group\")[\"observations\"].transform(\"sum\"))\n                .assign(ideal_group_size=round(self._ideal_group_size))\n                .assign(diff_from_ideal_group_size=lambda df: df[\"obs_per_group\"] - df[\"ideal_group_size\"])\n            )\n        except AttributeError:\n            raise AttributeError(\".summary() only works after having ran .split(X, y, groups).\")\n\n    def split(self, X=None, y=None, groups=None):\n        \"\"\"Generate the train-test splits of all the folds\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features), default=None\n            Data to split.\n        y : array-like of shape (n_samples,), default=None\n            The target variable for supervised learning problems.\n        groups : array-like of shape (n_samples,), default=None\n            Group labels for the samples used while splitting the dataset into train/test set,\n\n        Returns\n        -------\n        List[np.ndarray]\n            List containing train-test split indices of each fold.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\"Groups cannot be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_groups = np.unique(groups).shape[0]\n        if self.n_splits &gt;= n_groups:\n            raise ValueError(\n                (\"n_splits({0}) must be less than the amount\" \" of unique groups({1}).\").format(self.n_splits, n_groups)\n            )\n        return list(self._iter_test_indices(X, y, groups))\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Get the amount of splits\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features), default=None\n            Ignored, present for compatibility.\n        y : array-like of shape (n_samples,), default=None\n            Ignored, present for compatibility.\n        groups : array-like of shape (n_samples,), default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        int\n            Amount of splits.\n        \"\"\"\n        return self.n_splits\n\n    def _check_for_long_estimated_runtime(self, groups):\n        \"\"\"Check for combinations of n_splits and unique groups and raises `UserWarning` if runtime is expected to take\n        over one minute.\n\n        Parameters\n        ----------\n        groups : array-like of shape (n_samples,)\n            Groups to check for unique groups and n_splits.\n\n        Raises\n        ------\n        UserWarning\n            If runtime is expected to take over one minute.\n        \"\"\"\n        unique_groups = len(set(groups))\n        warning = (\n            \"Finding the optimal split points\"\n            \" with {0} unique groups and n_splits at {1}\"\n            \" can take several minutes.\"\n        ).format(unique_groups, self.n_splits)\n        if self.n_splits == 4 and unique_groups &gt; 250:\n            warn(\n                warning + \" Consider to decrease n_splits to 3 or lower.\",\n                UserWarning,\n            )\n\n        elif self.n_splits == 5 and unique_groups &gt; 100:\n            warn(\n                warning + \" Consider to decrease n_splits to 4 or lower.\",\n                UserWarning,\n            )\n\n        elif self.n_splits &gt; 5 and unique_groups &gt; 30:\n            warn(\n                warning + \" Consider to decrease n_splits to 5 or lower.\",\n                UserWarning,\n            )\n\n    def _iter_test_indices(self, X=None, y=None, groups=None):\n        \"\"\"Calculate the optimal division of groups into folds so that every fold is as equally large as possible.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features), default=None\n            Ignored, present for compatibility.\n        y : array-like of shape (n_samples,), default=None\n            Ignored, present for compatibility.\n        groups : array-like of shape (n_samples,), default=None\n            Array with groups.\n\n        Yields\n        ------\n        tuple[np.ndarray, np.ndarray]\n            Train and test indices of the same fold.\n        \"\"\"\n        self._check_for_long_estimated_runtime(groups)\n        self._first_split_index, self._last_split_index = self._calc_first_and_last_split_index(groups=groups)\n        self._best_splits = self._get_split_indices()\n        groups = self._regroup(groups)\n        for i in range(self.n_splits):\n            yield np.where(groups == i)[0], np.where(groups == i + 1)[0]\n\n    def _calc_first_and_last_split_index(self, X=None, y=None, groups=None):\n        \"\"\"Calculate an approximate first and last split point to reduce the amount of options during a brute force\n        search.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features), default=None\n            Ignored, present for compatibility.\n        y : array-like of shape (n_samples,), default=None\n            Ignored, present for compatibility.\n        groups : array-like of shape (n_samples,), default=None\n            Array with groups.\n\n        Returns\n        -------\n        tuple[int, int]\n            Approximate first and last split indexes.\n        \"\"\"\n\n        # get the counts (=amount of rows) for each group\n        self._grouped_df = (\n            pd.DataFrame(np.array(groups))\n            .rename(columns={0: \"index\"})\n            .groupby(\"index\")\n            .size()\n            .sort_index()\n            .to_frame()\n            .rename(columns={0: \"observations\"})\n        )\n\n        # set the ideal group_size and reduce it to 90% to have some leverage\n        self._ideal_group_size = np.sum(self._grouped_df[\"observations\"]) / (self.n_splits + 1)\n        init_ideal_group_size = self._ideal_group_size * 0.9\n\n        # initialize the index of the first split, to reduce the amount of possible index split options\n        first_split_index = (\n            self._grouped_df.assign(cumsum_obs=lambda df: df[\"observations\"].cumsum())\n            .assign(group_id=lambda df: (df[\"cumsum_obs\"] - 1) // init_ideal_group_size)\n            .reset_index()\n            .loc[lambda df: df[\"group_id\"] != 0]\n            .iloc[0]\n            .name\n        )\n        # initialize the index of the last split point, to reduce the amount of possible index split options\n        last_split_index = len(self._grouped_df) - (\n            self._grouped_df.assign(\n                observations=lambda df: df[\"observations\"].values[::-1],\n                cumsum_obs=lambda df: df[\"observations\"].cumsum(),\n            )\n            .reset_index()\n            .assign(group_id=lambda df: (df[\"cumsum_obs\"] - 1) // init_ideal_group_size)\n            .loc[lambda df: df[\"group_id\"] != 0]\n            .iloc[0]\n            .name\n            - 1\n        )\n        return first_split_index, last_split_index\n\n    def _get_split_indices(self):\n        \"\"\"Calculate for each possible splits the total absolute different of the groups to the ideal group size and\n        saves the split with the least absolute difference.\n\n        Returns\n        -------\n        int\n            Index of the best split point.\n        \"\"\"\n        # set the index range to search possible splits for\n        index_range = range(self._first_split_index, self._last_split_index)\n\n        observations = self._grouped_df[\"observations\"].tolist()\n\n        # create generator with all the possible index splits\n        # e.g. for [0, 1, 3, 5, 8] and self.n_splits = 2\n        # [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)]\n        # with for the first split:\n        # group1 = [:1]\n        # group2 = [1:2]\n        # group3 = [2:]\n        splits_generator_shifted = combinations(index_range, self.n_splits)\n\n        # get the first iteration\n        first_splits = next(splits_generator_shifted)\n\n        # create a new generator that starts from the beginning again\n        splits_generator = combinations(index_range, self.n_splits)\n\n        # generate a list, with for every group the difference between them and the ideal group size, e.g.\n        # ideal_group_size = 100\n        # group_sizes = [10,20,270]\n        # diff_from_ideal_list = [-90, -80, 170]\n        diff_from_ideal_list = [sum(observations[: first_splits[0]]) - self._ideal_group_size]\n        for split in sliding_window(first_splits, window_size=2, step_size=1):\n            try:\n                diff_from_ideal_list += [sum(observations[split[0] : split[1]]) - self._ideal_group_size]\n            except IndexError:\n                diff_from_ideal_list += [sum(observations[split[0] :]) - self._ideal_group_size]\n\n        # keep track of the minimum of the total difference from all groups to the ideal group size\n        min_diff = sum([abs(diff) for diff in diff_from_ideal_list])\n        best_splits = first_splits\n\n        # loop through all possible split points and check whether a new split\n        # has a less total difference from all groups to the ideal group size\n        for prev_splits, new_splits in zip(splits_generator, splits_generator_shifted):\n            diff_from_ideal_list = self._calc_new_diffs(observations, diff_from_ideal_list, prev_splits, new_splits)\n            new_diff = sum([abs(diff) for diff in diff_from_ideal_list])\n\n            # if with the new split the difference is less than the current most optimal, save the new split\n            if new_diff &lt; min_diff:\n                min_diff = new_diff\n                best_splits = new_splits\n        return best_splits\n\n    @staticmethod\n    def _calc_new_diffs(values, diff_list, prev_splits, new_splits):\n        \"\"\"Calculate the new group size differences compared to the optimal group size.\n\n        Parameters\n        ----------\n        values : list\n            List of values.\n        diff_list : list\n            List of values with for each index split its difference from the optimal group size.\n        prev_splits : tuple\n            Indices of the previous splits, excluding index 0 and the last index.\n        new_splits : tuple\n            Indices of the new splits, excluding index 0 and the last index.\n\n        Returns\n        -------\n        list\n            List of values with for each index split its difference from the optimal group size.\n        \"\"\"\n        # calculate which indices have changed, e.g.:\n        # new_index = (1,2,5)\n        # prev_index = (1,2,4)\n        # index_diffs = (0,0,1)\n        index_diffs = [new_index - prev_index for prev_index, new_index in zip(prev_splits, new_splits)]\n        new_diff_list = diff_list.copy()\n\n        # calculate the effects of the index change to the groups\n        for index, diff in enumerate(index_diffs):\n            if diff != 0:\n                start_index, end_index = (\n                    (prev_splits[index], new_splits[index])\n                    if prev_splits[index] &lt; new_splits[index]\n                    else (new_splits[index], prev_splits[index])\n                )\n\n                # calculate the value change from one group to another\n                value_change = sum(values[start_index:end_index])\n\n                # if diff &lt; 0 the previous group gains values, so change value_change to -value_change\n                value_change = value_change if diff &gt; 0 else -value_change\n\n                # change the values of the current and next group\n                new_diff_list[index] += value_change\n                new_diff_list[index + 1] -= value_change\n\n        return new_diff_list\n\n    def _regroup(self, groups):\n        \"\"\"Specify in which group every observation belongs.\n\n        Parameters\n        ----------\n        groups : array-like of shape (n_samples,)\n            Array with original groups.\n\n        Returns\n        -------\n        np.ndarray\n            Indices for the train and test splits of each fold\n        \"\"\"\n\n        df = self._grouped_df.copy().reset_index()\n        # set each unique group to the right group_id to group them into folds\n        df.loc[: self._best_splits[0], \"group\"] = 0\n        for group_id, splits in enumerate(sliding_window(self._best_splits, 2, 1)):\n            try:\n                df.loc[splits[0] : splits[1], \"group\"] = group_id + 1\n            except IndexError:\n                df.loc[splits[0] :, \"group\"] = group_id + 1\n\n        self._grouped_df = df\n        # create a mapper to set every group to the right group_id\n        mapper = dict(zip(df[\"index\"], df[\"group\"]))\n        return np.vectorize(mapper.get)(groups)\n</code></pre>"},{"location":"api/model-selection/#sklego.model_selection.GroupTimeSeriesSplit.get_n_splits","title":"<code>get_n_splits(X=None, y=None, groups=None)</code>","text":"<p>Get the amount of splits</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <code>groups</code> <code>array-like of shape (n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Amount of splits.</p> Source code in <code>sklego/model_selection.py</code> <pre><code>def get_n_splits(self, X=None, y=None, groups=None):\n    \"\"\"Get the amount of splits\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features), default=None\n        Ignored, present for compatibility.\n    y : array-like of shape (n_samples,), default=None\n        Ignored, present for compatibility.\n    groups : array-like of shape (n_samples,), default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    int\n        Amount of splits.\n    \"\"\"\n    return self.n_splits\n</code></pre>"},{"location":"api/model-selection/#sklego.model_selection.GroupTimeSeriesSplit.split","title":"<code>split(X=None, y=None, groups=None)</code>","text":"<p>Generate the train-test splits of all the folds</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Data to split.</p> <code>None</code> <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target variable for supervised learning problems.</p> <code>None</code> <code>groups</code> <code>array-like of shape (n_samples,)</code> <p>Group labels for the samples used while splitting the dataset into train/test set,</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List containing train-test split indices of each fold.</p> Source code in <code>sklego/model_selection.py</code> <pre><code>def split(self, X=None, y=None, groups=None):\n    \"\"\"Generate the train-test splits of all the folds\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features), default=None\n        Data to split.\n    y : array-like of shape (n_samples,), default=None\n        The target variable for supervised learning problems.\n    groups : array-like of shape (n_samples,), default=None\n        Group labels for the samples used while splitting the dataset into train/test set,\n\n    Returns\n    -------\n    List[np.ndarray]\n        List containing train-test split indices of each fold.\n    \"\"\"\n    if groups is None:\n        raise ValueError(\"Groups cannot be None\")\n    X, y, groups = indexable(X, y, groups)\n    n_groups = np.unique(groups).shape[0]\n    if self.n_splits &gt;= n_groups:\n        raise ValueError(\n            (\"n_splits({0}) must be less than the amount\" \" of unique groups({1}).\").format(self.n_splits, n_groups)\n        )\n    return list(self._iter_test_indices(X, y, groups))\n</code></pre>"},{"location":"api/model-selection/#sklego.model_selection.GroupTimeSeriesSplit.summary","title":"<code>summary()</code>","text":"<p>Describe all folds in a pd.DataFrame which displays the groups splits and extra statistics about it.</p> <p>Can only be run after having applied the <code>.split()</code> method to the <code>GroupTimeSeriesSplit</code> instance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Summary of all folds.</p> Source code in <code>sklego/model_selection.py</code> <pre><code>def summary(self):\n    \"\"\"Describe all folds in a pd.DataFrame which displays the groups splits and extra statistics about it.\n\n    Can only be run after having applied the `.split()` method to the `GroupTimeSeriesSplit` instance.\n\n    Returns\n    -------\n    pd.DataFrame\n        Summary of all folds.\n    \"\"\"\n    try:\n        return (\n            self._grouped_df.sort_index()\n            .assign(group=lambda df: df[\"group\"].astype(int))\n            .assign(obs_per_group=lambda df: df.groupby(\"group\")[\"observations\"].transform(\"sum\"))\n            .assign(ideal_group_size=round(self._ideal_group_size))\n            .assign(diff_from_ideal_group_size=lambda df: df[\"obs_per_group\"] - df[\"ideal_group_size\"])\n        )\n    except AttributeError:\n        raise AttributeError(\".summary() only works after having ran .split(X, y, groups).\")\n</code></pre>"},{"location":"api/model-selection/#sklego.model_selection.KlusterFoldValidation","title":"<code>sklego.model_selection.KlusterFoldValidation</code>","text":"<p>KlusterFold cross validator. Create folds based on provided cluster method</p> <p>Parameters:</p> Name Type Description Default <code>cluster_method</code> <code>Clusterer</code> <p>Clustering method to use for the fold validation.</p> <code>None</code> Source code in <code>sklego/model_selection.py</code> <pre><code>class KlusterFoldValidation:\n    \"\"\"KlusterFold cross validator. Create folds based on provided cluster method\n\n    Parameters\n    ----------\n    cluster_method : Clusterer\n        Clustering method to use for the fold validation.\n    \"\"\"\n\n    def __init__(self, cluster_method=None):\n        if not isinstance(cluster_method, Clusterer):\n            raise ValueError(\"The KlusterFoldValidation only works on cluster methods with .fit_predict.\")\n\n        self.cluster_method = cluster_method\n        self.n_splits = None\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Array to split on.\n        y : array-like of shape (n_samples,) | None, default=None\n            Ignored, present for compatibility.\n        groups : array-like of shape (n_samples,) | None, default=None\n            Ignored, present for compatibility.\n\n        Yields\n        -------\n        tuple[np.ndarray, np.ndarray]\n            Train and test indices of the same fold.\n        \"\"\"\n\n        X = check_array(X)\n\n        if not self._method_is_fitted(X):\n            self.cluster_method.fit(X)\n        clusters = self.cluster_method.predict(X)\n\n        self.n_splits = len(np.unique(clusters))\n\n        if self.n_splits &lt; 2:\n            raise ValueError(f\"Clustering method resulted in {self.n_splits} cluster, too few for fold validation\")\n\n        for label in np.unique(clusters):\n            yield (\n                np.where(clusters != label)[0],\n                np.where(clusters == label)[0],\n            )\n\n    def _method_is_fitted(self, X):\n        \"\"\"Check if the `cluster_method` is fitted\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Array to use if the method is fitted\n\n        Returns\n        -------\n        bool\n            True if fitted, else False\n        \"\"\"\n\n        try:\n            self.cluster_method.predict(X[0:1, :])\n            return True\n        except NotFittedError:\n            return False\n</code></pre>"},{"location":"api/model-selection/#sklego.model_selection.KlusterFoldValidation.split","title":"<code>split(X, y=None, groups=None)</code>","text":"<p>Generate indices to split data into training and test set.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Array to split on.</p> required <code>y</code> <code>array-like of shape (n_samples,) | None</code> <p>Ignored, present for compatibility.</p> <code>None</code> <code>groups</code> <code>array-like of shape (n_samples,) | None</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Yields:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>Train and test indices of the same fold.</p> Source code in <code>sklego/model_selection.py</code> <pre><code>def split(self, X, y=None, groups=None):\n    \"\"\"Generate indices to split data into training and test set.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Array to split on.\n    y : array-like of shape (n_samples,) | None, default=None\n        Ignored, present for compatibility.\n    groups : array-like of shape (n_samples,) | None, default=None\n        Ignored, present for compatibility.\n\n    Yields\n    -------\n    tuple[np.ndarray, np.ndarray]\n        Train and test indices of the same fold.\n    \"\"\"\n\n    X = check_array(X)\n\n    if not self._method_is_fitted(X):\n        self.cluster_method.fit(X)\n    clusters = self.cluster_method.predict(X)\n\n    self.n_splits = len(np.unique(clusters))\n\n    if self.n_splits &lt; 2:\n        raise ValueError(f\"Clustering method resulted in {self.n_splits} cluster, too few for fold validation\")\n\n    for label in np.unique(clusters):\n        yield (\n            np.where(clusters != label)[0],\n            np.where(clusters == label)[0],\n        )\n</code></pre>"},{"location":"api/model-selection/#sklego.model_selection.TimeGapSplit","title":"<code>sklego.model_selection.TimeGapSplit</code>","text":"<p>Provides train/test indices to split time series data samples.</p> <p>This cross-validation object is a variation of TimeSeriesSplit with the following  differences:</p> <ul> <li>The splits are made based on datetime duration, instead of number of rows.</li> <li>The user specifies the <code>valid_duration</code> and either <code>train_duration</code> or <code>n_splits</code>.</li> <li>The user can specify a <code>gap_duration</code> that is added after the training split and before the validation split.</li> </ul> <p>The 3 duration parameters can be used to really replicate how the model is going to be used in production in batch learning.</p> <p>Each validation fold doesn't overlap. The entire <code>window</code> moves by 1 <code>valid_duration</code> until there is not enough data.</p> <p>If this would lead to more splits then specified with <code>n_splits</code>, the <code>window</code> moves by <code>valid_duration</code> times the fraction of possible splits and requested splits:</p> <ul> <li><code>n_possible_splits = (total_length - train_duration-gap_duration) // valid_duration</code></li> <li><code>time_shift = valid_duration * n_possible_splits / n_slits</code></li> </ul> <p>so the CV spans the whole dataset.</p> <p>If <code>train_duration</code> is not passed but <code>n_splits</code> is, the training duration is increased to:</p> <p><code>train_duration = total_length - (gap_duration + valid_duration * n_splits)</code></p> <p>such that the shifting the entire window by one validation duration spans the whole training set.</p> <p>Parameters:</p> Name Type Description Default <code>date_serie</code> <code>Series</code> <p>Series with the date, that should have all the indices of X used in the split() method.</p> required <code>valid_duration</code> <code>timedelta</code> <p>Retraining period.</p> required <code>train_duration</code> <code>timedelta | None</code> <p>Historical training data.</p> <code>None</code> <code>gap_duration</code> <code>timedelta</code> <p>Forward looking window of the target. The period of the forward looking window necessary to create your target variable.</p> <p>This period is dropped at the end of your training folds due to lack of recent data.</p> <p>In production you would have not been able to create the target for that period, and you would have drop it from the training data.</p> <code>timedelta(0)</code> <code>n_splits</code> <code>int | None</code> <p>Number of splits.</p> <code>None</code> <code>window</code> <code>Literal[rolling, expanding]</code> <p>Type of moving window to use.</p> <ul> <li><code>\"rolling\"</code> window has fixed size and is shifted entirely.</li> <li><code>\"expanding\"</code> left side of window is fixed, right border increases each fold.</li> </ul> <code>\"rolling\"</code> Source code in <code>sklego/model_selection.py</code> <pre><code>class TimeGapSplit:\n    r\"\"\"Provides train/test indices to split time series data samples.\n\n    This cross-validation object is a variation of TimeSeriesSplit with the following  differences:\n\n    - The splits are made based on datetime duration, instead of number of rows.\n    - The user specifies the `valid_duration` and either `train_duration` or `n_splits`.\n    - The user can specify a `gap_duration` that is added after the training split and before the validation split.\n\n    The 3 duration parameters can be used to really replicate how the model is going to be used in production in batch\n    learning.\n\n    Each validation fold doesn't overlap. The entire `window` moves by 1 `valid_duration` until there is not enough\n    data.\n\n    If this would lead to more splits then specified with `n_splits`, the `window` moves by `valid_duration` times the\n    fraction of possible splits and requested splits:\n\n    - `n_possible_splits = (total_length - train_duration-gap_duration) // valid_duration`\n    - `time_shift = valid_duration * n_possible_splits / n_slits`\n\n    so the CV spans the whole dataset.\n\n    If `train_duration` is not passed but `n_splits` is, the training duration is increased to:\n\n    `train_duration = total_length - (gap_duration + valid_duration * n_splits)`\n\n    such that the shifting the entire window by one validation duration spans the whole training set.\n\n    Parameters\n    ----------\n    date_serie : pd.Series\n        Series with the date, that should have all the indices of X used in the split() method.\n    valid_duration : datetime.timedelta\n        Retraining period.\n    train_duration : datetime.timedelta | None, default=None\n        Historical training data.\n    gap_duration : datetime.timedelta, default=timedelta(0)\n        Forward looking window of the target. The period of the forward looking window necessary to create your target\n        variable.\n\n        This period is dropped at the end of your training folds due to lack of recent data.\n\n        In production you would have not been able to create the target for that period, and you would have drop it from\n        the training data.\n    n_splits : int | None, default=None\n        Number of splits.\n    window : Literal[\"rolling\", \"expanding\"], default=\"rolling\"\n        Type of moving window to use.\n\n        - `\"rolling\"` window has fixed size and is shifted entirely.\n        - `\"expanding\"` left side of window is fixed, right border increases each fold.\n    \"\"\"\n\n    def __init__(\n        self,\n        date_serie,\n        valid_duration,\n        train_duration=None,\n        gap_duration=timedelta(0),\n        n_splits=None,\n        window=\"rolling\",\n    ):\n        if (train_duration is None) and (n_splits is None):\n            raise ValueError(\"Either train_duration or n_splits have to be defined\")\n\n        if (train_duration is not None) and (train_duration &lt;= gap_duration):\n            raise ValueError(\"gap_duration is longer than train_duration, it should be shorter.\")\n\n        if not date_serie.index.is_unique:\n            raise ValueError(\"date_serie doesn't have a unique index\")\n\n        self.date_serie = date_serie.copy()\n        self.date_serie = self.date_serie.rename(\"__date__\")\n        self.train_duration = train_duration\n        self.valid_duration = valid_duration\n        self.gap_duration = gap_duration\n        self.n_splits = n_splits\n        self.window = window\n\n    def _join_date_and_x(self, X):\n        \"\"\"Creates a DataFrame indexed by the pandas index (the same as `date_serie`) with date column joined with that\n        index and with the 'numpy index' column (i.e. just a range) that is required for the output and the rest of\n        sklearn.\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            Dataframe with the data to split\n        \"\"\"\n        X_index_df = pd.DataFrame(range(len(X)), columns=[\"np_index\"], index=X.index)\n        X_index_df = X_index_df.join(self.date_serie)\n\n        return X_index_df\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            Dataframe with the data to split.\n        y : array-like | None, default=None\n            Ignored, present for compatibility.\n        groups : array-like | None, default=None\n            Ignored, present for compatibility.\n\n        Yields\n        -------\n        tuple[np.ndarray, np.ndarray]\n            Train and test indices of the same fold.\n        \"\"\"\n\n        X_index_df = self._join_date_and_x(X)\n        X_index_df = X_index_df.sort_values(\"__date__\", ascending=True)\n\n        if len(X) != len(X_index_df):\n            raise AssertionError(\n                \"X and X_index_df are not the same length, \" \"there must be some index missing in 'self.date_serie'\"\n            )\n\n        date_min = X_index_df[\"__date__\"].min()\n        date_max = X_index_df[\"__date__\"].max()\n        date_length = X_index_df[\"__date__\"].max() - X_index_df[\"__date__\"].min()\n\n        if (self.train_duration is None) and (self.n_splits is not None):\n            self.train_duration = date_length - (self.gap_duration + self.valid_duration * self.n_splits)\n\n        if (self.train_duration is not None) and (self.train_duration &lt;= self.gap_duration):\n            raise ValueError(\"gap_duration is longer than train_duration, it should be shorter.\")\n\n        n_split_max = (date_length - self.train_duration - self.gap_duration) / self.valid_duration\n        if self.n_splits:\n            if n_split_max &lt; self.n_splits:\n                raise ValueError(\n                    (\n                        \"Number of folds requested = {1} are greater\"\n                        \" than maximum  ={0} possible without\"\n                        \" overlapping validation sets.\"\n                    ).format(n_split_max, self.n_splits)\n                )\n\n        current_date = date_min\n        start_date = date_min\n        # if the n_splits is smaller than what would usually be done for train val and gap duration,\n        # the next fold is slightly further in time than just valid_duration\n        if self.n_splits is not None:\n            time_shift = self.valid_duration * n_split_max / self.n_splits\n        else:\n            time_shift = self.valid_duration\n        while True:\n            if current_date + self.train_duration + time_shift + self.gap_duration &gt; date_max:\n                break\n\n            X_train_df = X_index_df[\n                (X_index_df[\"__date__\"] &gt;= start_date) &amp; (X_index_df[\"__date__\"] &lt; current_date + self.train_duration)\n            ]\n            X_valid_df = X_index_df[\n                (X_index_df[\"__date__\"] &gt;= current_date + self.train_duration + self.gap_duration)\n                &amp; (\n                    X_index_df[\"__date__\"]\n                    &lt; current_date + self.train_duration + self.valid_duration + self.gap_duration\n                )\n            ]\n\n            current_date = current_date + time_shift\n            if self.window == \"rolling\":\n                start_date = current_date\n            yield (\n                X_train_df[\"np_index\"].values,\n                X_valid_df[\"np_index\"].values,\n            )\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Get the number of splits\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            Dataframe with the data to split.\n        y : array-like | None, default=None\n            Ignored, present for compatibility.\n        groups : array-like | None, default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        int\n            Number of splits.\n        \"\"\"\n        return sum(1 for x in self.split(X, y, groups))\n\n    def summary(self, X):\n        \"\"\"Describe all folds.\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            Dataframe with the data to split.\n\n        Returns\n        -------\n        pd.DataFrame\n            Summary of all folds.\n        \"\"\"\n        summary = []\n        X_index_df = self._join_date_and_x(X)\n\n        def get_split_info(X, indices, j, part, summary):\n            dates = X_index_df.iloc[indices][\"__date__\"]\n            mindate = dates.min()\n            maxdate = dates.max()\n\n            s = pd.Series(\n                {\n                    \"Start date\": mindate,\n                    \"End date\": maxdate,\n                    \"Period\": pd.to_datetime(maxdate, format=\"%Y%m%d\") - pd.to_datetime(mindate, format=\"%Y%m%d\"),\n                    \"Unique days\": len(dates.unique()),\n                    \"nbr samples\": len(indices),\n                },\n                name=(j, part),\n            )\n            summary.append(s)\n            return summary\n\n        j = 0\n        for i in self.split(X):\n            summary = get_split_info(X, i[0], j, \"train\", summary)\n            summary = get_split_info(X, i[1], j, \"valid\", summary)\n            j = j + 1\n\n        return pd.DataFrame(summary)\n</code></pre>"},{"location":"api/model-selection/#sklego.model_selection.TimeGapSplit.get_n_splits","title":"<code>get_n_splits(X=None, y=None, groups=None)</code>","text":"<p>Get the number of splits</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Dataframe with the data to split.</p> <code>None</code> <code>y</code> <code>array - like | None</code> <p>Ignored, present for compatibility.</p> <code>None</code> <code>groups</code> <code>array - like | None</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of splits.</p> Source code in <code>sklego/model_selection.py</code> <pre><code>def get_n_splits(self, X=None, y=None, groups=None):\n    \"\"\"Get the number of splits\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n        Dataframe with the data to split.\n    y : array-like | None, default=None\n        Ignored, present for compatibility.\n    groups : array-like | None, default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    int\n        Number of splits.\n    \"\"\"\n    return sum(1 for x in self.split(X, y, groups))\n</code></pre>"},{"location":"api/model-selection/#sklego.model_selection.TimeGapSplit.split","title":"<code>split(X, y=None, groups=None)</code>","text":"<p>Generate indices to split data into training and test set.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Dataframe with the data to split.</p> required <code>y</code> <code>array - like | None</code> <p>Ignored, present for compatibility.</p> <code>None</code> <code>groups</code> <code>array - like | None</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Yields:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>Train and test indices of the same fold.</p> Source code in <code>sklego/model_selection.py</code> <pre><code>def split(self, X, y=None, groups=None):\n    \"\"\"Generate indices to split data into training and test set.\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n        Dataframe with the data to split.\n    y : array-like | None, default=None\n        Ignored, present for compatibility.\n    groups : array-like | None, default=None\n        Ignored, present for compatibility.\n\n    Yields\n    -------\n    tuple[np.ndarray, np.ndarray]\n        Train and test indices of the same fold.\n    \"\"\"\n\n    X_index_df = self._join_date_and_x(X)\n    X_index_df = X_index_df.sort_values(\"__date__\", ascending=True)\n\n    if len(X) != len(X_index_df):\n        raise AssertionError(\n            \"X and X_index_df are not the same length, \" \"there must be some index missing in 'self.date_serie'\"\n        )\n\n    date_min = X_index_df[\"__date__\"].min()\n    date_max = X_index_df[\"__date__\"].max()\n    date_length = X_index_df[\"__date__\"].max() - X_index_df[\"__date__\"].min()\n\n    if (self.train_duration is None) and (self.n_splits is not None):\n        self.train_duration = date_length - (self.gap_duration + self.valid_duration * self.n_splits)\n\n    if (self.train_duration is not None) and (self.train_duration &lt;= self.gap_duration):\n        raise ValueError(\"gap_duration is longer than train_duration, it should be shorter.\")\n\n    n_split_max = (date_length - self.train_duration - self.gap_duration) / self.valid_duration\n    if self.n_splits:\n        if n_split_max &lt; self.n_splits:\n            raise ValueError(\n                (\n                    \"Number of folds requested = {1} are greater\"\n                    \" than maximum  ={0} possible without\"\n                    \" overlapping validation sets.\"\n                ).format(n_split_max, self.n_splits)\n            )\n\n    current_date = date_min\n    start_date = date_min\n    # if the n_splits is smaller than what would usually be done for train val and gap duration,\n    # the next fold is slightly further in time than just valid_duration\n    if self.n_splits is not None:\n        time_shift = self.valid_duration * n_split_max / self.n_splits\n    else:\n        time_shift = self.valid_duration\n    while True:\n        if current_date + self.train_duration + time_shift + self.gap_duration &gt; date_max:\n            break\n\n        X_train_df = X_index_df[\n            (X_index_df[\"__date__\"] &gt;= start_date) &amp; (X_index_df[\"__date__\"] &lt; current_date + self.train_duration)\n        ]\n        X_valid_df = X_index_df[\n            (X_index_df[\"__date__\"] &gt;= current_date + self.train_duration + self.gap_duration)\n            &amp; (\n                X_index_df[\"__date__\"]\n                &lt; current_date + self.train_duration + self.valid_duration + self.gap_duration\n            )\n        ]\n\n        current_date = current_date + time_shift\n        if self.window == \"rolling\":\n            start_date = current_date\n        yield (\n            X_train_df[\"np_index\"].values,\n            X_valid_df[\"np_index\"].values,\n        )\n</code></pre>"},{"location":"api/model-selection/#sklego.model_selection.TimeGapSplit.summary","title":"<code>summary(X)</code>","text":"<p>Describe all folds.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Dataframe with the data to split.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Summary of all folds.</p> Source code in <code>sklego/model_selection.py</code> <pre><code>def summary(self, X):\n    \"\"\"Describe all folds.\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n        Dataframe with the data to split.\n\n    Returns\n    -------\n    pd.DataFrame\n        Summary of all folds.\n    \"\"\"\n    summary = []\n    X_index_df = self._join_date_and_x(X)\n\n    def get_split_info(X, indices, j, part, summary):\n        dates = X_index_df.iloc[indices][\"__date__\"]\n        mindate = dates.min()\n        maxdate = dates.max()\n\n        s = pd.Series(\n            {\n                \"Start date\": mindate,\n                \"End date\": maxdate,\n                \"Period\": pd.to_datetime(maxdate, format=\"%Y%m%d\") - pd.to_datetime(mindate, format=\"%Y%m%d\"),\n                \"Unique days\": len(dates.unique()),\n                \"nbr samples\": len(indices),\n            },\n            name=(j, part),\n        )\n        summary.append(s)\n        return summary\n\n    j = 0\n    for i in self.split(X):\n        summary = get_split_info(X, i[0], j, \"train\", summary)\n        summary = get_split_info(X, i[1], j, \"valid\", summary)\n        j = j + 1\n\n    return pd.DataFrame(summary)\n</code></pre>"},{"location":"api/naive-bayes/","title":"Naive Bayes","text":""},{"location":"api/naive-bayes/#sklego.naive_bayes.GaussianMixtureNB","title":"<code>sklego.naive_bayes.GaussianMixtureNB</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>The <code>GaussianMixtureNB</code> estimator is a naive bayes classifier that uses a mixture of gaussians instead of merely a single one. In particular it trains a <code>GaussianMixture</code> model for each class in the target and for each feature in the data, on the subset of <code>X</code> where <code>y == class</code>.</p> <p>You can pass any keyword parameter that scikit-learn's GaussianMixture model uses and it will be passed along to each of the models.</p> <p>Attributes:</p> Name Type Description <code>gmms_</code> <code>dict[int, List[GaussianMixture]]</code> <p>A dictionary of Gaussian Mixture Models, one for each class.</p> <code>classes_</code> <code>np.ndarray of shape (n_classes,)</code> <p>The classes seen during <code>fit</code>.</p> <code>n_features_in_</code> <code>int</code> <p>The number of features seen during <code>fit</code>.</p> <code>num_fit_cols_</code> <code>int</code> <p>Deprecated, please use <code>n_features_in_</code> instead.</p> Source code in <code>sklego/naive_bayes.py</code> <pre><code>class GaussianMixtureNB(BaseEstimator, ClassifierMixin):\n    \"\"\"The `GaussianMixtureNB` estimator is a naive bayes classifier that uses a mixture of gaussians instead of\n    merely a single one. In particular it trains a `GaussianMixture` model for each class in the target and for each\n    feature in the data, on the subset of `X` where `y == class`.\n\n    You can pass any keyword parameter that scikit-learn's\n    [GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)\n    model uses and it will be passed along to each of the models.\n\n    Attributes\n    ----------\n    gmms_ : dict[int, List[GaussianMixture]]\n        A dictionary of Gaussian Mixture Models, one for each class.\n    classes_ : np.ndarray of shape (n_classes,)\n        The classes seen during `fit`.\n    n_features_in_ : int\n        The number of features seen during `fit`.\n    num_fit_cols_ : int\n        Deprecated, please use `n_features_in_` instead.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components=1,\n        covariance_type=\"full\",\n        tol=1e-3,\n        reg_covar=1e-6,\n        max_iter=100,\n        n_init=1,\n        init_params=\"kmeans\",\n        weights_init=None,\n        means_init=None,\n        precisions_init=None,\n        random_state=None,\n        warm_start=False,\n    ):\n        self.n_components = n_components\n        self.covariance_type = covariance_type\n        self.tol = tol\n        self.reg_covar = reg_covar\n        self.max_iter = max_iter\n        self.n_init = n_init\n        self.init_params = init_params\n        self.weights_init = weights_init\n        self.means_init = means_init\n        self.precisions_init = precisions_init\n        self.random_state = random_state\n        self.warm_start = warm_start\n\n    def fit(self, X, y) -&gt; \"GaussianMixtureNB\":\n        \"\"\"Fit the `GaussianMixtureNB` estimator using `X` and `y` as training data by fitting a `GaussianMixture` model\n        for each class in the target and for each feature in the data, on the subset of `X` where `y == class`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n\n        Returns\n        -------\n        self : GaussianMixtureNB\n            The fitted estimator.\n        \"\"\"\n        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n        if X.ndim == 1:\n            X = np.expand_dims(X, 1)\n\n        self.gmms_ = {}\n        self.classes_ = unique_labels(y)\n        self.n_features_in_ = X.shape[1]\n        for c in self.classes_:\n            subset_x, subset_y = X[y == c], y[y == c]\n            self.gmms_[c] = [\n                GaussianMixture(\n                    n_components=self.n_components,\n                    covariance_type=self.covariance_type,\n                    tol=self.tol,\n                    reg_covar=self.reg_covar,\n                    max_iter=self.max_iter,\n                    n_init=self.n_init,\n                    init_params=self.init_params,\n                    weights_init=self.weights_init,\n                    means_init=self.means_init,\n                    precisions_init=self.precisions_init,\n                    random_state=self.random_state,\n                    warm_start=self.warm_start,\n                ).fit(subset_x[:, i].reshape(-1, 1), subset_y)\n                for i in range(X.shape[1])\n            ]\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict labels for `X` using fitted estimator and `predict_proba` method, by picking the class with the\n        highest probability.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        check_is_fitted(self, [\"gmms_\", \"classes_\", \"n_features_in_\"])\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        return self.classes_[self.predict_proba(X).argmax(axis=1)]\n\n    def predict_proba(self, X: np.ndarray):\n        \"\"\"Predict probabilities for `X` using fitted estimator by summing the probabilities of each gaussian for each\n        class.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_classes)\n            The predicted probabilities.\n        \"\"\"\n        check_is_fitted(self, [\"gmms_\", \"classes_\", \"n_features_in_\"])\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        if self.n_features_in_ != X.shape[1]:\n            raise ValueError(f\"number of columns {X.shape[1]} does not match fit size {self.n_features_in_}\")\n        check_is_fitted(self, [\"gmms_\", \"classes_\"])\n        probs = np.zeros((X.shape[0], len(self.classes_)))\n        for k, v in self.gmms_.items():\n            class_idx = int(np.argwhere(self.classes_ == k))\n            probs[:, class_idx] = np.array(\n                [m.score_samples(np.expand_dims(X[:, idx], 1)) for idx, m in enumerate(v)]\n            ).sum(axis=0)\n        likelihood = np.exp(probs)\n        return likelihood / likelihood.sum(axis=1).reshape(-1, 1)\n\n    @property\n    def num_fit_cols_(self):\n        warn(\n            \"Please use `n_features_in_` instead of `num_fit_cols_`,\"\n            \"`num_fit_cols_` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self.n_features_in_\n</code></pre>"},{"location":"api/naive-bayes/#sklego.naive_bayes.GaussianMixtureNB.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the <code>GaussianMixtureNB</code> estimator using <code>X</code> and <code>y</code> as training data by fitting a <code>GaussianMixture</code> model for each class in the target and for each feature in the data, on the subset of <code>X</code> where <code>y == class</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>GaussianMixtureNB</code> <p>The fitted estimator.</p> Source code in <code>sklego/naive_bayes.py</code> <pre><code>def fit(self, X, y) -&gt; \"GaussianMixtureNB\":\n    \"\"\"Fit the `GaussianMixtureNB` estimator using `X` and `y` as training data by fitting a `GaussianMixture` model\n    for each class in the target and for each feature in the data, on the subset of `X` where `y == class`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n\n    Returns\n    -------\n    self : GaussianMixtureNB\n        The fitted estimator.\n    \"\"\"\n    X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n    if X.ndim == 1:\n        X = np.expand_dims(X, 1)\n\n    self.gmms_ = {}\n    self.classes_ = unique_labels(y)\n    self.n_features_in_ = X.shape[1]\n    for c in self.classes_:\n        subset_x, subset_y = X[y == c], y[y == c]\n        self.gmms_[c] = [\n            GaussianMixture(\n                n_components=self.n_components,\n                covariance_type=self.covariance_type,\n                tol=self.tol,\n                reg_covar=self.reg_covar,\n                max_iter=self.max_iter,\n                n_init=self.n_init,\n                init_params=self.init_params,\n                weights_init=self.weights_init,\n                means_init=self.means_init,\n                precisions_init=self.precisions_init,\n                random_state=self.random_state,\n                warm_start=self.warm_start,\n            ).fit(subset_x[:, i].reshape(-1, 1), subset_y)\n            for i in range(X.shape[1])\n        ]\n    return self\n</code></pre>"},{"location":"api/naive-bayes/#sklego.naive_bayes.GaussianMixtureNB.predict","title":"<code>predict(X)</code>","text":"<p>Predict labels for <code>X</code> using fitted estimator and <code>predict_proba</code> method, by picking the class with the highest probability.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted data.</p> Source code in <code>sklego/naive_bayes.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for `X` using fitted estimator and `predict_proba` method, by picking the class with the\n    highest probability.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted data.\n    \"\"\"\n    check_is_fitted(self, [\"gmms_\", \"classes_\", \"n_features_in_\"])\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    return self.classes_[self.predict_proba(X).argmax(axis=1)]\n</code></pre>"},{"location":"api/naive-bayes/#sklego.naive_bayes.GaussianMixtureNB.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict probabilities for <code>X</code> using fitted estimator by summing the probabilities of each gaussian for each class.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_classes)</code> <p>The predicted probabilities.</p> Source code in <code>sklego/naive_bayes.py</code> <pre><code>def predict_proba(self, X: np.ndarray):\n    \"\"\"Predict probabilities for `X` using fitted estimator by summing the probabilities of each gaussian for each\n    class.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_classes)\n        The predicted probabilities.\n    \"\"\"\n    check_is_fitted(self, [\"gmms_\", \"classes_\", \"n_features_in_\"])\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    if self.n_features_in_ != X.shape[1]:\n        raise ValueError(f\"number of columns {X.shape[1]} does not match fit size {self.n_features_in_}\")\n    check_is_fitted(self, [\"gmms_\", \"classes_\"])\n    probs = np.zeros((X.shape[0], len(self.classes_)))\n    for k, v in self.gmms_.items():\n        class_idx = int(np.argwhere(self.classes_ == k))\n        probs[:, class_idx] = np.array(\n            [m.score_samples(np.expand_dims(X[:, idx], 1)) for idx, m in enumerate(v)]\n        ).sum(axis=0)\n    likelihood = np.exp(probs)\n    return likelihood / likelihood.sum(axis=1).reshape(-1, 1)\n</code></pre>"},{"location":"api/naive-bayes/#sklego.naive_bayes.BayesianGaussianMixtureNB","title":"<code>sklego.naive_bayes.BayesianGaussianMixtureNB</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>The <code>BayesianGaussianMixtureNB</code> estimator is a naive bayes classifier that uses a bayesian mixture of gaussians instead of merely a single one. In particular it trains a <code>BayesianGaussianMixture</code> model for each class in the target and for each feature in the data, on the subset of <code>X</code> where <code>y == class</code>.</p> <p>You can pass any keyword parameter that scikit-learn's <code>BayesianGaussianMixture</code> model uses and it will be passed along to each of the models.</p> <p>Attributes:</p> Name Type Description <code>gmms_</code> <code>dict[int, List[BayesianGaussianMixture]]</code> <p>A dictionary of Gaussian Mixture Models, one for each class.</p> <code>classes_</code> <code>np.ndarray of shape (n_classes,)</code> <p>The classes seen during <code>fit</code>.</p> <code>n_features_in_</code> <code>int</code> <p>The number of features seen during <code>fit</code>.</p> <code>num_fit_cols_</code> <code>int</code> <p>Deprecated, please use <code>n_features_in_</code> instead.</p> Source code in <code>sklego/naive_bayes.py</code> <pre><code>class BayesianGaussianMixtureNB(BaseEstimator, ClassifierMixin):\n    \"\"\"The `BayesianGaussianMixtureNB` estimator is a naive bayes classifier that uses a bayesian mixture of gaussians\n    instead of merely a single one. In particular it trains a `BayesianGaussianMixture` model for each class in the\n    target and for each feature in the data, on the subset of `X` where `y == class`.\n\n    You can pass any keyword parameter that scikit-learn's\n    [`BayesianGaussianMixture`](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html)\n    model uses and it will be passed along to each of the models.\n\n    Attributes\n    ----------\n    gmms_ : dict[int, List[BayesianGaussianMixture]]\n        A dictionary of Gaussian Mixture Models, one for each class.\n    classes_ : np.ndarray of shape (n_classes,)\n        The classes seen during `fit`.\n    n_features_in_ : int\n        The number of features seen during `fit`.\n    num_fit_cols_ : int\n        Deprecated, please use `n_features_in_` instead.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components=1,\n        covariance_type=\"full\",\n        tol=0.001,\n        reg_covar=1e-06,\n        max_iter=100,\n        n_init=1,\n        init_params=\"kmeans\",\n        weight_concentration_prior_type=\"dirichlet_process\",\n        weight_concentration_prior=None,\n        mean_precision_prior=None,\n        mean_prior=None,\n        degrees_of_freedom_prior=None,\n        covariance_prior=None,\n        random_state=None,\n        warm_start=False,\n        verbose=0,\n        verbose_interval=10,\n    ):\n        self.n_components = n_components\n        self.covariance_type = covariance_type\n        self.tol = tol\n        self.reg_covar = reg_covar\n        self.max_iter = max_iter\n        self.n_init = n_init\n        self.init_params = init_params\n        self.weight_concentration_prior_type = weight_concentration_prior_type\n        self.weight_concentration_prior = weight_concentration_prior\n        self.mean_precision_prior = mean_precision_prior\n        self.mean_prior = mean_prior\n        self.degrees_of_freedom_prior = degrees_of_freedom_prior\n        self.covariance_prior = covariance_prior\n        self.random_state = random_state\n        self.warm_start = warm_start\n        self.verbose = verbose\n        self.verbose_interval = verbose_interval\n\n    def fit(self, X, y) -&gt; \"BayesianGaussianMixtureNB\":\n        \"\"\"Fit the `BayesianGaussianMixtureNB` estimator using `X` and `y` as training data by fitting a\n        `BayesianGaussianMixture` model for each class in the target and for each feature in the data, on the subset of\n        `X` where `y == class`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features )\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n\n        Returns\n        -------\n        self : BayesianGaussianMixtureNB\n            The fitted estimator.\n        \"\"\"\n        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n        if X.ndim == 1:\n            X = np.expand_dims(X, 1)\n\n        self.gmms_ = {}\n        self.classes_ = unique_labels(y)\n        self.n_features_in_ = X.shape[1]\n        for c in self.classes_:\n            subset_x, subset_y = X[y == c], y[y == c]\n            self.gmms_[c] = [\n                BayesianGaussianMixture(\n                    n_components=self.n_components,\n                    covariance_type=self.covariance_type,\n                    tol=self.tol,\n                    reg_covar=self.reg_covar,\n                    max_iter=self.max_iter,\n                    n_init=self.n_init,\n                    init_params=self.init_params,\n                    weight_concentration_prior_type=self.weight_concentration_prior_type,\n                    weight_concentration_prior=self.weight_concentration_prior,\n                    mean_precision_prior=self.mean_precision_prior,\n                    mean_prior=self.mean_prior,\n                    degrees_of_freedom_prior=self.degrees_of_freedom_prior,\n                    covariance_prior=self.covariance_prior,\n                    random_state=self.random_state,\n                    warm_start=self.warm_start,\n                    verbose=self.verbose,\n                    verbose_interval=self.verbose_interval,\n                ).fit(subset_x[:, i].reshape(-1, 1), subset_y)\n                for i in range(X.shape[1])\n            ]\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict labels for `X` using fitted estimator and `predict_proba` method, by picking the class with the\n        highest probability.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        check_is_fitted(self, [\"gmms_\", \"classes_\", \"n_features_in_\"])\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        return self.classes_[self.predict_proba(X).argmax(axis=1)]\n\n    def predict_proba(self, X: np.ndarray):\n        \"\"\"Predict probabilities for `X` using fitted estimator by summing the probabilities of each gaussian for each\n        class.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_classes)\n            The predicted probabilities.\n        \"\"\"\n        check_is_fitted(self, [\"gmms_\", \"classes_\", \"n_features_in_\"])\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n        if self.n_features_in_ != X.shape[1]:\n            raise ValueError(f\"number of columns {X.shape[1]} does not match fit size {self.n_features_in_}\")\n        check_is_fitted(self, [\"gmms_\", \"classes_\"])\n        probs = np.zeros((X.shape[0], len(self.classes_)))\n        for k, v in self.gmms_.items():\n            class_idx = int(np.argwhere(self.classes_ == k))\n            probs[:, class_idx] = np.array(\n                [m.score_samples(np.expand_dims(X[:, idx], 1)) for idx, m in enumerate(v)]\n            ).sum(axis=0)\n        likelihood = np.exp(probs)\n        return likelihood / likelihood.sum(axis=1).reshape(-1, 1)\n\n    @property\n    def num_fit_cols_(self):\n        warn(\n            \"Please use `n_features_in_` instead of `num_fit_cols_`,\"\n            \"`num_fit_cols_` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self.n_features_in_\n</code></pre>"},{"location":"api/naive-bayes/#sklego.naive_bayes.BayesianGaussianMixtureNB.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the <code>BayesianGaussianMixtureNB</code> estimator using <code>X</code> and <code>y</code> as training data by fitting a <code>BayesianGaussianMixture</code> model for each class in the target and for each feature in the data, on the subset of <code>X</code> where <code>y == class</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features )</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>BayesianGaussianMixtureNB</code> <p>The fitted estimator.</p> Source code in <code>sklego/naive_bayes.py</code> <pre><code>def fit(self, X, y) -&gt; \"BayesianGaussianMixtureNB\":\n    \"\"\"Fit the `BayesianGaussianMixtureNB` estimator using `X` and `y` as training data by fitting a\n    `BayesianGaussianMixture` model for each class in the target and for each feature in the data, on the subset of\n    `X` where `y == class`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features )\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n\n    Returns\n    -------\n    self : BayesianGaussianMixtureNB\n        The fitted estimator.\n    \"\"\"\n    X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n    if X.ndim == 1:\n        X = np.expand_dims(X, 1)\n\n    self.gmms_ = {}\n    self.classes_ = unique_labels(y)\n    self.n_features_in_ = X.shape[1]\n    for c in self.classes_:\n        subset_x, subset_y = X[y == c], y[y == c]\n        self.gmms_[c] = [\n            BayesianGaussianMixture(\n                n_components=self.n_components,\n                covariance_type=self.covariance_type,\n                tol=self.tol,\n                reg_covar=self.reg_covar,\n                max_iter=self.max_iter,\n                n_init=self.n_init,\n                init_params=self.init_params,\n                weight_concentration_prior_type=self.weight_concentration_prior_type,\n                weight_concentration_prior=self.weight_concentration_prior,\n                mean_precision_prior=self.mean_precision_prior,\n                mean_prior=self.mean_prior,\n                degrees_of_freedom_prior=self.degrees_of_freedom_prior,\n                covariance_prior=self.covariance_prior,\n                random_state=self.random_state,\n                warm_start=self.warm_start,\n                verbose=self.verbose,\n                verbose_interval=self.verbose_interval,\n            ).fit(subset_x[:, i].reshape(-1, 1), subset_y)\n            for i in range(X.shape[1])\n        ]\n    return self\n</code></pre>"},{"location":"api/naive-bayes/#sklego.naive_bayes.BayesianGaussianMixtureNB.predict","title":"<code>predict(X)</code>","text":"<p>Predict labels for <code>X</code> using fitted estimator and <code>predict_proba</code> method, by picking the class with the highest probability.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted data.</p> Source code in <code>sklego/naive_bayes.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for `X` using fitted estimator and `predict_proba` method, by picking the class with the\n    highest probability.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted data.\n    \"\"\"\n    check_is_fitted(self, [\"gmms_\", \"classes_\", \"n_features_in_\"])\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    return self.classes_[self.predict_proba(X).argmax(axis=1)]\n</code></pre>"},{"location":"api/naive-bayes/#sklego.naive_bayes.BayesianGaussianMixtureNB.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict probabilities for <code>X</code> using fitted estimator by summing the probabilities of each gaussian for each class.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_classes)</code> <p>The predicted probabilities.</p> Source code in <code>sklego/naive_bayes.py</code> <pre><code>def predict_proba(self, X: np.ndarray):\n    \"\"\"Predict probabilities for `X` using fitted estimator by summing the probabilities of each gaussian for each\n    class.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_classes)\n        The predicted probabilities.\n    \"\"\"\n    check_is_fitted(self, [\"gmms_\", \"classes_\", \"n_features_in_\"])\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n    if self.n_features_in_ != X.shape[1]:\n        raise ValueError(f\"number of columns {X.shape[1]} does not match fit size {self.n_features_in_}\")\n    check_is_fitted(self, [\"gmms_\", \"classes_\"])\n    probs = np.zeros((X.shape[0], len(self.classes_)))\n    for k, v in self.gmms_.items():\n        class_idx = int(np.argwhere(self.classes_ == k))\n        probs[:, class_idx] = np.array(\n            [m.score_samples(np.expand_dims(X[:, idx], 1)) for idx, m in enumerate(v)]\n        ).sum(axis=0)\n    likelihood = np.exp(probs)\n    return likelihood / likelihood.sum(axis=1).reshape(-1, 1)\n</code></pre>"},{"location":"api/neighbors/","title":"Neighbors","text":""},{"location":"api/neighbors/#sklego.neighbors.BayesianKernelDensityClassifier","title":"<code>sklego.neighbors.BayesianKernelDensityClassifier</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>The <code>BayesianKernelDensityClassifier</code> estimator trains using Kernel Density estimations to generate the joint distribution.</p> <p>You can pass any keyword parameter that scikit-learn's KernelDensity model uses and it will be passed along.</p> <p>Attributes:</p> Name Type Description <code>classes_</code> <code>np.ndarray of shape (n_classes,)</code> <p>The classes seen during <code>fit</code>.</p> <code>models_</code> <code>dict[int, KernelDensity]</code> <p>The models for each class seen during <code>fit</code>.</p> <code>priors_logp_</code> <code>dict</code> <p>The log priors for each class seen during <code>fit</code> (estimated as <code>np.log(len(x_subset) / len(X))</code>)</p> Source code in <code>sklego/neighbors.py</code> <pre><code>class BayesianKernelDensityClassifier(BaseEstimator, ClassifierMixin):\n    \"\"\"The `BayesianKernelDensityClassifier` estimator trains using Kernel Density estimations to generate the joint\n    distribution.\n\n    You can pass any keyword parameter that scikit-learn's\n    [KernelDensity](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html)\n    model uses and it will be passed along.\n\n    Attributes\n    ----------\n    classes_ : np.ndarray of shape (n_classes,)\n        The classes seen during `fit`.\n    models_ : dict[int, KernelDensity]\n        The models for each class seen during `fit`.\n    priors_logp_ : dict\n        The log priors for each class seen during `fit` (estimated as `np.log(len(x_subset) / len(X))`)\n    \"\"\"\n\n    def __init__(\n        self,\n        bandwidth=0.2,\n        kernel=\"gaussian\",\n        algorithm=\"auto\",\n        metric=\"euclidean\",\n        atol=0,\n        rtol=0,\n        breath_first=True,\n        leaf_size=40,\n        metric_params=None,\n    ):\n        self.bandwidth = bandwidth\n        self.kernel = kernel\n        self.algorithm = algorithm\n        self.metric = metric\n        self.atol = atol\n        self.rtol = rtol\n        self.breath_first = breath_first\n        self.leaf_size = leaf_size\n        self.metric_params = metric_params\n\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"Fit the `BayesianKernelDensityClassifier` estimator using `X` and `y` as training data by fitting a\n        `KernelDensity` model for each class on the subset of X where y == class.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The training data.\n        y : array-like of shape (n_samples,)\n            The target values.\n\n        Returns\n        -------\n        self : BayesianKernelDensityClassifier\n            The fitted estimator.\n        \"\"\"\n        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n\n        self.classes_ = unique_labels(y)\n        self.models_, self.priors_logp_ = {}, {}\n        for target_label in self.classes_:\n            x_subset = X[y == target_label]\n\n            # Computing joint distribution\n            self.models_[target_label] = KernelDensity(\n                bandwidth=self.bandwidth,\n                kernel=self.kernel,\n                algorithm=self.algorithm,\n                metric=self.metric,\n                atol=self.atol,\n                rtol=self.rtol,\n                breadth_first=self.breath_first,\n                leaf_size=self.leaf_size,\n                metric_params=self.metric_params,\n            ).fit(x_subset)\n\n            # Computing target class prior\n            self.priors_logp_[target_label] = np.log(len(x_subset) / len(X))\n\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Predict probabilities for `X` using fitted estimator and the joint distribution.\n\n        The returned estimates for all classes are in the same order found in the `.classes_` attribute.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_classes)\n            The predicted probabilities for each class, ordered as in `self.classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n\n        log_prior = np.array([self.priors_logp_[target_label] for target_label in self.classes_])\n\n        log_likelihood = np.array([self.models_[target_label].score_samples(X) for target_label in self.classes_]).T\n\n        log_likelihood_and_prior = np.exp(log_likelihood + log_prior)\n        evidence = log_likelihood_and_prior.sum(axis=1, keepdims=True)\n        posterior = log_likelihood_and_prior / evidence\n        return posterior\n\n    def predict(self, X):\n        \"\"\"Predict labels for `X` using fitted estimator and `predict_proba()` method, by taking the class with the\n        highest probability.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to predict.\n\n        Returns\n        -------\n        array-like of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n\n        return self.classes_[np.argmax(self.predict_proba(X), 1)]\n</code></pre>"},{"location":"api/neighbors/#sklego.neighbors.BayesianKernelDensityClassifier.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the <code>BayesianKernelDensityClassifier</code> estimator using <code>X</code> and <code>y</code> as training data by fitting a <code>KernelDensity</code> model for each class on the subset of X where y == class.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>The target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>BayesianKernelDensityClassifier</code> <p>The fitted estimator.</p> Source code in <code>sklego/neighbors.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray):\n    \"\"\"Fit the `BayesianKernelDensityClassifier` estimator using `X` and `y` as training data by fitting a\n    `KernelDensity` model for each class on the subset of X where y == class.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The training data.\n    y : array-like of shape (n_samples,)\n        The target values.\n\n    Returns\n    -------\n    self : BayesianKernelDensityClassifier\n        The fitted estimator.\n    \"\"\"\n    X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n\n    self.classes_ = unique_labels(y)\n    self.models_, self.priors_logp_ = {}, {}\n    for target_label in self.classes_:\n        x_subset = X[y == target_label]\n\n        # Computing joint distribution\n        self.models_[target_label] = KernelDensity(\n            bandwidth=self.bandwidth,\n            kernel=self.kernel,\n            algorithm=self.algorithm,\n            metric=self.metric,\n            atol=self.atol,\n            rtol=self.rtol,\n            breadth_first=self.breath_first,\n            leaf_size=self.leaf_size,\n            metric_params=self.metric_params,\n        ).fit(x_subset)\n\n        # Computing target class prior\n        self.priors_logp_[target_label] = np.log(len(x_subset) / len(X))\n\n    return self\n</code></pre>"},{"location":"api/neighbors/#sklego.neighbors.BayesianKernelDensityClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict labels for <code>X</code> using fitted estimator and <code>predict_proba()</code> method, by taking the class with the highest probability.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples,)</code> <p>The predicted data.</p> Source code in <code>sklego/neighbors.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for `X` using fitted estimator and `predict_proba()` method, by taking the class with the\n    highest probability.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples,)\n        The predicted data.\n    \"\"\"\n    check_is_fitted(self)\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n\n    return self.classes_[np.argmax(self.predict_proba(X), 1)]\n</code></pre>"},{"location":"api/neighbors/#sklego.neighbors.BayesianKernelDensityClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict probabilities for <code>X</code> using fitted estimator and the joint distribution.</p> <p>The returned estimates for all classes are in the same order found in the <code>.classes_</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to predict.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_classes)</code> <p>The predicted probabilities for each class, ordered as in <code>self.classes_</code>.</p> Source code in <code>sklego/neighbors.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict probabilities for `X` using fitted estimator and the joint distribution.\n\n    The returned estimates for all classes are in the same order found in the `.classes_` attribute.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to predict.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_classes)\n        The predicted probabilities for each class, ordered as in `self.classes_`.\n    \"\"\"\n    check_is_fitted(self)\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n\n    log_prior = np.array([self.priors_logp_[target_label] for target_label in self.classes_])\n\n    log_likelihood = np.array([self.models_[target_label].score_samples(X) for target_label in self.classes_]).T\n\n    log_likelihood_and_prior = np.exp(log_likelihood + log_prior)\n    evidence = log_likelihood_and_prior.sum(axis=1, keepdims=True)\n    posterior = log_likelihood_and_prior / evidence\n    return posterior\n</code></pre>"},{"location":"api/pandas-utils/","title":"Pandas Utils","text":""},{"location":"api/pandas-utils/#sklego.pandas_utils.add_lags","title":"<code>sklego.pandas_utils.add_lags(X, cols, lags, drop_na=True)</code>","text":"<p>Appends lag column(s).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Data to be lagged.</p> required <code>cols</code> <code>str | int | List[str] | List[int]</code> <p>Column name(s) or index (indices).</p> required <code>lags</code> <code>int | List[int]</code> <p>The amount of lag for each col.</p> required <code>drop_na</code> <code>bool</code> <p>Whether or not to remove rows that contain NA values.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame | ndarray</code> <p>With only the selected cols.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input is not a <code>pd.DataFrame</code> or <code>np.ndarray</code>.</p> <p>Examples:</p> <pre><code>import pandas as pd\ndf = pd.DataFrame([[1, 2, 3],\n                   [4, 5, 6],\n                   [7, 8, 9]],\n        columns=[\"a\", \"b\", \"c\"],\n        index=[1, 2, 3]\n)\n\nadd_lags(df, \"a\", [1]) # doctest: +NORMALIZE_WHITESPACE\n'''\n    a  b  c  a1\n1  1  2  3  4.0\n2  4  5  6  7.0\n'''\n\nadd_lags(df, [\"a\", \"b\"], 2) # doctest: +NORMALIZE_WHITESPACE\n'''\n    a  b  c  a2   b2\n1  1  2  3  7.0  8.0\n'''\n\nimport numpy as np\nX = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\nadd_lags(X, 0, [1])\n# array([[1, 2, 3, 4],\n#        [4, 5, 6, 7]])\n\nadd_lags(X, 1, [-1, 1])\n# array([[4, 5, 6, 2, 8]])\n</code></pre> Source code in <code>sklego/pandas_utils.py</code> <pre><code>def add_lags(X, cols, lags, drop_na=True):\n    \"\"\"Appends lag column(s).\n\n    Parameters\n    ----------\n    X : array-like\n        Data to be lagged.\n    cols : str | int | List[str] | List[int]\n        Column name(s) or index (indices).\n    lags : int | List[int]\n        The amount of lag for each col.\n    drop_na : bool, default=True\n        Whether or not to remove rows that contain NA values.\n\n    Returns\n    -------\n    pd.DataFrame | np.ndarray\n        With only the selected cols.\n\n    Raises\n    ------\n    ValueError\n        If the input is not a `pd.DataFrame` or `np.ndarray`.\n\n    Examples\n    --------\n    ```py\n    import pandas as pd\n    df = pd.DataFrame([[1, 2, 3],\n                       [4, 5, 6],\n                       [7, 8, 9]],\n            columns=[\"a\", \"b\", \"c\"],\n            index=[1, 2, 3]\n    )\n\n    add_lags(df, \"a\", [1]) # doctest: +NORMALIZE_WHITESPACE\n    '''\n        a  b  c  a1\n    1  1  2  3  4.0\n    2  4  5  6  7.0\n    '''\n\n    add_lags(df, [\"a\", \"b\"], 2) # doctest: +NORMALIZE_WHITESPACE\n    '''\n        a  b  c  a2   b2\n    1  1  2  3  7.0  8.0\n    '''\n\n    import numpy as np\n    X = np.array([[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 8, 9]])\n\n    add_lags(X, 0, [1])\n    # array([[1, 2, 3, 4],\n    #        [4, 5, 6, 7]])\n\n    add_lags(X, 1, [-1, 1])\n    # array([[4, 5, 6, 2, 8]])\n    ```\n    \"\"\"\n\n    # A single lag will be put in a list\n    lags = as_list(lags)\n\n    # Now we can iterate over the list to determine\n    # whether it is a list of integers\n    if not all(isinstance(x, int) for x in lags):\n        raise ValueError(\"lags must be a list of type: \" + str(int))\n\n    # The keys of the allowed_inputs dict contain the allowed\n    # types, and the values contain the associated handlers\n    allowed_inputs = {\n        pd.core.frame.DataFrame: _add_lagged_pandas_columns,\n        np.ndarray: _add_lagged_numpy_columns,\n    }\n\n    # Choose the correct handler based on the input class\n    for allowed_input, handler in allowed_inputs.items():\n        if isinstance(X, allowed_input):\n            return handler(X, cols, lags, drop_na)\n\n    # Otherwise, raise a ValueError\n    allowed_input_names = list(allowed_inputs.keys())\n    raise ValueError(\"X type should be one of:\", allowed_input_names)\n</code></pre>"},{"location":"api/pandas-utils/#sklego.pandas_utils.log_step","title":"<code>sklego.pandas_utils.log_step(func=None, *, time_taken=True, shape=True, shape_delta=False, names=False, dtypes=False, print_fn=print, display_args=True, log_error=True)</code>","text":"<p>Decorates a function that transforms a pandas dataframe to add automated logging statements.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable | None</code> <p>The function to decorate with logs. If None, returns a partial function with the given arguments.</p> <code>None</code> <code>time_taken</code> <code>bool</code> <p>Whether or not to log the time it took to run a function.</p> <code>True</code> <code>shape</code> <code>bool</code> <p>Whether or not to log the shape of the output result.</p> <code>True</code> <code>shape_delta</code> <code>bool</code> <p>Whether or not to log the difference in shape of input and output.</p> <code>False</code> <code>names</code> <code>bool</code> <p>Whether or not to log the names of the columns of the result.</p> <code>False</code> <code>dtypes</code> <code>bool</code> <p>Whether or not to log the dtypes of the result.</p> <code>False</code> <code>print_fn</code> <code>Callable</code> <p>Print function to use (e.g. <code>print</code> or <code>logger.info</code>)</p> <code>print</code> <code>display_args</code> <code>bool</code> <p>Whether or not to display the arguments given to the function.</p> <code>True</code> <code>log_error</code> <code>bool</code> <p>Whether or not to add the Exception message to the log if the function fails.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The decorated function.</p> <p>Examples:</p> <pre><code>@log_step\ndef remove_outliers(df, min_obs=5):\n    pass\n\n@log_step(print_fn=logging.info, shape_delta=True)\ndef remove_outliers(df, min_obs=5):\n    pass\n</code></pre> Source code in <code>sklego/pandas_utils.py</code> <pre><code>def log_step(\n    func=None,\n    *,\n    time_taken=True,\n    shape=True,\n    shape_delta=False,\n    names=False,\n    dtypes=False,\n    print_fn=print,\n    display_args=True,\n    log_error=True,\n):\n    \"\"\"Decorates a function that transforms a pandas dataframe to add automated logging statements.\n\n    Parameters\n    ----------\n    func : Callable | None, default=None\n        The function to decorate with logs. If None, returns a partial function with the given arguments.\n    time_taken : bool, default=True\n        Whether or not to log the time it took to run a function.\n    shape : bool, default=True\n        Whether or not to log the shape of the output result.\n    shape_delta : bool, default=False\n        Whether or not to log the difference in shape of input and output.\n    names : bool, default=False\n        Whether or not to log the names of the columns of the result.\n    dtypes : bool, default=False\n        Whether or not to log the dtypes of the result.\n    print_fn : Callable, default=print\n        Print function to use (e.g. `print` or `logger.info`)\n    display_args : bool, default=True\n        Whether or not to display the arguments given to the function.\n    log_error : bool, default=True\n        Whether or not to add the Exception message to the log if the function fails.\n\n    Returns\n    -------\n    Callable\n        The decorated function.\n\n    Examples\n    --------\n    ```py\n    @log_step\n    def remove_outliers(df, min_obs=5):\n        pass\n\n    @log_step(print_fn=logging.info, shape_delta=True)\n    def remove_outliers(df, min_obs=5):\n        pass\n    ```\n    \"\"\"\n\n    if func is None:\n        return partial(\n            log_step,\n            time_taken=time_taken,\n            shape=shape,\n            shape_delta=shape_delta,\n            names=names,\n            dtypes=dtypes,\n            print_fn=print_fn,\n            display_args=display_args,\n            log_error=log_error,\n        )\n\n    names = False if dtypes else names\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if shape_delta:\n            old_shape = args[0].shape\n        tic = dt.datetime.now()\n\n        optional_strings = []\n        try:\n            result = func(*args, **kwargs)\n            optional_strings = [\n                f\"time={dt.datetime.now() - tic}\" if time_taken else None,\n                f\"n_obs={result.shape[0]}, n_col={result.shape[1]}\" if shape else None,\n                _get_shape_delta(old_shape, result.shape) if shape_delta else None,\n                f\"names={result.columns.to_list()}\" if names else None,\n                f\"dtypes={result.dtypes.to_dict()}\" if dtypes else None,\n            ]\n            return result\n        except Exception as exc:\n            optional_strings = [\n                f\"time={dt.datetime.now() - tic}\" if time_taken else None,\n                \"FAILED\" + (f\" with error: {exc}\" if log_error else \"\"),\n            ]\n            raise\n        finally:\n            combined = \" \".join([s for s in optional_strings if s])\n\n            if display_args:\n                func_args = inspect.signature(func).bind(*args, **kwargs).arguments\n                func_args_str = \"\".join(\", {} = {!r}\".format(*item) for item in list(func_args.items())[1:])\n                print_fn(\n                    f\"[{func.__name__}(df{func_args_str})] \" + combined,\n                )\n            else:\n                print_fn(\n                    f\"[{func.__name__}]\" + combined,\n                )\n\n    return wrapper\n</code></pre>"},{"location":"api/pandas-utils/#sklego.pandas_utils.log_step_extra","title":"<code>sklego.pandas_utils.log_step_extra(*log_functions, print_fn=print, **log_func_kwargs)</code>","text":"<p>Decorates a function that transforms a pandas dataframe to add automated logging statements.</p> <p>Parameters:</p> Name Type Description Default <code>*log_functions</code> <code>List[Callable]</code> <p>Functions that take the output of the decorated function and turn it into a log. Note that the output of each log_function is casted to string using <code>str()</code>.</p> <code>()</code> <code>print_fn</code> <p>Print function (e.g. <code>print</code> or <code>logger.info</code>).</p> <code>print</code> <code>**log_func_kwargs</code> <p>Keyword arguments to be passed to <code>log_functions</code></p> <code>{}</code> <p>Returns: Callable     The decorated function.</p> <p>Examples:</p> <pre><code>@log_step_extra(lambda d: d[\"some_column\"].value_counts())\ndef remove_outliers(df, min_obs=5):\n    pass\n</code></pre> Source code in <code>sklego/pandas_utils.py</code> <pre><code>def log_step_extra(\n    *log_functions,\n    print_fn=print,\n    **log_func_kwargs,\n):\n    \"\"\"Decorates a function that transforms a pandas dataframe to add automated logging statements.\n\n    Parameters\n    ----------\n    *log_functions : List[Callable]\n        Functions that take the output of the decorated function and turn it into a log.\n        Note that the output of each log_function is casted to string using `str()`.\n    print_fn: Callable, default=print\n        Print function (e.g. `print` or `logger.info`).\n    **log_func_kwargs: dict\n        Keyword arguments to be passed to `log_functions`\n\n    Returns:\n    Callable\n        The decorated function.\n\n    Examples\n    --------\n    ```py\n    @log_step_extra(lambda d: d[\"some_column\"].value_counts())\n    def remove_outliers(df, min_obs=5):\n        pass\n    ```\n    \"\"\"\n    if not log_functions:\n        raise ValueError(\"Supply at least one log_function for log_step_extra\")\n\n    def _log_step_extra(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            result = func(*args, **kwargs)\n\n            func_args = inspect.signature(func).bind(*args, **kwargs).arguments\n            func_args_str = \"\".join(\", {} = {!r}\".format(*item) for item in list(func_args.items())[1:])\n\n            try:\n                extra_logs = \" \".join([str(log_f(result, **log_func_kwargs)) for log_f in log_functions])\n            except TypeError:\n                raise ValueError(\n                    f\"All log functions should be callable, got {[type(log_f) for log_f in log_functions]}\"\n                )\n\n            print_fn(\n                f\"[{func.__name__}(df{func_args_str})] \" + extra_logs,\n            )\n\n            return result\n\n        return wrapper\n\n    return _log_step_extra\n</code></pre>"},{"location":"api/pipeline/","title":"Pipeline","text":"<p>Pipelines, variances to the sklearn.pipeline.Pipeline object.</p>"},{"location":"api/pipeline/#sklego.pipeline.DebugPipeline","title":"<code>sklego.pipeline.DebugPipeline</code>","text":"<p>             Bases: <code>Pipeline</code></p> <p>A pipeline that has a log statement in between each step, useful for debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>log_callback</code> <code>Callable | None</code> <p>The callback function that logs information in between each intermediate step. If set to <code>\"default\"</code>, <code>default_log_callback</code> is used.</p> <code>None</code>"},{"location":"api/pipeline/#sklego.pipeline.DebugPipeline--notes","title":"Notes","text":"<p>See <code>sklearn.pipeline.Pipeline</code> for all other variables.</p> <p>Note</p> <p>This implementation is a hack on the original sklearn Pipeline. It aims to have the same behaviour as the original sklearn Pipeline, while changing minimal amount of code.</p> <p>The log statement is added by overwriting the cache method of the memory, such that the function called in the cache is wrapped with a functions that calls the log callback function (<code>log_callback</code>).</p> <p>This hack will break when:</p> <ul> <li>The sklearn pipeline initialization function is changed.</li> <li>The memory is used differently in the fit.</li> <li>The <code>joblib.memory.Memory</code>     changes behaviour of the <code>cache</code> method.</li> <li>The <code>joblib.memory.Memory</code>     starts using a <code>_cache</code> method.</li> </ul> <p>Examples:</p> <pre><code># Set-up for example\nimport logging\nimport sys\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklego.pipeline import DebugPipeline\n\nlogging.basicConfig(\n    format=(\"[%(funcName)s:%(lineno)d] - %(message)s\"),\n    level=logging.INFO,\n    stream=sys.stdout,\n    )\n\n# DebugPipeline set-up\nn_samples, n_features = 3, 5\nX = np.zeros((n_samples, n_features))\ny = np.arange(n_samples)\n\nclass Adder(TransformerMixin, BaseEstimator):\n    def __init__(self, value):\n        self._value = value\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X + self._value\n\n    def __repr__(self):\n        return f\"Adder(value={self._value})\"\n\nsteps = [\n    (\"add_1\", Adder(value=1)),\n    (\"add_10\", Adder(value=10)),\n    (\"add_100\", Adder(value=100)),\n    (\"add_1000\", Adder(value=1000)),\n]\n\n# The DebugPipeline behaves the sames as the Sklearn pipeline.\npipe = DebugPipeline(steps)\n\n_ = pipe.fit(X, y=y)\nprint(pipe.transform(X))\n# [[1111. 1111. 1111. 1111. 1111.]\n#  [1111. 1111. 1111. 1111. 1111.]\n#  [1111. 1111. 1111. 1111. 1111.]]\n\n# But it has the option to set a `log_callback`, that logs in between each step.\npipe = DebugPipeline(steps, log_callback=\"default\")\n\n_ = pipe.fit(X, y=y)\n# [default_log_callback:34] - [Adder(value=1)] shape=(3, 5) time=0s\n# [default_log_callback:34] - [Adder(value=10)] shape=(3, 5) time=0s\n# [default_log_callback:34] - [Adder(value=100)] shape=(3, 5) time=0s\n\nprint(pipe.transform(X))\n# [[1111. 1111. 1111. 1111. 1111.]\n#  [1111. 1111. 1111. 1111. 1111.]\n#  [1111. 1111. 1111. 1111. 1111.]]\n\n# It is possible to set the `log_callback` later then initialisation.\npipe = DebugPipeline(steps)\npipe.log_callback = \"default\"\n\n_ = pipe.fit(X, y=y)\n# [default_log_callback:34] - [Adder(value=1)] shape=(3, 5) time=0s\n# [default_log_callback:34] - [Adder(value=10)] shape=(3, 5) time=0s\n# [default_log_callback:34] - [Adder(value=100)] shape=(3, 5) time=0s\n\nprint(pipe.transform(X))\n# [[1111. 1111. 1111. 1111. 1111.]\n#  [1111. 1111. 1111. 1111. 1111.]\n#  [1111. 1111. 1111. 1111. 1111.]]\n\n# It is possible to define your own `log_callback` function.\ndef log_callback(output, execution_time, **kwargs):\n    '''My custom `log_callback` function\n\n    Parameters\n    output : tuple[np.ndarray | pd.DataFrame, estimator | transformer]\n        The output of the step and a step in the pipeline.\n    execution_time : float\n        The execution time of the step.\n\n    Note\n    The **kwargs are for arguments that are not used in this callback.\n    '''\n    logger = logging.getLogger(__name__)\n    step_result, step = output\n    logger.info(\n        f\"[{step}] shape={step_result.shape} \"\n        f\"nbytes={step_result.nbytes} time={int(execution_time)}s\")\n\npipe.log_callback = log_callback\n\n_ = pipe.fit(X, y=y)\n# [log_callback:20] - [Adder(value=1)] shape=(3, 5) nbytes=120 time=0s\n# [log_callback:20] - [Adder(value=10)] shape=(3, 5) nbytes=120 time=0s\n# [log_callback:20] - [Adder(value=100)] shape=(3, 5) nbytes=120 time=0s\n\nprint(pipe.transform(X))\n# [[1111. 1111. 1111. 1111. 1111.]\n#  [1111. 1111. 1111. 1111. 1111.]\n#  [1111. 1111. 1111. 1111. 1111.]]\n\n# Remove the `log_callback` when you want to stop logging.\npipe.log_callback = None\n\n_ = pipe.fit(X, y=y)\nprint(pipe.transform(X))\n# [[1111. 1111. 1111. 1111. 1111.]\n#  [1111. 1111. 1111. 1111. 1111.]\n#  [1111. 1111. 1111. 1111. 1111.]]\n\n# Logging also works with FeatureUnion\nfrom sklearn.pipeline import FeatureUnion\npipe_w_default_log_callback = DebugPipeline(steps, log_callback=\"default\")\npipe_w_custom_log_callback = DebugPipeline(steps, log_callback=log_callback)\n\npipe_union = DebugPipeline([\n    (\"feature_union\", FeatureUnion([\n        (\"pipe_w_default_log_callback\", pipe_w_default_log_callback),\n        (\"pipe_w_custom_log_callback\", pipe_w_custom_log_callback),\n    ])),\n    (\"final_adder\", Adder(10000))\n], log_callback=\"default\")\n\n_ = pipe_union.fit(X, y=y)   # doctest:+ELLIPSIS\n# [default_log_callback:34] - [Adder(value=1)] shape=(3, 5) time=0s\n# [default_log_callback:34] - [Adder(value=10)] shape=(3, 5) time=0s\n# [default_log_callback:34] - [Adder(value=100)] shape=(3, 5) time=0s\n# [log_callback:20] - [Adder(value=1)] shape=(3, 5) nbytes=120 time=0s\n# [log_callback:20] - [Adder(value=10)] shape=(3, 5) nbytes=120 time=0s\n# [log_callback:20] - [Adder(value=100)] shape=(3, 5) nbytes=120 time=0s\n# [default_log_callback:34] - [FeatureUnion(...)] shape=(3, 10) time=0s\n\nprint(pipe_union.transform(X))\n# [[11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111.]\n#  [11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111.]\n#  [11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111.]]\n</code></pre> Source code in <code>sklego/pipeline.py</code> <pre><code>class DebugPipeline(Pipeline):\n    \"\"\"A pipeline that has a log statement in between each step, useful for debugging purposes.\n\n    Parameters\n    ----------\n    log_callback : Callable | None, default=None\n        The callback function that logs information in between each intermediate step.\n        If set to `\"default\"`, `default_log_callback` is used.\n\n    Notes\n    -----\n    See [`sklearn.pipeline.Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)\n    for all other variables.\n\n    !!! note\n        This implementation is a hack on the original sklearn Pipeline. It aims to have the same behaviour as the\n        original sklearn Pipeline, while changing minimal amount of code.\n\n        The log statement is added by overwriting the cache method of the memory, such that the function called in the\n        cache is wrapped with a functions that calls the log callback function (`log_callback`).\n\n        This hack will break when:\n\n        - The sklearn pipeline initialization function is changed.\n        - The memory is used differently in the fit.\n        - The [`joblib.memory.Memory`](https://joblib.readthedocs.io/en/latest/generated/joblib.Memory.html)\n            changes behaviour of the `cache` method.\n        - The [`joblib.memory.Memory`](https://joblib.readthedocs.io/en/latest/generated/joblib.Memory.html)\n            starts using a `_cache` method.\n\n\n    Examples\n    --------\n    ```py\n    # Set-up for example\n    import logging\n    import sys\n\n    import numpy as np\n    import pandas as pd\n\n    from sklearn.base import BaseEstimator, TransformerMixin\n    from sklego.pipeline import DebugPipeline\n\n    logging.basicConfig(\n        format=(\"[%(funcName)s:%(lineno)d] - %(message)s\"),\n        level=logging.INFO,\n        stream=sys.stdout,\n        )\n\n    # DebugPipeline set-up\n    n_samples, n_features = 3, 5\n    X = np.zeros((n_samples, n_features))\n    y = np.arange(n_samples)\n\n    class Adder(TransformerMixin, BaseEstimator):\n        def __init__(self, value):\n            self._value = value\n\n        def fit(self, X, y=None):\n            return self\n\n        def transform(self, X):\n            return X + self._value\n\n        def __repr__(self):\n            return f\"Adder(value={self._value})\"\n\n    steps = [\n        (\"add_1\", Adder(value=1)),\n        (\"add_10\", Adder(value=10)),\n        (\"add_100\", Adder(value=100)),\n        (\"add_1000\", Adder(value=1000)),\n    ]\n\n    # The DebugPipeline behaves the sames as the Sklearn pipeline.\n    pipe = DebugPipeline(steps)\n\n    _ = pipe.fit(X, y=y)\n    print(pipe.transform(X))\n    # [[1111. 1111. 1111. 1111. 1111.]\n    #  [1111. 1111. 1111. 1111. 1111.]\n    #  [1111. 1111. 1111. 1111. 1111.]]\n\n    # But it has the option to set a `log_callback`, that logs in between each step.\n    pipe = DebugPipeline(steps, log_callback=\"default\")\n\n    _ = pipe.fit(X, y=y)\n    # [default_log_callback:34] - [Adder(value=1)] shape=(3, 5) time=0s\n    # [default_log_callback:34] - [Adder(value=10)] shape=(3, 5) time=0s\n    # [default_log_callback:34] - [Adder(value=100)] shape=(3, 5) time=0s\n\n    print(pipe.transform(X))\n    # [[1111. 1111. 1111. 1111. 1111.]\n    #  [1111. 1111. 1111. 1111. 1111.]\n    #  [1111. 1111. 1111. 1111. 1111.]]\n\n    # It is possible to set the `log_callback` later then initialisation.\n    pipe = DebugPipeline(steps)\n    pipe.log_callback = \"default\"\n\n    _ = pipe.fit(X, y=y)\n    # [default_log_callback:34] - [Adder(value=1)] shape=(3, 5) time=0s\n    # [default_log_callback:34] - [Adder(value=10)] shape=(3, 5) time=0s\n    # [default_log_callback:34] - [Adder(value=100)] shape=(3, 5) time=0s\n\n    print(pipe.transform(X))\n    # [[1111. 1111. 1111. 1111. 1111.]\n    #  [1111. 1111. 1111. 1111. 1111.]\n    #  [1111. 1111. 1111. 1111. 1111.]]\n\n    # It is possible to define your own `log_callback` function.\n    def log_callback(output, execution_time, **kwargs):\n        '''My custom `log_callback` function\n\n        Parameters\n        output : tuple[np.ndarray | pd.DataFrame, estimator | transformer]\n            The output of the step and a step in the pipeline.\n        execution_time : float\n            The execution time of the step.\n\n        Note\n        The **kwargs are for arguments that are not used in this callback.\n        '''\n        logger = logging.getLogger(__name__)\n        step_result, step = output\n        logger.info(\n            f\"[{step}] shape={step_result.shape} \"\n            f\"nbytes={step_result.nbytes} time={int(execution_time)}s\")\n\n    pipe.log_callback = log_callback\n\n    _ = pipe.fit(X, y=y)\n    # [log_callback:20] - [Adder(value=1)] shape=(3, 5) nbytes=120 time=0s\n    # [log_callback:20] - [Adder(value=10)] shape=(3, 5) nbytes=120 time=0s\n    # [log_callback:20] - [Adder(value=100)] shape=(3, 5) nbytes=120 time=0s\n\n    print(pipe.transform(X))\n    # [[1111. 1111. 1111. 1111. 1111.]\n    #  [1111. 1111. 1111. 1111. 1111.]\n    #  [1111. 1111. 1111. 1111. 1111.]]\n\n    # Remove the `log_callback` when you want to stop logging.\n    pipe.log_callback = None\n\n    _ = pipe.fit(X, y=y)\n    print(pipe.transform(X))\n    # [[1111. 1111. 1111. 1111. 1111.]\n    #  [1111. 1111. 1111. 1111. 1111.]\n    #  [1111. 1111. 1111. 1111. 1111.]]\n\n    # Logging also works with FeatureUnion\n    from sklearn.pipeline import FeatureUnion\n    pipe_w_default_log_callback = DebugPipeline(steps, log_callback=\"default\")\n    pipe_w_custom_log_callback = DebugPipeline(steps, log_callback=log_callback)\n\n    pipe_union = DebugPipeline([\n        (\"feature_union\", FeatureUnion([\n            (\"pipe_w_default_log_callback\", pipe_w_default_log_callback),\n            (\"pipe_w_custom_log_callback\", pipe_w_custom_log_callback),\n        ])),\n        (\"final_adder\", Adder(10000))\n    ], log_callback=\"default\")\n\n    _ = pipe_union.fit(X, y=y)   # doctest:+ELLIPSIS\n    # [default_log_callback:34] - [Adder(value=1)] shape=(3, 5) time=0s\n    # [default_log_callback:34] - [Adder(value=10)] shape=(3, 5) time=0s\n    # [default_log_callback:34] - [Adder(value=100)] shape=(3, 5) time=0s\n    # [log_callback:20] - [Adder(value=1)] shape=(3, 5) nbytes=120 time=0s\n    # [log_callback:20] - [Adder(value=10)] shape=(3, 5) nbytes=120 time=0s\n    # [log_callback:20] - [Adder(value=100)] shape=(3, 5) nbytes=120 time=0s\n    # [default_log_callback:34] - [FeatureUnion(...)] shape=(3, 10) time=0s\n\n    print(pipe_union.transform(X))\n    # [[11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111.]\n    #  [11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111.]\n    #  [11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111. 11111.]]\n    ```\n    \"\"\"\n\n    def __init__(self, steps, memory=None, verbose=False, *, log_callback=None):\n        self.log_callback = log_callback\n        super().__init__(steps=steps, memory=memory, verbose=verbose)\n\n    @property\n    def memory(self):\n        # When no log callback function is given, change nothing.\n        # Or, if the memory cache was changed, set it back to its original.\n        if self._log_callback is None:\n            if hasattr(self._memory, \"_cache\"):\n                self._memory.cache = self._memory._cache\n            return self._memory\n\n        self._memory = check_memory(self._memory)\n\n        # Overwrite cache function of memory such that it logs the\n        # output when the function is called\n        if not hasattr(self._memory, \"_cache\"):\n            self._memory._cache = self._memory.cache\n        self._memory.cache = _cache_with_function_log_statement(self._log_callback).__get__(\n            self._memory, self._memory.__class__\n        )\n        return self._memory\n\n    @memory.setter\n    def memory(self, memory):\n        self._memory = memory\n\n    @property\n    def log_callback(self):\n        return self._log_callback\n\n    @log_callback.setter\n    def log_callback(self, func):\n        self._log_callback = func\n        if self._log_callback == \"default\":\n            self._log_callback = default_log_callback\n</code></pre>"},{"location":"api/pipeline/#sklego.pipeline.make_debug_pipeline","title":"<code>sklego.pipeline.make_debug_pipeline(*steps, **kwargs)</code>","text":"<p>Construct a <code>DebugPipeline</code> from the given estimators.</p> <p>This is a shorthand for the <code>DebugPipeline</code> constructor; it does not require, and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically.</p> <p>Parameters:</p> Name Type Description Default <code>*steps</code> <code>list</code> <p>List of estimators to be included in the pipeline.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to the <code>DebugPipeline</code> constructor. Possible arguments are <code>memory</code>, <code>verbose</code> and <code>log_callback</code>:</p> <ul> <li> <p><code>memory</code> : str | object with the joblib.Memory interface, default=None</p> <p>Used to cache the fitted transformers of the pipeline. The last step will never be cached, even if it is a transformer. By default, no caching is performed. If a string is given, it is the path to the caching directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. Use the attribute <code>named_steps</code> or <code>steps</code> to inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time consuming.</p> </li> <li> <p><code>verbose</code> : bool, default=False</p> <p>If True, the time elapsed while fitting each step will be printed as it is completed.</p> </li> <li> <p><code>log_callback</code> : str | Callable | None, default=None.</p> <p>The callback function that logs information in between each intermediate step. If set to <code>\"default\"</code>, <code>default_log_callback</code> is used.</p> </li> </ul> <code>{}</code> <p>Returns:</p> Type Description <code>DebugPipeline</code> <p>Instance with given steps, <code>memory</code>, <code>verbose</code> and <code>log_callback</code>.</p> <p>Examples:</p> <pre><code>from sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\n\nmake_debug_pipeline(StandardScaler(), GaussianNB(priors=None))\n# DebugPipeline(steps=[(\"standardscaler\", StandardScaler()),\n#                 (\"gaussiannb\", GaussianNB())])\n</code></pre>"},{"location":"api/pipeline/#sklego.pipeline.make_debug_pipeline--see-also","title":"See Also","text":"<p>sklego.pipeline.DebugPipeline : Class for creating a pipeline of transforms with a final estimator.</p> Source code in <code>sklego/pipeline.py</code> <pre><code>def make_debug_pipeline(*steps, **kwargs):\n    \"\"\"Construct a `DebugPipeline` from the given estimators.\n\n    This is a shorthand for the `DebugPipeline` constructor; it does not require, and does not permit, naming the\n    estimators. Instead, their names will be set to the lowercase of their types automatically.\n\n    Parameters\n    ----------\n    *steps : list\n        List of estimators to be included in the pipeline.\n    **kwargs : dict\n        Additional keyword arguments passed to the `DebugPipeline` constructor.\n        Possible arguments are `memory`, `verbose` and `log_callback`:\n\n        - `memory` : str | object with the joblib.Memory interface, default=None\n\n            Used to cache the fitted transformers of the pipeline. The last step will never be cached, even if it is a\n            transformer. By default, no caching is performed. If a string is given, it is the path to the caching\n            directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer\n            instance given to the pipeline cannot be inspected directly. Use the attribute `named_steps` or `steps` to\n            inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time\n            consuming.\n\n        - `verbose` : bool, default=False\n\n            If True, the time elapsed while fitting each step will be printed as it is completed.\n\n        - `log_callback` : str | Callable | None, default=None.\n\n            The callback function that logs information in between each intermediate step. If set to `\"default\"`,\n            `default_log_callback` is used.\n\n    Returns\n    -------\n    DebugPipeline\n        Instance with given steps, `memory`, `verbose` and `log_callback`.\n\n    Examples\n    --------\n    ```py\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.preprocessing import StandardScaler\n\n    make_debug_pipeline(StandardScaler(), GaussianNB(priors=None))\n    # DebugPipeline(steps=[(\"standardscaler\", StandardScaler()),\n    #                 (\"gaussiannb\", GaussianNB())])\n    ```\n\n    See Also\n    --------\n    sklego.pipeline.DebugPipeline : Class for creating a pipeline of transforms with a final estimator.\n    \"\"\"\n    memory = kwargs.pop(\"memory\", None)\n    verbose = kwargs.pop(\"verbose\", False)\n    log_callback = kwargs.pop(\"log_callback\", None)\n    if kwargs:\n        raise TypeError('Unknown keyword arguments: \"{}\"'.format(list(kwargs.keys())[0]))\n    return DebugPipeline(\n        _name_estimators(steps),\n        memory=memory,\n        verbose=verbose,\n        log_callback=log_callback,\n    )\n</code></pre>"},{"location":"api/pipeline/#sklego.pipeline.default_log_callback","title":"<code>sklego.pipeline.default_log_callback(output, execution_time, **kwargs)</code>","text":"<p>The default log callback which logs the step name, shape of the output and the execution time of the step.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>tuple[ndarray | DataFrame, estimator | transformer]</code> <p>The output of the step and a step in the pipeline.</p> required <code>execution_time</code> <code>float</code> <p>The execution time of the step.</p> required <p>Info</p> <p>If you write your custom callback function the input is:</p> Parameter Type Description <code>func</code> Callable[..., T] The function to be wrapped <code>input_args</code> tuple The input arguments <code>input_kwargs</code> dict The input key-word arguments <code>output</code> T The output of the function <code>execution_time</code> float The execution time of the step Source code in <code>sklego/pipeline.py</code> <pre><code>def default_log_callback(output, execution_time, **kwargs):\n    \"\"\"The default log callback which logs the step name, shape of the output and the execution time of the step.\n\n    Parameters\n    ----------\n    output : tuple[np.ndarray | pd.DataFrame, estimator | transformer]\n        The output of the step and a step in the pipeline.\n    execution_time : float\n        The execution time of the step.\n\n    !!! info\n\n        If you write your custom callback function the input is:\n\n        | Parameter        | Type             | Description                    |\n        | ---------------- | ---------------- | ------------------------------ |\n        | `func`           | Callable[..., T] | The function to be wrapped     |\n        | `input_args`     | tuple            | The input arguments            |\n        | `input_kwargs`   | dict             | The input key-word arguments   |\n        | `output`         | T                | The output of the function     |\n        | `execution_time` | float            | The execution time of the step |\n\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    step_result, step = output\n    logger.info(f\"[{step}] shape={step_result.shape} \" f\"time={int(execution_time)}s\")\n</code></pre>"},{"location":"api/preprocessing/","title":"Preprocessing","text":""},{"location":"api/preprocessing/#sklego.preprocessing.columncapper.ColumnCapper","title":"<code>sklego.preprocessing.columncapper.ColumnCapper</code>","text":"<p>             Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>The <code>ColumnCapper</code> transformer caps the values of columns according to the given quantile thresholds.</p> <p>The capping is performed independently for each column of the input data. The quantile thresholds are computed during the fitting phase. The capping is performed during the transformation phase.</p> <p>Parameters:</p> Name Type Description Default <code>quantile_range</code> <code>Tuple[float, float] | List[float]</code> <p>The quantile ranges to perform the capping. Their values must be in the interval [0; 100].</p> <code>(5.0, 95.0)</code> <code>interpolation</code> <code>Literal[linear, lower, higher, midpoint, nearest]</code> <p>The interpolation method to compute the quantiles when the desired quantile lies between two data points <code>i</code> and <code>j</code>. This value is passed to <code>numpy.nanquantile</code> function.</p> <p>The Available values are:</p> <ul> <li><code>\"linear\"</code>: <code>i + (j - i) * fraction</code>, where <code>fraction</code> is the fractional part of the index surrounded by <code>i</code>     and <code>j</code>.</li> <li><code>\"lower\"</code>: <code>i</code>.</li> <li><code>\"higher\"</code>: <code>j</code>.</li> <li><code>\"nearest\"</code>: <code>i</code> or <code>j</code> whichever is nearest.</li> <li><code>\"midpoint\"</code>: (<code>i</code> + <code>j</code>) / 2.</li> </ul> <code>\"linear\"</code> <code>discard_infs</code> <code>bool</code> <p>Whether to discard <code>-np.inf</code> and <code>np.inf</code> values or not. If False, such values will be capped. If True, they will be replaced by <code>np.nan</code>.</p> <p>Info</p> <p>Setting <code>discard_infs=True</code> is important if the <code>inf</code> values are results of divisions by 0, which are interpreted by <code>pandas</code> as <code>-np.inf</code> or <code>np.inf</code> depending on the sign of the numerator.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>If False, try to avoid a copy and do inplace capping instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>quantiles_</code> <code>np.ndarray of shape (2, n_features)</code> <p>The computed quantiles for each column of the input data. The first row contains the lower quantile, the second row contains the upper quantile.</p> <code>n_features_in_</code> <code>int</code> <p>Number of features seen during <code>fit</code>.</p> <code>n_columns_</code> <code>int</code> <p>Deprecated, please use <code>n_features_in_</code> instead.</p> <p>Examples:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklego.preprocessing import ColumnCapper\n\ndf = pd.DataFrame({'a':[2, 4.5, 7, 9], 'b':[11, 12, np.inf, 14]})\ndf\n'''\n     a     b\n0  2.0  11.0\n1  4.5  12.0\n2  7.0   inf\n3  9.0  14.0\n'''\n\ncapper = ColumnCapper()\ncapper.fit_transform(df)\n'''\narray([[ 2.375, 11.1  ],\n       [ 4.5  , 12.   ],\n       [ 7.   , 13.8  ],\n       [ 8.7  , 13.8  ]])\n'''\n\ncapper = ColumnCapper(discard_infs=True) # Discarding infs\ndf[['a', 'b']] = capper.fit_transform(df)\ndf\n'''\n       a     b\n0  2.375  11.1\n1  4.500  12.0\n2  7.000   NaN\n3  8.700  13.8\n'''\n</code></pre> Source code in <code>sklego/preprocessing/columncapper.py</code> <pre><code>class ColumnCapper(TransformerMixin, BaseEstimator):\n    \"\"\"The `ColumnCapper` transformer caps the values of columns according to the given quantile thresholds.\n\n    The capping is performed independently for each column of the input data. The quantile thresholds are computed\n    during the fitting phase. The capping is performed during the transformation phase.\n\n    Parameters\n    ----------\n    quantile_range : Tuple[float, float] | List[float], default=(5.0, 95.0)\n        The quantile ranges to perform the capping. Their values must be in the interval [0; 100].\n    interpolation : Literal[\"linear\", \"lower\", \"higher\", \"midpoint\", \"nearest\"], default=\"linear\"\n        The interpolation method to compute the quantiles when the desired quantile lies between two data points `i`\n        and `j`. This value is passed to\n        [`numpy.nanquantile`](https://numpy.org/doc/stable/reference/generated/numpy.nanquantile.html) function.\n\n        The Available values are:\n\n        - `\"linear\"`: `i + (j - i) * fraction`, where `fraction` is the fractional part of the index surrounded by `i`\n            and `j`.\n        * `\"lower\"`: `i`.\n        * `\"higher\"`: `j`.\n        * `\"nearest\"`: `i` or `j` whichever is nearest.\n        * `\"midpoint\"`: (`i` + `j`) / 2.\n    discard_infs : bool, default=False\n        Whether to discard `-np.inf` and `np.inf` values or not. If False, such values will be capped. If True,\n        they will be replaced by `np.nan`.\n\n        !!! info\n            Setting `discard_infs=True` is important if the `inf` values are results of divisions by 0, which are\n            interpreted by `pandas` as `-np.inf` or `np.inf` depending on the sign of the numerator.\n    copy : bool, default=True\n        If False, try to avoid a copy and do inplace capping instead. This is not guaranteed to always work inplace;\n        e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.\n\n    Attributes\n    ----------\n    quantiles_ : np.ndarray of shape (2, n_features)\n        The computed quantiles for each column of the input data. The first row contains the lower quantile, the second\n        row contains the upper quantile.\n    n_features_in_ : int\n        Number of features seen during `fit`.\n    n_columns_ : int\n        Deprecated, please use `n_features_in_` instead.\n\n    Examples\n    --------\n    ```py\n    import pandas as pd\n    import numpy as np\n    from sklego.preprocessing import ColumnCapper\n\n    df = pd.DataFrame({'a':[2, 4.5, 7, 9], 'b':[11, 12, np.inf, 14]})\n    df\n    '''\n         a     b\n    0  2.0  11.0\n    1  4.5  12.0\n    2  7.0   inf\n    3  9.0  14.0\n    '''\n\n    capper = ColumnCapper()\n    capper.fit_transform(df)\n    '''\n    array([[ 2.375, 11.1  ],\n           [ 4.5  , 12.   ],\n           [ 7.   , 13.8  ],\n           [ 8.7  , 13.8  ]])\n    '''\n\n    capper = ColumnCapper(discard_infs=True) # Discarding infs\n    df[['a', 'b']] = capper.fit_transform(df)\n    df\n    '''\n           a     b\n    0  2.375  11.1\n    1  4.500  12.0\n    2  7.000   NaN\n    3  8.700  13.8\n    '''\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        quantile_range=(5.0, 95.0),\n        interpolation=\"linear\",\n        discard_infs=False,\n        copy=True,\n    ):\n        self._check_quantile_range(quantile_range)\n        self._check_interpolation(interpolation)\n\n        self.quantile_range = quantile_range\n        self.interpolation = interpolation\n        self.discard_infs = discard_infs\n        self.copy = copy\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the `ColumnCapper` transformer by computing quantiles for each column of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data used to compute the quantiles for capping.\n        y : array-like of shape (n_samples,), default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : ColumnCapper\n            The fitted transformer.\n\n        Raises\n        ------\n        ValueError\n            If `X` contains non-numeric columns.\n        \"\"\"\n        X = check_array(X, copy=True, force_all_finite=False, dtype=FLOAT_DTYPES, estimator=self)\n\n        # If X contains infs, we need to replace them by nans before computing quantiles\n        np.putmask(X, (X == np.inf) | (X == -np.inf), np.nan)\n\n        # There should be no column containing only nan cells at this point. If that's not the case,\n        # it means that the user asked ColumnCapper to fit some column containing only nan or inf cells.\n        nans_mask = np.isnan(X)\n        invalid_columns_mask = nans_mask.sum(axis=0) == X.shape[0]  # Contains as many nans as rows\n        if invalid_columns_mask.any():\n            raise ValueError(\"ColumnCapper cannot fit columns containing only inf/nan values\")\n\n        q = [quantile_limit / 100 for quantile_limit in self.quantile_range]\n        self.quantiles_ = np.nanquantile(a=X, q=q, axis=0, overwrite_input=True, method=self.interpolation)\n\n        # Saving the number of columns to ensure coherence between fit and transform inputs\n        self.n_features_in_ = X.shape[1]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Performs the capping on the column(s) of `X` according to the quantile thresholds computed during fitting.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data for which the capping limit(s) will be applied.\n\n        Returns\n        -------\n        X : np.ndarray of shape (n_samples, n_features)\n            `X` values with capped limits.\n\n        Raises\n        ------\n        ValueError\n            If the number of columns from `X` differs from the number of columns when fitting.\n        \"\"\"\n        check_is_fitted(self, \"quantiles_\")\n        X = check_array(\n            X,\n            copy=self.copy,\n            force_all_finite=False,\n            dtype=FLOAT_DTYPES,\n            estimator=self,\n        )\n\n        if X.shape[1] != self.n_features_in_:\n            raise ValueError(\"X must have the same number of columns in fit and transform\")\n\n        if self.discard_infs:\n            np.putmask(X, (X == np.inf) | (X == -np.inf), np.nan)\n\n        # Actually capping\n        X = np.minimum(X, self.quantiles_[1, :])\n        X = np.maximum(X, self.quantiles_[0, :])\n\n        return X\n\n    @staticmethod\n    def _check_quantile_range(quantile_range):\n        \"\"\"Checks for the validity of quantile_range.\n\n        Parameters\n        ----------\n        quantile_range : Tuple[float, float] | List[float]\n            The quantile ranges to perform the capping. Their values must be in the interval [0; 100].\n\n        Raises\n        ------\n        TypeError\n            If `quantile_range` is not a tuple or a list.\n        ValueError\n            - If `quantile_range` does not contain exactly 2 elements.\n            - If `quantile_range` contains values outside of [0; 100].\n            - If `quantile_range` contains values in the wrong order.\n        \"\"\"\n        if not isinstance(quantile_range, tuple) and not isinstance(quantile_range, list):\n            raise TypeError(\"quantile_range must be a tuple or a list\")\n        if len(quantile_range) != 2:\n            raise ValueError(\"quantile_range must contain 2 elements: min_quantile and max_quantile\")\n\n        min_quantile, max_quantile = quantile_range\n\n        for quantile in min_quantile, max_quantile:\n            if not isinstance(quantile, float) and not isinstance(quantile, int):\n                raise TypeError(\"min_quantile and max_quantile must be numbers\")\n            if quantile &lt; 0 or 100 &lt; quantile:\n                raise ValueError(\"min_quantile and max_quantile must be in [0; 100]\")\n\n        if min_quantile &gt; max_quantile:\n            raise ValueError(\"min_quantile must be less than or equal to max_quantile\")\n\n    @staticmethod\n    def _check_interpolation(interpolation):\n        \"\"\"Checks for the validity of interpolation.\n\n        Parameters\n        ----------\n        interpolation : Literal[\"linear\", \"lower\", \"higher\", \"midpoint\", \"nearest\"]\n            Interpolation method to compute the quantiles\n\n        Raises\n        ------\n        ValueError\n            If `interpolation` is not one of the allowed values.\n        \"\"\"\n        allowed_interpolations = (\"linear\", \"lower\", \"higher\", \"midpoint\", \"nearest\")\n        if interpolation not in allowed_interpolations:\n            raise ValueError(\"Available interpolation methods: {}\".format(\", \".join(allowed_interpolations)))\n\n    @property\n    def n_columns_(self):\n        warn(\n            \"Please use `n_features_in_` instead of `n_columns_`, `n_columns_` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self.n_features_in_\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.columncapper.ColumnCapper.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the <code>ColumnCapper</code> transformer by computing quantiles for each column of <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data used to compute the quantiles for capping.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>ColumnCapper</code> <p>The fitted transformer.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>X</code> contains non-numeric columns.</p> Source code in <code>sklego/preprocessing/columncapper.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the `ColumnCapper` transformer by computing quantiles for each column of `X`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data used to compute the quantiles for capping.\n    y : array-like of shape (n_samples,), default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : ColumnCapper\n        The fitted transformer.\n\n    Raises\n    ------\n    ValueError\n        If `X` contains non-numeric columns.\n    \"\"\"\n    X = check_array(X, copy=True, force_all_finite=False, dtype=FLOAT_DTYPES, estimator=self)\n\n    # If X contains infs, we need to replace them by nans before computing quantiles\n    np.putmask(X, (X == np.inf) | (X == -np.inf), np.nan)\n\n    # There should be no column containing only nan cells at this point. If that's not the case,\n    # it means that the user asked ColumnCapper to fit some column containing only nan or inf cells.\n    nans_mask = np.isnan(X)\n    invalid_columns_mask = nans_mask.sum(axis=0) == X.shape[0]  # Contains as many nans as rows\n    if invalid_columns_mask.any():\n        raise ValueError(\"ColumnCapper cannot fit columns containing only inf/nan values\")\n\n    q = [quantile_limit / 100 for quantile_limit in self.quantile_range]\n    self.quantiles_ = np.nanquantile(a=X, q=q, axis=0, overwrite_input=True, method=self.interpolation)\n\n    # Saving the number of columns to ensure coherence between fit and transform inputs\n    self.n_features_in_ = X.shape[1]\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.columncapper.ColumnCapper.transform","title":"<code>transform(X)</code>","text":"<p>Performs the capping on the column(s) of <code>X</code> according to the quantile thresholds computed during fitting.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data for which the capping limit(s) will be applied.</p> required <p>Returns:</p> Name Type Description <code>X</code> <code>np.ndarray of shape (n_samples, n_features)</code> <p><code>X</code> values with capped limits.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of columns from <code>X</code> differs from the number of columns when fitting.</p> Source code in <code>sklego/preprocessing/columncapper.py</code> <pre><code>def transform(self, X):\n    \"\"\"Performs the capping on the column(s) of `X` according to the quantile thresholds computed during fitting.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data for which the capping limit(s) will be applied.\n\n    Returns\n    -------\n    X : np.ndarray of shape (n_samples, n_features)\n        `X` values with capped limits.\n\n    Raises\n    ------\n    ValueError\n        If the number of columns from `X` differs from the number of columns when fitting.\n    \"\"\"\n    check_is_fitted(self, \"quantiles_\")\n    X = check_array(\n        X,\n        copy=self.copy,\n        force_all_finite=False,\n        dtype=FLOAT_DTYPES,\n        estimator=self,\n    )\n\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(\"X must have the same number of columns in fit and transform\")\n\n    if self.discard_infs:\n        np.putmask(X, (X == np.inf) | (X == -np.inf), np.nan)\n\n    # Actually capping\n    X = np.minimum(X, self.quantiles_[1, :])\n    X = np.maximum(X, self.quantiles_[0, :])\n\n    return X\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.pandastransformers.ColumnDropper","title":"<code>sklego.preprocessing.pandastransformers.ColumnDropper</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>The <code>ColumnDropper</code> transformer allows dropping specific columns from a pandas DataFrame by name. Can be useful in a sklearn Pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>str | list[str]</code> <p>Column name(s) to be selected.</p> required <p>Attributes:</p> Name Type Description <code>feature_names_</code> <code>list[str]</code> <p>The names of the features to keep during transform.</p> <p>Examples:</p> <pre><code># Selecting a single column from a pandas DataFrame\nimport pandas as pd\nfrom sklego.preprocessing import ColumnDropper\n\ndf = pd.DataFrame({\n    \"name\": [\"Swen\", \"Victor\", \"Alex\"],\n    \"length\": [1.82, 1.85, 1.80],\n    \"shoesize\": [42, 44, 45]\n})\nColumnDropper([\"name\"]).fit_transform(df)\n'''\n   length  shoesize\n0    1.82        42\n1    1.85        44\n2    1.80        45\n'''\n\n# Selecting multiple columns from a pandas DataFrame\nColumnDropper([\"length\", \"shoesize\"]).fit_transform(df)\n'''\n     name\n0    Swen\n1  Victor\n2    Alex\n'''\n\n# Selecting non-existent columns returns in a KeyError\nColumnDropper([\"weight\"]).fit_transform(df)\n# Traceback (most recent call last):\n#     ...\n# KeyError: \"['weight'] column(s) not in DataFrame\"\n\n# How to use the ColumnSelector in a sklearn Pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\npipe = Pipeline([\n    (\"select\", ColumnDropper([\"name\", \"shoesize\"])),\n    (\"scale\", StandardScaler()),\n])\npipe.fit_transform(df)\n# array([[-0.16222142],\n#        [ 1.29777137],\n#        [-1.13554995]])\n</code></pre> <p>Warning</p> <ul> <li>Raises a <code>TypeError</code> if input provided is not a DataFrame.</li> <li>Raises a <code>ValueError</code> if columns provided are not in the input DataFrame.</li> </ul> Source code in <code>sklego/preprocessing/pandastransformers.py</code> <pre><code>class ColumnDropper(BaseEstimator, TransformerMixin):\n    \"\"\"The `ColumnDropper` transformer allows dropping specific columns from a pandas DataFrame by name.\n    Can be useful in a sklearn Pipeline.\n\n    Parameters\n    ----------\n    columns : str | list[str]\n        Column name(s) to be selected.\n\n    Attributes\n    ----------\n    feature_names_ : list[str]\n        The names of the features to keep during transform.\n\n    Examples\n    --------\n    ```py\n    # Selecting a single column from a pandas DataFrame\n    import pandas as pd\n    from sklego.preprocessing import ColumnDropper\n\n    df = pd.DataFrame({\n        \"name\": [\"Swen\", \"Victor\", \"Alex\"],\n        \"length\": [1.82, 1.85, 1.80],\n        \"shoesize\": [42, 44, 45]\n    })\n    ColumnDropper([\"name\"]).fit_transform(df)\n    '''\n       length  shoesize\n    0    1.82        42\n    1    1.85        44\n    2    1.80        45\n    '''\n\n    # Selecting multiple columns from a pandas DataFrame\n    ColumnDropper([\"length\", \"shoesize\"]).fit_transform(df)\n    '''\n         name\n    0    Swen\n    1  Victor\n    2    Alex\n    '''\n\n    # Selecting non-existent columns returns in a KeyError\n    ColumnDropper([\"weight\"]).fit_transform(df)\n    # Traceback (most recent call last):\n    #     ...\n    # KeyError: \"['weight'] column(s) not in DataFrame\"\n\n    # How to use the ColumnSelector in a sklearn Pipeline\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n    pipe = Pipeline([\n        (\"select\", ColumnDropper([\"name\", \"shoesize\"])),\n        (\"scale\", StandardScaler()),\n    ])\n    pipe.fit_transform(df)\n    # array([[-0.16222142],\n    #        [ 1.29777137],\n    #        [-1.13554995]])\n    ```\n\n    !!! warning\n\n        - Raises a `TypeError` if input provided is not a DataFrame.\n        - Raises a `ValueError` if columns provided are not in the input DataFrame.\n    \"\"\"\n\n    def __init__(self, columns: list):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the transformer by storing the column names to keep during `.transform()` step.\n\n        Checks:\n\n        1. If input is a `pd.DataFrame` object\n        2. If column names are in such DataFrame\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            The data on which we apply the column selection.\n        y : pd.Series, default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : ColumnDropper\n            The fitted transformer.\n\n        Raises\n        ------\n        TypeError\n            If `X` is not a `pd.DataFrame` object.\n        KeyError\n            If one or more of the columns provided doesn't exist in the input DataFrame.\n        ValueError\n            If dropping the specified columns would result in an empty output DataFrame.\n        \"\"\"\n        self.columns_ = as_list(self.columns)\n        self._check_X_for_type(X)\n        self._check_column_names(X)\n        self.feature_names_ = X.columns.drop(self.columns_).tolist()\n        self._check_column_length()\n        return self\n\n    def transform(self, X):\n        \"\"\"Returns a pandas DataFrame with only the specified columns.\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            The data on which we apply the column selection.\n\n        Returns\n        -------\n        pd.DataFrame\n            The data with the specified columns dropped.\n\n        Raises\n        ------\n        TypeError\n            If `X` is not a `pd.DataFrame` object.\n        \"\"\"\n        check_is_fitted(self, [\"feature_names_\"])\n        self._check_X_for_type(X)\n        if self.columns_:\n            return X.drop(columns=self.columns_)\n        return X\n\n    def get_feature_names(self):\n        \"\"\"Alias for `.feature_names_` attribute\"\"\"\n        return self.feature_names_\n\n    def _check_column_length(self):\n        \"\"\"Check if all columns are dropped\"\"\"\n        if len(self.feature_names_) == 0:\n            raise ValueError(f\"Dropping {self.columns_} would result in an empty output DataFrame\")\n\n    def _check_column_names(self, X):\n        \"\"\"Check if one or more of the columns provided doesn't exist in the input DataFrame\"\"\"\n        non_existent_columns = set(self.columns_).difference(X.columns)\n        if len(non_existent_columns) &gt; 0:\n            raise KeyError(f\"{list(non_existent_columns)} column(s) not in DataFrame\")\n\n    @staticmethod\n    def _check_X_for_type(X):\n        \"\"\"Checks if input of the Selector is of the required dtype\"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise TypeError(\"Provided variable X is not of type pandas.DataFrame\")\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.pandastransformers.ColumnDropper.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the transformer by storing the column names to keep during <code>.transform()</code> step.</p> <p>Checks:</p> <ol> <li>If input is a <code>pd.DataFrame</code> object</li> <li>If column names are in such DataFrame</li> </ol> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The data on which we apply the column selection.</p> required <code>y</code> <code>Series</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>ColumnDropper</code> <p>The fitted transformer.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>X</code> is not a <code>pd.DataFrame</code> object.</p> <code>KeyError</code> <p>If one or more of the columns provided doesn't exist in the input DataFrame.</p> <code>ValueError</code> <p>If dropping the specified columns would result in an empty output DataFrame.</p> Source code in <code>sklego/preprocessing/pandastransformers.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the transformer by storing the column names to keep during `.transform()` step.\n\n    Checks:\n\n    1. If input is a `pd.DataFrame` object\n    2. If column names are in such DataFrame\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n        The data on which we apply the column selection.\n    y : pd.Series, default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : ColumnDropper\n        The fitted transformer.\n\n    Raises\n    ------\n    TypeError\n        If `X` is not a `pd.DataFrame` object.\n    KeyError\n        If one or more of the columns provided doesn't exist in the input DataFrame.\n    ValueError\n        If dropping the specified columns would result in an empty output DataFrame.\n    \"\"\"\n    self.columns_ = as_list(self.columns)\n    self._check_X_for_type(X)\n    self._check_column_names(X)\n    self.feature_names_ = X.columns.drop(self.columns_).tolist()\n    self._check_column_length()\n    return self\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.pandastransformers.ColumnDropper.get_feature_names","title":"<code>get_feature_names()</code>","text":"<p>Alias for <code>.feature_names_</code> attribute</p> Source code in <code>sklego/preprocessing/pandastransformers.py</code> <pre><code>def get_feature_names(self):\n    \"\"\"Alias for `.feature_names_` attribute\"\"\"\n    return self.feature_names_\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.pandastransformers.ColumnDropper.transform","title":"<code>transform(X)</code>","text":"<p>Returns a pandas DataFrame with only the specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The data on which we apply the column selection.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The data with the specified columns dropped.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>X</code> is not a <code>pd.DataFrame</code> object.</p> Source code in <code>sklego/preprocessing/pandastransformers.py</code> <pre><code>def transform(self, X):\n    \"\"\"Returns a pandas DataFrame with only the specified columns.\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n        The data on which we apply the column selection.\n\n    Returns\n    -------\n    pd.DataFrame\n        The data with the specified columns dropped.\n\n    Raises\n    ------\n    TypeError\n        If `X` is not a `pd.DataFrame` object.\n    \"\"\"\n    check_is_fitted(self, [\"feature_names_\"])\n    self._check_X_for_type(X)\n    if self.columns_:\n        return X.drop(columns=self.columns_)\n    return X\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.pandastransformers.ColumnSelector","title":"<code>sklego.preprocessing.pandastransformers.ColumnSelector</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>The <code>ColumnSelector</code> transformer allows selecting specific columns from a pandas DataFrame by name. Can be useful in a sklearn Pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>str | list[str]</code> <p>Column name(s) to be selected.</p> required <p>Attributes:</p> Name Type Description <code>columns_</code> <code>list[str]</code> <p>The names of the features to keep during transform.</p> <p>Examples:</p> <pre><code># Selecting a single column from a pandas DataFrame\nimport pandas as pd\nfrom sklego.preprocessing import ColumnSelector\n\ndf = pd.DataFrame({\n    \"name\": [\"Swen\", \"Victor\", \"Alex\"],\n    \"length\": [1.82, 1.85, 1.80],\n    \"shoesize\": [42, 44, 45]\n})\nColumnSelector([\"length\"]).fit_transform(df)\n'''\n    length\n0    1.82\n1    1.85\n2    1.80\n'''\n\n# Selecting multiple columns from a pandas DataFrame\nColumnSelector([\"length\", \"shoesize\"]).fit_transform(df)\n'''\n   length  shoesize\n0    1.82        42\n1    1.85        44\n2    1.80        45\n'''\n\n# Selecting non-existent columns returns in a KeyError\nColumnSelector([\"weight\"]).fit_transform(df)\n# Traceback (most recent call last):\n#     ...\n# KeyError: \"['weight'] column(s) not in DataFrame\"\n\n# How to use the ColumnSelector in a sklearn Pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\npipe = Pipeline([\n    (\"select\", ColumnSelector([\"length\"])),\n    (\"scale\", StandardScaler()),\n])\npipe.fit_transform(df)\n# array([[-0.16222142],\n#        [ 1.29777137],\n#        [-1.13554995]])\n</code></pre> <p>Warning</p> <p>Raises a <code>TypeError</code> if input provided is not a DataFrame.</p> <p>Raises a <code>ValueError</code> if columns provided are not in the input DataFrame.</p> Source code in <code>sklego/preprocessing/pandastransformers.py</code> <pre><code>class ColumnSelector(BaseEstimator, TransformerMixin):\n    \"\"\"The `ColumnSelector` transformer allows selecting specific columns from a pandas DataFrame by name.\n    Can be useful in a sklearn Pipeline.\n\n    Parameters\n    ----------\n    columns : str | list[str]\n        Column name(s) to be selected.\n\n    Attributes\n    ----------\n    columns_ : list[str]\n        The names of the features to keep during transform.\n\n    Examples\n    --------\n    ```py\n    # Selecting a single column from a pandas DataFrame\n    import pandas as pd\n    from sklego.preprocessing import ColumnSelector\n\n    df = pd.DataFrame({\n        \"name\": [\"Swen\", \"Victor\", \"Alex\"],\n        \"length\": [1.82, 1.85, 1.80],\n        \"shoesize\": [42, 44, 45]\n    })\n    ColumnSelector([\"length\"]).fit_transform(df)\n    '''\n        length\n    0    1.82\n    1    1.85\n    2    1.80\n    '''\n\n    # Selecting multiple columns from a pandas DataFrame\n    ColumnSelector([\"length\", \"shoesize\"]).fit_transform(df)\n    '''\n       length  shoesize\n    0    1.82        42\n    1    1.85        44\n    2    1.80        45\n    '''\n\n    # Selecting non-existent columns returns in a KeyError\n    ColumnSelector([\"weight\"]).fit_transform(df)\n    # Traceback (most recent call last):\n    #     ...\n    # KeyError: \"['weight'] column(s) not in DataFrame\"\n\n    # How to use the ColumnSelector in a sklearn Pipeline\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n    pipe = Pipeline([\n        (\"select\", ColumnSelector([\"length\"])),\n        (\"scale\", StandardScaler()),\n    ])\n    pipe.fit_transform(df)\n    # array([[-0.16222142],\n    #        [ 1.29777137],\n    #        [-1.13554995]])\n    ```\n\n    !!! warning\n\n        Raises a `TypeError` if input provided is not a DataFrame.\n\n        Raises a `ValueError` if columns provided are not in the input DataFrame.\n    \"\"\"\n\n    def __init__(self, columns: list):\n        # if the columns parameter is not a list, make it into a list\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the transformer by storing the column names to keep during transform.\n\n        Checks:\n\n        1. If input is a `pd.DataFrame` object\n        2. If column names are in such DataFrame\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            The data on which we apply the column selection.\n        y : pd.Series, default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : ColumnSelector\n            The fitted transformer.\n\n        Raises\n        ------\n        TypeError\n            If `X` is not a `pd.DataFrame` object.\n        KeyError\n            If one or more of the columns provided doesn't exist in the input DataFrame.\n        ValueError\n            If dropping the specified columns would result in an empty output DataFrame.\n        \"\"\"\n        self.columns_ = as_list(self.columns)\n        self._check_X_for_type(X)\n        self._check_column_length()\n        self._check_column_names(X)\n        return self\n\n    def transform(self, X):\n        \"\"\"Returns a pandas DataFrame with only the specified columns.\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            The data on which we apply the column selection.\n\n        Returns\n        -------\n        pd.DataFrame\n            The data with the specified columns dropped.\n\n        Raises\n        ------\n        TypeError\n            If `X` is not a `pd.DataFrame` object.\n        \"\"\"\n        self._check_X_for_type(X)\n        if self.columns:\n            return X[self.columns_]\n        return X\n\n    def get_feature_names(self):\n        \"\"\"Alias for `.columns_` attribute\"\"\"\n        return self.columns_\n\n    def _check_column_length(self):\n        \"\"\"Check if no column is selected\"\"\"\n        if len(self.columns_) == 0:\n            raise ValueError(\"Expected columns to be at least of length 1, found length of 0 instead\")\n\n    def _check_column_names(self, X):\n        \"\"\"Check if one or more of the columns provided doesn't exist in the input DataFrame\"\"\"\n        non_existent_columns = set(self.columns_).difference(X.columns)\n        if len(non_existent_columns) &gt; 0:\n            raise KeyError(f\"{list(non_existent_columns)} column(s) not in DataFrame\")\n\n    @staticmethod\n    def _check_X_for_type(X):\n        \"\"\"Checks if input of the Selector is of the required dtype\"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise TypeError(\"Provided variable X is not of type pandas.DataFrame\")\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.pandastransformers.ColumnSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the transformer by storing the column names to keep during transform.</p> <p>Checks:</p> <ol> <li>If input is a <code>pd.DataFrame</code> object</li> <li>If column names are in such DataFrame</li> </ol> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The data on which we apply the column selection.</p> required <code>y</code> <code>Series</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>ColumnSelector</code> <p>The fitted transformer.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>X</code> is not a <code>pd.DataFrame</code> object.</p> <code>KeyError</code> <p>If one or more of the columns provided doesn't exist in the input DataFrame.</p> <code>ValueError</code> <p>If dropping the specified columns would result in an empty output DataFrame.</p> Source code in <code>sklego/preprocessing/pandastransformers.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the transformer by storing the column names to keep during transform.\n\n    Checks:\n\n    1. If input is a `pd.DataFrame` object\n    2. If column names are in such DataFrame\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n        The data on which we apply the column selection.\n    y : pd.Series, default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : ColumnSelector\n        The fitted transformer.\n\n    Raises\n    ------\n    TypeError\n        If `X` is not a `pd.DataFrame` object.\n    KeyError\n        If one or more of the columns provided doesn't exist in the input DataFrame.\n    ValueError\n        If dropping the specified columns would result in an empty output DataFrame.\n    \"\"\"\n    self.columns_ = as_list(self.columns)\n    self._check_X_for_type(X)\n    self._check_column_length()\n    self._check_column_names(X)\n    return self\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.pandastransformers.ColumnSelector.get_feature_names","title":"<code>get_feature_names()</code>","text":"<p>Alias for <code>.columns_</code> attribute</p> Source code in <code>sklego/preprocessing/pandastransformers.py</code> <pre><code>def get_feature_names(self):\n    \"\"\"Alias for `.columns_` attribute\"\"\"\n    return self.columns_\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.pandastransformers.ColumnSelector.transform","title":"<code>transform(X)</code>","text":"<p>Returns a pandas DataFrame with only the specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The data on which we apply the column selection.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The data with the specified columns dropped.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>X</code> is not a <code>pd.DataFrame</code> object.</p> Source code in <code>sklego/preprocessing/pandastransformers.py</code> <pre><code>def transform(self, X):\n    \"\"\"Returns a pandas DataFrame with only the specified columns.\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n        The data on which we apply the column selection.\n\n    Returns\n    -------\n    pd.DataFrame\n        The data with the specified columns dropped.\n\n    Raises\n    ------\n    TypeError\n        If `X` is not a `pd.DataFrame` object.\n    \"\"\"\n    self._check_X_for_type(X)\n    if self.columns:\n        return X[self.columns_]\n    return X\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.dictmapper.DictMapper","title":"<code>sklego.preprocessing.dictmapper.DictMapper</code>","text":"<p>             Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>The <code>DictMapper</code> transformer maps the values of columns according to the input <code>mapper</code> dictionary, fall back to the <code>default</code> value if the key is not present in the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>mapper</code> <code>dict[..., int]</code> <p>The dictionary containing the mapping of the values.</p> required <code>default</code> <code>int</code> <p>The value to fall back to if the value is not in the mapper.</p> required <p>Attributes:</p> Name Type Description <code>n_features_in_</code> <code>int</code> <p>Number of features seen during <code>fit</code>.</p> <code>dim_</code> <code>int</code> <p>Deprecated, please use <code>n_features_in_</code> instead.</p> Source code in <code>sklego/preprocessing/dictmapper.py</code> <pre><code>class DictMapper(TransformerMixin, BaseEstimator):\n    \"\"\"The `DictMapper` transformer maps the values of columns according to the input `mapper` dictionary, fall back to\n    the `default` value if the key is not present in the dictionary.\n\n    Parameters\n    ----------\n    mapper : dict[..., int]\n        The dictionary containing the mapping of the values.\n    default : int\n        The value to fall back to if the value is not in the mapper.\n\n    Attributes\n    ----------\n    n_features_in_ : int\n        Number of features seen during `fit`.\n    dim_ : int\n        Deprecated, please use `n_features_in_` instead.\n    \"\"\"\n\n    def __init__(self, mapper, default):\n        self.mapper = mapper\n        self.default = default\n\n    def fit(self, X, y=None):\n        \"\"\"Checks the input data and records the number of features.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to fit.\n        y : array-like of shape (n_samples,), default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : DictMapper\n            The fitted transformer.\n        \"\"\"\n        X = check_array(\n            X,\n            copy=True,\n            estimator=self,\n            force_all_finite=True,\n            dtype=None,\n            ensure_2d=True,\n        )\n        self.n_features_in_ = X.shape[1]\n        return self\n\n    def transform(self, X):\n        \"\"\"Performs the mapping on the column(s) of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data for which the mapping will be applied.\n\n        Returns\n        -------\n        np.ndarray of shape (n_samples, n_features)\n            The data with the mapping applied.\n\n        Raises\n        ------\n        ValueError\n            If the number of columns from `X` differs from the number of columns when fitting.\n        \"\"\"\n        check_is_fitted(self, [\"n_features_in_\"])\n        X = check_array(\n            X,\n            copy=True,\n            estimator=self,\n            force_all_finite=True,\n            dtype=None,\n            ensure_2d=True,\n        )\n\n        if X.shape[1] != self.n_features_in_:\n            raise ValueError(f\"number of columns {X.shape[1]} does not match fit size {self.n_features_in_}\")\n        return np.vectorize(self.mapper.get, otypes=[int])(X, self.default)\n\n    @property\n    def dim_(self):\n        warn(\n            \"Please use `n_features_in_` instead of `dim_`, `dim_` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self.n_features_in_\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.dictmapper.DictMapper.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Checks the input data and records the number of features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to fit.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>DictMapper</code> <p>The fitted transformer.</p> Source code in <code>sklego/preprocessing/dictmapper.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Checks the input data and records the number of features.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to fit.\n    y : array-like of shape (n_samples,), default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : DictMapper\n        The fitted transformer.\n    \"\"\"\n    X = check_array(\n        X,\n        copy=True,\n        estimator=self,\n        force_all_finite=True,\n        dtype=None,\n        ensure_2d=True,\n    )\n    self.n_features_in_ = X.shape[1]\n    return self\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.dictmapper.DictMapper.transform","title":"<code>transform(X)</code>","text":"<p>Performs the mapping on the column(s) of <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data for which the mapping will be applied.</p> required <p>Returns:</p> Type Description <code>np.ndarray of shape (n_samples, n_features)</code> <p>The data with the mapping applied.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of columns from <code>X</code> differs from the number of columns when fitting.</p> Source code in <code>sklego/preprocessing/dictmapper.py</code> <pre><code>def transform(self, X):\n    \"\"\"Performs the mapping on the column(s) of `X`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data for which the mapping will be applied.\n\n    Returns\n    -------\n    np.ndarray of shape (n_samples, n_features)\n        The data with the mapping applied.\n\n    Raises\n    ------\n    ValueError\n        If the number of columns from `X` differs from the number of columns when fitting.\n    \"\"\"\n    check_is_fitted(self, [\"n_features_in_\"])\n    X = check_array(\n        X,\n        copy=True,\n        estimator=self,\n        force_all_finite=True,\n        dtype=None,\n        ensure_2d=True,\n    )\n\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(f\"number of columns {X.shape[1]} does not match fit size {self.n_features_in_}\")\n    return np.vectorize(self.mapper.get, otypes=[int])(X, self.default)\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.identitytransformer.IdentityTransformer","title":"<code>sklego.preprocessing.identitytransformer.IdentityTransformer</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>The <code>IdentityTransformer</code> returns what it is fed. Does not apply any transformation.</p> <p>The reason for having it is because you can build more expressive pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>check_X</code> <code>bool</code> <p>Whether to validate <code>X</code> to be non-empty 2D array of finite values and attempt to cast <code>X</code> to float. If disabled, the model/pipeline is expected to handle e.g. missing, non-numeric, or non-finite values.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>n_samples_</code> <code>int</code> <p>The number of samples seen during <code>fit</code>.</p> <code>n_features_in_</code> <code>int</code> <p>The number of features seen during <code>fit</code>.</p> <code>shape_</code> <code>tuple[int, int]</code> <p>Deprecated, please use <code>n_samples_</code> and <code>n_features_in_</code> instead.</p> Source code in <code>sklego/preprocessing/identitytransformer.py</code> <pre><code>class IdentityTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"The `IdentityTransformer` returns what it is fed. Does not apply any transformation.\n\n    The reason for having it is because you can build more expressive pipelines.\n\n    Parameters\n    ----------\n    check_X : bool, default=False\n        Whether to validate `X` to be non-empty 2D array of finite values and attempt to cast `X` to float.\n        If disabled, the model/pipeline is expected to handle e.g. missing, non-numeric, or non-finite values.\n\n    Attributes\n    ----------\n    n_samples_ : int\n        The number of samples seen during `fit`.\n    n_features_in_ : int\n        The number of features seen during `fit`.\n    shape_ : tuple[int, int]\n        Deprecated, please use `n_samples_` and `n_features_in_` instead.\n    \"\"\"\n\n    def __init__(self, check_X: bool = False):\n        self.check_X = check_X\n\n    def fit(self, X, y=None):\n        \"\"\"Check the input data if `check_X` is enabled and and records its shape.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to fit.\n        y : array-like of shape (n_samples,), default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : IdentityTransformer\n            The fitted transformer.\n        \"\"\"\n        if self.check_X:\n            X = check_array(X, copy=True, estimator=self)\n        self.n_samples_, self.n_features_in_ = X.shape\n        return self\n\n    def transform(self, X):\n        \"\"\"Performs identity \"transformation\" on `X` - which is no transformation at all.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_features)\n            Unchanged input data.\n\n        Raises\n        ------\n        ValueError\n            If the number of columns from `X` differs from the number of columns when fitting.\n        \"\"\"\n        if self.check_X:\n            X = check_array(X, copy=True, estimator=self)\n        check_is_fitted(self, \"n_features_in_\")\n        if X.shape[1] != self.n_features_in_:\n            raise ValueError(\n                f\"Wrong shape is passed to transform. Trained on {self.n_features_in_} cols got {X.shape[1]}\"\n            )\n        return X\n\n    @property\n    def shape_(self):\n        \"\"\"Returns the shape of the estimator.\"\"\"\n        return (self.n_samples_, self.n_features_in_)\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.identitytransformer.IdentityTransformer.shape_","title":"<code>shape_</code>  <code>property</code>","text":"<p>Returns the shape of the estimator.</p>"},{"location":"api/preprocessing/#sklego.preprocessing.identitytransformer.IdentityTransformer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Check the input data if <code>check_X</code> is enabled and and records its shape.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to fit.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>IdentityTransformer</code> <p>The fitted transformer.</p> Source code in <code>sklego/preprocessing/identitytransformer.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Check the input data if `check_X` is enabled and and records its shape.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to fit.\n    y : array-like of shape (n_samples,), default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : IdentityTransformer\n        The fitted transformer.\n    \"\"\"\n    if self.check_X:\n        X = check_array(X, copy=True, estimator=self)\n    self.n_samples_, self.n_features_in_ = X.shape\n    return self\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.identitytransformer.IdentityTransformer.transform","title":"<code>transform(X)</code>","text":"<p>Performs identity \"transformation\" on <code>X</code> - which is no transformation at all.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Input data.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_features)</code> <p>Unchanged input data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of columns from <code>X</code> differs from the number of columns when fitting.</p> Source code in <code>sklego/preprocessing/identitytransformer.py</code> <pre><code>def transform(self, X):\n    \"\"\"Performs identity \"transformation\" on `X` - which is no transformation at all.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input data.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_features)\n        Unchanged input data.\n\n    Raises\n    ------\n    ValueError\n        If the number of columns from `X` differs from the number of columns when fitting.\n    \"\"\"\n    if self.check_X:\n        X = check_array(X, copy=True, estimator=self)\n    check_is_fitted(self, \"n_features_in_\")\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(\n            f\"Wrong shape is passed to transform. Trained on {self.n_features_in_} cols got {X.shape[1]}\"\n        )\n    return X\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.projections.InformationFilter","title":"<code>sklego.preprocessing.projections.InformationFilter</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>The <code>InformationFilter</code> transformer uses a variant of the Gram-Schmidt process to filter information out of the dataset.</p> <p>This can be useful if you want to filter information out of a dataset because of fairness.</p> <p>To explain how it works: given a training matrix \\(X\\) that contains columns \\(x_1, ..., x_k\\). If we assume columns \\(x_1\\) and \\(x_2\\) to be the sensitive columns then the information-filter will remove information by applying these transformations:</p> \\[\\begin{split}    v_1 &amp; = x_1 \\\\    v_2 &amp; = x_2 - \\frac{x_2 v_1}{v_1 v_1} \\\\    v_3 &amp; = x_3 - \\frac{x_k v_1}{v_1 v_1} - \\frac{x_2 v_2}{v_2 v_2} \\\\        &amp; ... \\\\    v_k &amp; = x_k - \\frac{x_k v_1}{v_1 v_1} - \\frac{x_2 v_2}{v_2 v_2}    \\end{split}\\] <p>Concatenating our vectors (but removing the sensitive ones) gives us a new training matrix</p> \\[X_{fair} = [v_3, ..., v_k]\\] <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>int | str | Sequence[int] | Sequence[str]</code> <p>The columns to filter out. This can be a sequence of either int (in the case of numpy) or string (in the case of pandas).</p> required <code>alpha</code> <code>float</code> <p>Parameter to control how much to filter:</p> <ul> <li><code>alpha=1</code> we filter out all information.</li> <li><code>alpha=0</code> we don't apply any filtering.</li> </ul> <p>Should be between 0 and 1.</p> <code>1.0</code> <p>Attributes:</p> Name Type Description <code>projection_</code> <code>array-like of shape (n_features, n_features)</code> <p>The projection matrix that can be used to filter information out of a dataset.</p> <code>col_ids_</code> <code>List[int] of length `len(columns)`</code> <p>The list of column ids of the sensitive columns.</p> Source code in <code>sklego/preprocessing/projections.py</code> <pre><code>class InformationFilter(BaseEstimator, TransformerMixin):\n    r\"\"\"The `InformationFilter` transformer uses a variant of the\n    [Gram-Schmidt process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process) to filter information out of the\n    dataset.\n\n    This can be useful if you want to filter information out of a dataset because of fairness.\n\n    To explain how it works: given a training matrix $X$ that contains columns $x_1, ..., x_k$.\n    If we assume columns $x_1$ and $x_2$ to be the _sensitive_ columns then the information-filter will remove\n    information by applying these transformations:\n\n    $$\\begin{split}\n       v_1 &amp; = x_1 \\\\\n       v_2 &amp; = x_2 - \\frac{x_2 v_1}{v_1 v_1} \\\\\n       v_3 &amp; = x_3 - \\frac{x_k v_1}{v_1 v_1} - \\frac{x_2 v_2}{v_2 v_2} \\\\\n           &amp; ... \\\\\n       v_k &amp; = x_k - \\frac{x_k v_1}{v_1 v_1} - \\frac{x_2 v_2}{v_2 v_2}\n       \\end{split}$$\n\n    Concatenating our vectors (but removing the sensitive ones) gives us a new training matrix\n\n    $$X_{fair} = [v_3, ..., v_k]$$\n\n    Parameters\n    ----------\n    columns : int | str | Sequence[int] | Sequence[str]\n        The columns to filter out. This can be a sequence of either int (in the case of numpy) or string\n        (in the case of pandas).\n    alpha : float, default=1.0\n        Parameter to control how much to filter:\n\n        - `alpha=1` we filter out all information.\n        - `alpha=0` we don't apply any filtering.\n\n        Should be between 0 and 1.\n\n    Attributes\n    ----------\n    projection_ : array-like of shape (n_features, n_features)\n        The projection matrix that can be used to filter information out of a dataset.\n    col_ids_ : List[int] of length `len(columns)`\n        The list of column ids of the sensitive columns.\n    \"\"\"\n\n    def __init__(self, columns, alpha=1):\n        self.columns = columns\n        self.alpha = alpha\n\n    def _check_coltype(self, X):\n        \"\"\"Check if the `columns` type(s) are compatible with `X` type.\"\"\"\n        for col in as_list(self.columns):\n            if isinstance(col, str):\n                if isinstance(X, np.ndarray):\n                    raise ValueError(f\"column {col} is a string but datatype receive is numpy.\")\n                if isinstance(X, pd.DataFrame):\n                    if col not in X.columns:\n                        raise ValueError(f\"column {col} is not in {X.columns}\")\n            if isinstance(col, int):\n                if col not in range(np.atleast_2d(np.array(X)).shape[1]):\n                    raise ValueError(f\"column {col} is out of bounds for input shape {X.shape}\")\n\n    def _col_idx(self, X, name):\n        \"\"\"Get the column index of a column name.\"\"\"\n        if isinstance(name, str):\n            if isinstance(X, np.ndarray):\n                raise ValueError(\"You cannot have a column of type string on a numpy input matrix.\")\n            return {name: i for i, name in enumerate(X.columns)}[name]\n        return name\n\n    def _make_v_vectors(self, X, col_ids):\n        \"\"\"Make the v vectors that we will use to filter out information.\"\"\"\n        vs = np.zeros((X.shape[0], len(col_ids)))\n        for i, c in enumerate(col_ids):\n            vs[:, i] = X[:, col_ids[i]]\n            for j in range(0, i):\n                vs[:, i] = vs[:, i] - vector_projection(vs[:, i], vs[:, j])\n        return vs\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the transformer by learning the projection required to make the dataset orthogonal to sensitive\n        columns.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to fit.\n        y : array-like of shape (n_samples,), default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : InformationFilter\n            The fitted transformer.\n\n        Raises\n        ------\n        ValueError\n            If `columns` type(s) are incompatible with input data `X` type.\n        \"\"\"\n        self._check_coltype(X)\n        self.col_ids_ = [v if isinstance(v, int) else self._col_idx(X, v) for v in as_list(self.columns)]\n        X = check_array(X, estimator=self)\n        X_fair = X.copy()\n        v_vectors = self._make_v_vectors(X, self.col_ids_)\n        # gram smidt process but only on sensitive attributes\n        for i, col in enumerate(X_fair.T):\n            for v in v_vectors.T:\n                X_fair[:, i] = X_fair[:, i] - vector_projection(X_fair[:, i], v)\n        # we want to learn matrix P: X P = X_fair\n        # this means we first need to create X_fair in order to learn P\n        self.projection_, resid, rank, s = np.linalg.lstsq(X, X_fair, rcond=None)\n        return self\n\n    def transform(self, X):\n        \"\"\"Transforms `X` by applying the information filter.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to transform.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_features)\n            The transformed data.\n\n        Raises\n        ------\n        ValueError\n            If `columns` type(s) are incompatible with input data `X` type.\n        \"\"\"\n        check_is_fitted(self, [\"projection_\", \"col_ids_\"])\n        self._check_coltype(X)\n        X = check_array(X, estimator=self)\n        # apply the projection and remove the column we won't need\n        X_fair = X @ self.projection_\n        X_removed = np.delete(X_fair, self.col_ids_, axis=1)\n        X_orig = np.delete(X, self.col_ids_, axis=1)\n        return self.alpha * np.atleast_2d(X_removed) + (1 - self.alpha) * np.atleast_2d(X_orig)\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.projections.InformationFilter.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the transformer by learning the projection required to make the dataset orthogonal to sensitive columns.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to fit.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>InformationFilter</code> <p>The fitted transformer.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>columns</code> type(s) are incompatible with input data <code>X</code> type.</p> Source code in <code>sklego/preprocessing/projections.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the transformer by learning the projection required to make the dataset orthogonal to sensitive\n    columns.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to fit.\n    y : array-like of shape (n_samples,), default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : InformationFilter\n        The fitted transformer.\n\n    Raises\n    ------\n    ValueError\n        If `columns` type(s) are incompatible with input data `X` type.\n    \"\"\"\n    self._check_coltype(X)\n    self.col_ids_ = [v if isinstance(v, int) else self._col_idx(X, v) for v in as_list(self.columns)]\n    X = check_array(X, estimator=self)\n    X_fair = X.copy()\n    v_vectors = self._make_v_vectors(X, self.col_ids_)\n    # gram smidt process but only on sensitive attributes\n    for i, col in enumerate(X_fair.T):\n        for v in v_vectors.T:\n            X_fair[:, i] = X_fair[:, i] - vector_projection(X_fair[:, i], v)\n    # we want to learn matrix P: X P = X_fair\n    # this means we first need to create X_fair in order to learn P\n    self.projection_, resid, rank, s = np.linalg.lstsq(X, X_fair, rcond=None)\n    return self\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.projections.InformationFilter.transform","title":"<code>transform(X)</code>","text":"<p>Transforms <code>X</code> by applying the information filter.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to transform.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_features)</code> <p>The transformed data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>columns</code> type(s) are incompatible with input data <code>X</code> type.</p> Source code in <code>sklego/preprocessing/projections.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transforms `X` by applying the information filter.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to transform.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_features)\n        The transformed data.\n\n    Raises\n    ------\n    ValueError\n        If `columns` type(s) are incompatible with input data `X` type.\n    \"\"\"\n    check_is_fitted(self, [\"projection_\", \"col_ids_\"])\n    self._check_coltype(X)\n    X = check_array(X, estimator=self)\n    # apply the projection and remove the column we won't need\n    X_fair = X @ self.projection_\n    X_removed = np.delete(X_fair, self.col_ids_, axis=1)\n    X_orig = np.delete(X, self.col_ids_, axis=1)\n    return self.alpha * np.atleast_2d(X_removed) + (1 - self.alpha) * np.atleast_2d(X_orig)\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.intervalencoder.IntervalEncoder","title":"<code>sklego.preprocessing.intervalencoder.IntervalEncoder</code>","text":"<p>             Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>The <code>IntervalEncoder</code> transformer bends features in <code>X</code> with regards to<code>y</code>.</p> <p>We take each column in <code>X</code> separately and smooth it towards <code>y</code> using the strategy that is defined in <code>method</code>.</p> <p>Note that this allows us to make certain features strictly monotonic in your machine learning model if you follow this with an appropriate model.</p> <p>Parameters:</p> Name Type Description Default <code>n_chunks</code> <code>int</code> <p>The number of cuts that makes the interval.</p> <code>10</code> <code>span</code> <code>float</code> <p>A hyperparameter for the interpolation method, if the method is <code>\"normal\"</code> it resembles the width of the radial basis function used to weigh the points. It is ignored if the method is <code>\"increasing\"</code> or <code>\"decreasing\"</code>.</p> <code>1.0</code> <code>method</code> <code>Literal[average, normal, increasing, decreasing]</code> <p>The interpolation method used, can be either <code>\"average\"</code>, <code>\"normal\"</code>, <code>\"increasing\"</code> or <code>\"decreasing\"</code>.</p> <code>\"normal\"</code> <p>Attributes:</p> Name Type Description <code>quantiles_</code> <code>np.ndarray of shape (n_features, n_chunks)</code> <p>The quantiles that are used to cut the interval.</p> <code>heights_</code> <code>np.ndarray of shape (n_features, n_chunks)</code> <p>The heights of the quantiles that are used to cut the interval.</p> <code>n_features_in_</code> <code>int</code> <p>Number of features seen during <code>fit</code>.</p> <code>num_cols_</code> <code>int</code> <p>Deprecated, please use <code>n_features_in_</code> instead.</p> Source code in <code>sklego/preprocessing/intervalencoder.py</code> <pre><code>class IntervalEncoder(TransformerMixin, BaseEstimator):\n    \"\"\"The `IntervalEncoder` transformer bends features in `X` with regards to`y`.\n\n    We take each column in `X` separately and smooth it towards `y` using the strategy that is defined in `method`.\n\n    Note that this allows us to make certain features strictly monotonic in your machine learning model if you follow\n    this with an appropriate model.\n\n    Parameters\n    ----------\n    n_chunks : int, default=10\n        The number of cuts that makes the interval.\n    span : float, default=1.0\n        A hyperparameter for the interpolation method, if the method is `\"normal\"` it resembles the width of the radial\n        basis function used to weigh the points. It is ignored if the method is `\"increasing\"` or `\"decreasing\"`.\n    method : Literal[\"average\", \"normal\", \"increasing\", \"decreasing\"], default=\"normal\"\n        The interpolation method used, can be either `\"average\"`, `\"normal\"`, `\"increasing\"` or `\"decreasing\"`.\n\n    Attributes\n    ----------\n    quantiles_ : np.ndarray of shape (n_features, n_chunks)\n        The quantiles that are used to cut the interval.\n    heights_ : np.ndarray of shape (n_features, n_chunks)\n        The heights of the quantiles that are used to cut the interval.\n    n_features_in_ : int\n        Number of features seen during `fit`.\n    num_cols_ : int\n        Deprecated, please use `n_features_in_` instead.\n    \"\"\"\n\n    _ALLOWED_METHODS = (\"average\", \"normal\", \"increasing\", \"decreasing\")\n\n    def __init__(self, n_chunks=10, span=1, method=\"normal\"):\n        self.span = span\n        self.method = method\n        self.n_chunks = n_chunks\n\n    def fit(self, X, y):\n        \"\"\"Fit the `IntervalEncoder` transformer by computing interpolation quantiles for each column of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : IntervalEncoder\n            The fitted transformer.\n\n        Raises\n        ------\n        ValueError\n            - If `method` is not one of `\"average\"`, `\"normal\"`, `\"increasing\"` or `\"decreasing\"`.\n            - If `n_chunks` is not a positive integer.\n            - If `span` is not between 0 and 1.\n        \"\"\"\n\n        if self.method not in self._ALLOWED_METHODS:\n            raise ValueError(f\"`method` must be in {self._ALLOWED_METHODS}, got `{self.method}`\")\n        if self.n_chunks &lt;= 0:\n            raise ValueError(f\"`n_chunks` must be &gt;= 1, received {self.n_chunks}\")\n        if self.span &gt; 1.0:\n            raise ValueError(f\"Error, we expect 0 &lt;= span &lt;= 1, received span={self.span}\")\n        if self.span &lt; 0.0:\n            raise ValueError(f\"Error, we expect 0 &lt;= span &lt;= 1, received span={self.span}\")\n\n        # these two matrices will have shape (columns, quantiles)\n        # quantiles indicate where the interval split occurs\n        X, y = check_X_y(X, y, estimator=self)\n        self.quantiles_ = np.zeros((X.shape[1], self.n_chunks))\n        # heights indicate what heights these intervals will have\n        self.heights_ = np.zeros((X.shape[1], self.n_chunks))\n        self.n_features_in_ = X.shape[1]\n\n        average_func = _mk_average if self.method in [\"average\", \"normal\"] else _mk_monotonic_average\n\n        for col in range(X.shape[1]):\n            self.quantiles_[col, :] = np.quantile(X[:, col], q=np.linspace(0, 1, self.n_chunks))\n            self.heights_[col, :] = average_func(\n                X[:, col],\n                y,\n                self.quantiles_[col, :],\n                span=self.span,\n                method=self.method,\n            )\n        return self\n\n    def transform(self, X):\n        \"\"\"Performs smoothing on the column(s) of `X` according to the quantile values computed during fitting.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data for which the smoothing will be applied.\n\n        Returns\n        -------\n        X : np.ndarray of shape (n_samples, n_features)\n            `X` values with smoothed values.\n\n        Raises\n        ------\n        ValueError\n            If the number of columns from `X` differs from the number of columns when fitting.\n        \"\"\"\n        check_is_fitted(self, [\"quantiles_\", \"heights_\", \"n_features_in_\"])\n        X = check_array(X, estimator=self)\n        if X.shape[1] != self.n_features_in_:\n            raise ValueError(f\"fitted on {self.n_features_in_} features but received {X.shape[1]}\")\n        transformed = np.zeros(X.shape)\n        for col in range(transformed.shape[1]):\n            transformed[:, col] = np.interp(X[:, col], self.quantiles_[col, :], self.heights_[col, :])\n        return transformed\n\n    @property\n    def num_cols_(self):\n        warn(\n            \"Please use `n_features_in_` instead of `num_cols_`, `num_cols_` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self.n_features_in_\n\n    @property\n    def allowed_methods(self):\n        warn(\n            \"Please use `_ALLOWED_METHODS` instead of `allowed_methods`,\"\n            \"`allowed_methods` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self._ALLOWED_METHODS\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.intervalencoder.IntervalEncoder.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the <code>IntervalEncoder</code> transformer by computing interpolation quantiles for each column of <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>IntervalEncoder</code> <p>The fitted transformer.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If <code>method</code> is not one of <code>\"average\"</code>, <code>\"normal\"</code>, <code>\"increasing\"</code> or <code>\"decreasing\"</code>.</li> <li>If <code>n_chunks</code> is not a positive integer.</li> <li>If <code>span</code> is not between 0 and 1.</li> </ul> Source code in <code>sklego/preprocessing/intervalencoder.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the `IntervalEncoder` transformer by computing interpolation quantiles for each column of `X`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training data.\n    y : array-like of shape (n_samples,)\n        Target values.\n\n    Returns\n    -------\n    self : IntervalEncoder\n        The fitted transformer.\n\n    Raises\n    ------\n    ValueError\n        - If `method` is not one of `\"average\"`, `\"normal\"`, `\"increasing\"` or `\"decreasing\"`.\n        - If `n_chunks` is not a positive integer.\n        - If `span` is not between 0 and 1.\n    \"\"\"\n\n    if self.method not in self._ALLOWED_METHODS:\n        raise ValueError(f\"`method` must be in {self._ALLOWED_METHODS}, got `{self.method}`\")\n    if self.n_chunks &lt;= 0:\n        raise ValueError(f\"`n_chunks` must be &gt;= 1, received {self.n_chunks}\")\n    if self.span &gt; 1.0:\n        raise ValueError(f\"Error, we expect 0 &lt;= span &lt;= 1, received span={self.span}\")\n    if self.span &lt; 0.0:\n        raise ValueError(f\"Error, we expect 0 &lt;= span &lt;= 1, received span={self.span}\")\n\n    # these two matrices will have shape (columns, quantiles)\n    # quantiles indicate where the interval split occurs\n    X, y = check_X_y(X, y, estimator=self)\n    self.quantiles_ = np.zeros((X.shape[1], self.n_chunks))\n    # heights indicate what heights these intervals will have\n    self.heights_ = np.zeros((X.shape[1], self.n_chunks))\n    self.n_features_in_ = X.shape[1]\n\n    average_func = _mk_average if self.method in [\"average\", \"normal\"] else _mk_monotonic_average\n\n    for col in range(X.shape[1]):\n        self.quantiles_[col, :] = np.quantile(X[:, col], q=np.linspace(0, 1, self.n_chunks))\n        self.heights_[col, :] = average_func(\n            X[:, col],\n            y,\n            self.quantiles_[col, :],\n            span=self.span,\n            method=self.method,\n        )\n    return self\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.intervalencoder.IntervalEncoder.transform","title":"<code>transform(X)</code>","text":"<p>Performs smoothing on the column(s) of <code>X</code> according to the quantile values computed during fitting.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data for which the smoothing will be applied.</p> required <p>Returns:</p> Name Type Description <code>X</code> <code>np.ndarray of shape (n_samples, n_features)</code> <p><code>X</code> values with smoothed values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of columns from <code>X</code> differs from the number of columns when fitting.</p> Source code in <code>sklego/preprocessing/intervalencoder.py</code> <pre><code>def transform(self, X):\n    \"\"\"Performs smoothing on the column(s) of `X` according to the quantile values computed during fitting.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data for which the smoothing will be applied.\n\n    Returns\n    -------\n    X : np.ndarray of shape (n_samples, n_features)\n        `X` values with smoothed values.\n\n    Raises\n    ------\n    ValueError\n        If the number of columns from `X` differs from the number of columns when fitting.\n    \"\"\"\n    check_is_fitted(self, [\"quantiles_\", \"heights_\", \"n_features_in_\"])\n    X = check_array(X, estimator=self)\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(f\"fitted on {self.n_features_in_} features but received {X.shape[1]}\")\n    transformed = np.zeros(X.shape)\n    for col in range(transformed.shape[1]):\n        transformed[:, col] = np.interp(X[:, col], self.quantiles_[col, :], self.heights_[col, :])\n    return transformed\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.projections.OrthogonalTransformer","title":"<code>sklego.preprocessing.projections.OrthogonalTransformer</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>The <code>OrthogonalTransformer</code> transforms the columns of a dataframe or numpy array to orthogonal (or orthonormal if <code>normalize=True</code>) matrix.</p> <p>It learns matrices \\(Q, R\\) such that \\(X = Q \\cdot R\\), with \\(Q\\) orthogonal, from which follows \\(Q = X \\cdot R^{-1}\\)</p> <p>Parameters:</p> Name Type Description Default <code>normalize</code> <code>bool</code> <p>Whether or not orthogonal matrix should be orthonormal as well.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>inv_R_</code> <code>array-like of shape (n_features, n_features)</code> <p>The inverse of R of the QR decomposition of <code>X</code>.</p> <code>normalization_vector_</code> <code>array-like of shape (n_features,)</code> <p>The normalization terms to make the orthogonal matrix orthonormal.</p> <p>Examples:</p> <pre><code>from sklearn.datasets import make_regression\nfrom sklego.preprocessing import OrthogonalTransformer\n\n# Generate a synthetic dataset\nX, y = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=42)\n\n# Instantiate the transformer\ntransformer = OrthogonalTransformer(normalize=True)\n\n# Fit the pipeline with the training data\ntransformer.fit(X)\n\n# Transform the data using the fitted transformer\nX_transformed = transformer.transform(X)\n</code></pre> Source code in <code>sklego/preprocessing/projections.py</code> <pre><code>class OrthogonalTransformer(BaseEstimator, TransformerMixin):\n    r\"\"\"The `OrthogonalTransformer` transforms the columns of a dataframe or numpy array to orthogonal (or\n    orthonormal if `normalize=True`) matrix.\n\n    It learns matrices $Q, R$ such that $X = Q \\cdot R$, with $Q$ orthogonal, from which follows $Q = X \\cdot R^{-1}$\n\n    Parameters\n    ----------\n    normalize : bool, default=False\n        Whether or not orthogonal matrix should be orthonormal as well.\n\n    Attributes\n    ----------\n    inv_R_ : array-like of shape (n_features, n_features)\n        The inverse of R of the QR decomposition of `X`.\n    normalization_vector_ : array-like of shape (n_features,)\n        The normalization terms to make the orthogonal matrix orthonormal.\n\n    Examples\n    --------\n    ```py\n    from sklearn.datasets import make_regression\n    from sklego.preprocessing import OrthogonalTransformer\n\n    # Generate a synthetic dataset\n    X, y = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=42)\n\n    # Instantiate the transformer\n    transformer = OrthogonalTransformer(normalize=True)\n\n    # Fit the pipeline with the training data\n    transformer.fit(X)\n\n    # Transform the data using the fitted transformer\n    X_transformed = transformer.transform(X)\n    ```\n    \"\"\"\n\n    def __init__(self, normalize=False):\n        self.normalize = normalize\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the transformer to the input data by calculating the inverse of R of the QR decomposition of `X`.\n        This can be used to calculate the orthogonal projection of `X`.\n\n        If normalization is required, also stores a vector with normalization terms.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to fit.\n        y : array-like of shape (n_samples,), default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : OrthogonalTransformer\n            The fitted transformer.\n        \"\"\"\n        X = check_array(X, estimator=self)\n\n        if not X.shape[0] &gt; 1:\n            raise ValueError(\"Orthogonal transformation not valid for one sample\")\n\n        # Q, R such that X = Q*R, with Q orthogonal, from which follows Q = X*inv(R)\n        Q, R = np.linalg.qr(X)\n        self.inv_R_ = np.linalg.inv(R)\n\n        if self.normalize:\n            self.normalization_vector_ = np.linalg.norm(Q, ord=2, axis=0)\n        else:\n            self.normalization_vector_ = np.ones((X.shape[1],))\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transforms `X` using the fitted inverse of R. Normalizes the result if required.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to transform.\n\n        Returns\n        -------\n        array-like of shape (n_samples, n_features)\n            The transformed data.\n        \"\"\"\n        if self.normalize:\n            check_is_fitted(self, [\"inv_R_\", \"normalization_vector_\"])\n        else:\n            check_is_fitted(self, [\"inv_R_\"])\n\n        X = check_array(X, estimator=self)\n\n        return X @ self.inv_R_ / self.normalization_vector_\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.projections.OrthogonalTransformer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the transformer to the input data by calculating the inverse of R of the QR decomposition of <code>X</code>. This can be used to calculate the orthogonal projection of <code>X</code>.</p> <p>If normalization is required, also stores a vector with normalization terms.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to fit.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>OrthogonalTransformer</code> <p>The fitted transformer.</p> Source code in <code>sklego/preprocessing/projections.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the transformer to the input data by calculating the inverse of R of the QR decomposition of `X`.\n    This can be used to calculate the orthogonal projection of `X`.\n\n    If normalization is required, also stores a vector with normalization terms.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to fit.\n    y : array-like of shape (n_samples,), default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : OrthogonalTransformer\n        The fitted transformer.\n    \"\"\"\n    X = check_array(X, estimator=self)\n\n    if not X.shape[0] &gt; 1:\n        raise ValueError(\"Orthogonal transformation not valid for one sample\")\n\n    # Q, R such that X = Q*R, with Q orthogonal, from which follows Q = X*inv(R)\n    Q, R = np.linalg.qr(X)\n    self.inv_R_ = np.linalg.inv(R)\n\n    if self.normalize:\n        self.normalization_vector_ = np.linalg.norm(Q, ord=2, axis=0)\n    else:\n        self.normalization_vector_ = np.ones((X.shape[1],))\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.projections.OrthogonalTransformer.transform","title":"<code>transform(X)</code>","text":"<p>Transforms <code>X</code> using the fitted inverse of R. Normalizes the result if required.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to transform.</p> required <p>Returns:</p> Type Description <code>array-like of shape (n_samples, n_features)</code> <p>The transformed data.</p> Source code in <code>sklego/preprocessing/projections.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transforms `X` using the fitted inverse of R. Normalizes the result if required.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to transform.\n\n    Returns\n    -------\n    array-like of shape (n_samples, n_features)\n        The transformed data.\n    \"\"\"\n    if self.normalize:\n        check_is_fitted(self, [\"inv_R_\", \"normalization_vector_\"])\n    else:\n        check_is_fitted(self, [\"inv_R_\"])\n\n    X = check_array(X, estimator=self)\n\n    return X @ self.inv_R_ / self.normalization_vector_\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.outlier_remover.OutlierRemover","title":"<code>sklego.preprocessing.outlier_remover.OutlierRemover</code>","text":"<p>             Bases: <code>TrainOnlyTransformerMixin</code>, <code>BaseEstimator</code></p> <p>The <code>OutlierRemover</code> transformer removes outliers (train-time only) using the supplied removal model. The removal model should implement <code>.fit()</code> and <code>.predict()</code> methods.</p> <p>Parameters:</p> Name Type Description Default <code>outlier_detector</code> <code>object</code> <p>An outlier detector that implements <code>.fit()</code> and <code>.predict()</code> methods.</p> required <code>refit</code> <code>bool</code> <p>If True, fits the estimator during <code>pipeline.fit()</code>. If False, the estimator is not fitted during <code>pipeline.fit()</code>.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>estimator_</code> <code>object</code> <p>The fitted outlier detector.</p> Source code in <code>sklego/preprocessing/outlier_remover.py</code> <pre><code>class OutlierRemover(TrainOnlyTransformerMixin, BaseEstimator):\n    \"\"\"The `OutlierRemover` transformer removes outliers (train-time only) using the supplied removal model. The\n    removal model should implement `.fit()` and `.predict()` methods.\n\n    Parameters\n    ----------\n    outlier_detector : object\n        An outlier detector that implements `.fit()` and `.predict()` methods.\n    refit : bool, default=True\n        If True, fits the estimator during `pipeline.fit()`. If False, the estimator is not fitted during\n        `pipeline.fit()`.\n\n    Attributes\n    ----------\n    estimator_ : object\n        The fitted outlier detector.\n    \"\"\"\n\n    def __init__(self, outlier_detector, refit=True):\n        self.outlier_detector = outlier_detector\n        self.refit = refit\n        self.estimator_ = None\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the estimator on training data `X` and `y` by fitting the underlying outlier detector if `refit` is True.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,), default=None\n            Target values.\n\n        Returns\n        -------\n        self : OutlierRemover\n            The fitted transformer.\n        \"\"\"\n        self.estimator_ = clone(self.outlier_detector)\n        if self.refit:\n            super().fit(X, y)\n            self.estimator_.fit(X, y)\n        return self\n\n    def transform_train(self, X):\n        \"\"\"Removes outliers from `X` using the fitted estimator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data for which the outliers will be removed.\n\n        Returns\n        -------\n        np.ndarray of shape (n_not_outliers, n_features)\n            The data with the outliers removed, where `n_not_outliers = n_samples - n_outliers`.\n        \"\"\"\n        check_is_fitted(self, \"estimator_\")\n        predictions = self.estimator_.predict(X)\n        check_array(predictions, estimator=self.outlier_detector, ensure_2d=False)\n        return X[predictions != -1]\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.outlier_remover.OutlierRemover.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the estimator on training data <code>X</code> and <code>y</code> by fitting the underlying outlier detector if <code>refit</code> is True.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target values.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>OutlierRemover</code> <p>The fitted transformer.</p> Source code in <code>sklego/preprocessing/outlier_remover.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the estimator on training data `X` and `y` by fitting the underlying outlier detector if `refit` is True.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training data.\n    y : array-like of shape (n_samples,), default=None\n        Target values.\n\n    Returns\n    -------\n    self : OutlierRemover\n        The fitted transformer.\n    \"\"\"\n    self.estimator_ = clone(self.outlier_detector)\n    if self.refit:\n        super().fit(X, y)\n        self.estimator_.fit(X, y)\n    return self\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.outlier_remover.OutlierRemover.transform_train","title":"<code>transform_train(X)</code>","text":"<p>Removes outliers from <code>X</code> using the fitted estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data for which the outliers will be removed.</p> required <p>Returns:</p> Type Description <code>np.ndarray of shape (n_not_outliers, n_features)</code> <p>The data with the outliers removed, where <code>n_not_outliers = n_samples - n_outliers</code>.</p> Source code in <code>sklego/preprocessing/outlier_remover.py</code> <pre><code>def transform_train(self, X):\n    \"\"\"Removes outliers from `X` using the fitted estimator.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data for which the outliers will be removed.\n\n    Returns\n    -------\n    np.ndarray of shape (n_not_outliers, n_features)\n        The data with the outliers removed, where `n_not_outliers = n_samples - n_outliers`.\n    \"\"\"\n    check_is_fitted(self, \"estimator_\")\n    predictions = self.estimator_.predict(X)\n    check_array(predictions, estimator=self.outlier_detector, ensure_2d=False)\n    return X[predictions != -1]\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.pandastransformers.PandasTypeSelector","title":"<code>sklego.preprocessing.pandastransformers.PandasTypeSelector</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>The <code>PandasTypeSelector</code> transformer allows to select columns in a pandas DataFrame based on their type. Can be useful in a sklearn Pipeline.</p> <p>It uses pandas.DataFrame.select_dtypes method.</p> <p>Parameters:</p> Name Type Description Default <code>include</code> <code>scalar or list - like</code> <p>Column type(s) to be selected</p> <code>None</code> <code>exclude</code> <code>scalar or list - like</code> <p>Column type(s) to be excluded from selection</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>feature_names_</code> <code>list[str]</code> <p>The names of the features to keep during transform.</p> <code>X_dtypes_</code> <code>Series</code> <p>The dtypes of the columns in the input DataFrame.</p> <p>Warning</p> <p>Raises a <code>TypeError</code> if input provided is not a DataFrame.</p> Source code in <code>sklego/preprocessing/pandastransformers.py</code> <pre><code>class PandasTypeSelector(BaseEstimator, TransformerMixin):\n    \"\"\"The `PandasTypeSelector` transformer allows to select columns in a pandas DataFrame based on their type.\n    Can be useful in a sklearn Pipeline.\n\n    It uses\n    [pandas.DataFrame.select_dtypes](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html)\n    method.\n\n    Parameters\n    ----------\n    include : scalar or list-like\n        Column type(s) to be selected\n    exclude : scalar or list-like\n        Column type(s) to be excluded from selection\n\n    Attributes\n    ----------\n    feature_names_ : list[str]\n        The names of the features to keep during transform.\n    X_dtypes_ : pd.Series\n        The dtypes of the columns in the input DataFrame.\n\n    !!! warning\n\n        Raises a `TypeError` if input provided is not a DataFrame.\n    \"\"\"\n\n    def __init__(self, include=None, exclude=None):\n        self.include = include\n        self.exclude = exclude\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the transformer by saving the column names to keep during transform.\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            The data on which we apply the column selection.\n        y : pd.Series, default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : PandasTypeSelector\n            The fitted transformer.\n\n        Raises\n        ------\n        TypeError\n            If `X` is not a `pd.DataFrame` object.\n        ValueError\n            If provided type(s) results in empty dataframe.\n        \"\"\"\n        self._check_X_for_type(X)\n        self.X_dtypes_ = X.dtypes\n        self.feature_names_ = list(X.select_dtypes(include=self.include, exclude=self.exclude).columns)\n\n        if len(self.feature_names_) == 0:\n            raise ValueError(\"Provided type(s) results in empty dataframe\")\n\n        return self\n\n    def get_feature_names(self, *args, **kwargs):\n        \"\"\"Alias for `.feature_names_` attribute\"\"\"\n        return self.feature_names_\n\n    def transform(self, X):\n        \"\"\"Returns a pandas DataFrame with columns (de)selected based on their dtype.\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            The data to select dtype for.\n\n        Returns\n        -------\n        pd.DataFrame\n            The data with the specified columns selected.\n\n        Raises\n        ------\n        TypeError\n            If `X` is not a `pd.DataFrame` object.\n        ValueError\n            If column dtypes were not equal during fit and transform.\n        \"\"\"\n        check_is_fitted(self, [\"X_dtypes_\", \"feature_names_\"])\n\n        try:\n            if (self.X_dtypes_ != X.dtypes).any():\n                raise ValueError(\n                    f\"Column dtypes were not equal during fit and transform. Fit types: \\n\"\n                    f\"{self.X_dtypes_}\\n\"\n                    f\"transform: \\n\"\n                    f\"{X.dtypes}\"\n                )\n        except ValueError as e:\n            raise ValueError(\"Columns were not equal during fit and transform\") from e\n\n        self._check_X_for_type(X)\n        transformed_df = X.select_dtypes(include=self.include, exclude=self.exclude)\n\n        return transformed_df\n\n    @staticmethod\n    def _check_X_for_type(X):\n        \"\"\"Checks if input of the Selector is of the required dtype\"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise TypeError(\"Provided variable X is not of type pandas.DataFrame\")\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.pandastransformers.PandasTypeSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the transformer by saving the column names to keep during transform.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The data on which we apply the column selection.</p> required <code>y</code> <code>Series</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>PandasTypeSelector</code> <p>The fitted transformer.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>X</code> is not a <code>pd.DataFrame</code> object.</p> <code>ValueError</code> <p>If provided type(s) results in empty dataframe.</p> Source code in <code>sklego/preprocessing/pandastransformers.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the transformer by saving the column names to keep during transform.\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n        The data on which we apply the column selection.\n    y : pd.Series, default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : PandasTypeSelector\n        The fitted transformer.\n\n    Raises\n    ------\n    TypeError\n        If `X` is not a `pd.DataFrame` object.\n    ValueError\n        If provided type(s) results in empty dataframe.\n    \"\"\"\n    self._check_X_for_type(X)\n    self.X_dtypes_ = X.dtypes\n    self.feature_names_ = list(X.select_dtypes(include=self.include, exclude=self.exclude).columns)\n\n    if len(self.feature_names_) == 0:\n        raise ValueError(\"Provided type(s) results in empty dataframe\")\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.pandastransformers.PandasTypeSelector.get_feature_names","title":"<code>get_feature_names(*args, **kwargs)</code>","text":"<p>Alias for <code>.feature_names_</code> attribute</p> Source code in <code>sklego/preprocessing/pandastransformers.py</code> <pre><code>def get_feature_names(self, *args, **kwargs):\n    \"\"\"Alias for `.feature_names_` attribute\"\"\"\n    return self.feature_names_\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.pandastransformers.PandasTypeSelector.transform","title":"<code>transform(X)</code>","text":"<p>Returns a pandas DataFrame with columns (de)selected based on their dtype.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The data to select dtype for.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The data with the specified columns selected.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>X</code> is not a <code>pd.DataFrame</code> object.</p> <code>ValueError</code> <p>If column dtypes were not equal during fit and transform.</p> Source code in <code>sklego/preprocessing/pandastransformers.py</code> <pre><code>def transform(self, X):\n    \"\"\"Returns a pandas DataFrame with columns (de)selected based on their dtype.\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n        The data to select dtype for.\n\n    Returns\n    -------\n    pd.DataFrame\n        The data with the specified columns selected.\n\n    Raises\n    ------\n    TypeError\n        If `X` is not a `pd.DataFrame` object.\n    ValueError\n        If column dtypes were not equal during fit and transform.\n    \"\"\"\n    check_is_fitted(self, [\"X_dtypes_\", \"feature_names_\"])\n\n    try:\n        if (self.X_dtypes_ != X.dtypes).any():\n            raise ValueError(\n                f\"Column dtypes were not equal during fit and transform. Fit types: \\n\"\n                f\"{self.X_dtypes_}\\n\"\n                f\"transform: \\n\"\n                f\"{X.dtypes}\"\n            )\n    except ValueError as e:\n        raise ValueError(\"Columns were not equal during fit and transform\") from e\n\n    self._check_X_for_type(X)\n    transformed_df = X.select_dtypes(include=self.include, exclude=self.exclude)\n\n    return transformed_df\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.patsytransformer.PatsyTransformer","title":"<code>sklego.preprocessing.patsytransformer.PatsyTransformer</code>","text":"<p>             Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>The <code>PatsyTransformer</code> offers a method to select the right columns from a dataframe as well as a DSL for transformations.</p> <p>It is inspired from R formulas. This is can be useful as a first step in the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>formula</code> <code>str</code> <p>A patsy-compatible formula.</p> required <code>return_type</code> <code>Literal[matrix, dataframe]</code> <p>Either \"matrix\" or \"dataframe\", passed on to patsy.</p> <code>\"matrix\"</code> <p>Attributes:</p> Name Type Description <code>design_info_</code> <code>[patsy.DesignInfo](https://patsy.readthedocs.io/en/latest/API-reference.html#patsy.DesignInfo)</code> <p>A DesignInfo object holds metadata about a design matrix.</p> Source code in <code>sklego/preprocessing/patsytransformer.py</code> <pre><code>@deprecated(\n    version=\"0.6.17\",\n    reason=\"Please use `sklego.preprocessing.FormulaicTransformer` instead. \"\n    \"This object will be removed from the preprocessing submodule in version 0.8.0.\",\n)\nclass PatsyTransformer(TransformerMixin, BaseEstimator):\n    \"\"\"The `PatsyTransformer` offers a method to select the right columns from a dataframe as well as a DSL for\n    transformations.\n\n    It is inspired from R formulas. This is can be useful as a first step in the pipeline.\n\n    Parameters\n    ----------\n    formula : str\n        A patsy-compatible formula.\n    return_type : Literal[\"matrix\", \"dataframe\"], default=\"matrix\"\n        Either \"matrix\" or \"dataframe\", passed on to patsy.\n\n    Attributes\n    ----------\n    design_info_ : [patsy.DesignInfo](https://patsy.readthedocs.io/en/latest/API-reference.html#patsy.DesignInfo)\n        A DesignInfo object holds metadata about a design matrix.\n    \"\"\"\n\n    def __init__(self, formula, return_type=\"matrix\"):\n        self.formula = formula\n        self.return_type = return_type\n\n    def fit(self, X, y=None):\n        \"\"\"Fits the transformer on input data `X` by constructing a design matrix given the `formula`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to fit.\n        y : array-like of shape (n_samples,), default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : PatsyTransformer\n            The fitted transformer.\n        \"\"\"\n        X_ = patsy.dmatrix(self.formula, X, NA_action=\"raise\", return_type=self.return_type)\n\n        # check the number of observations hasn't changed. This ought not to\n        # be necessary given NA_action='raise' above but just to be safe\n        if np.asarray(X_).shape[0] != np.asarray(X).shape[0]:\n            raise RuntimeError(\n                \"Number of observations has changed during fit. \"\n                \"This is likely because some rows have been removed \"\n                \"due to NA values.\"\n            )\n        self.design_info_ = X_.design_info\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform `X` by applying the fitted formula.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to transform.\n\n        Returns\n        -------\n        patsy.DesignMatrix | pd.DataFrame\n\n            - DesignMatrix if return_type=\"matrix\" (the default)\n            - pd.DataFrame if return_type=\"dataframe\"\n        \"\"\"\n        check_is_fitted(self, \"design_info_\")\n        try:\n            return patsy.build_design_matrices([self.design_info_], X, return_type=self.return_type)[0]\n        except patsy.PatsyError as e:\n            raise RuntimeError from e\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.patsytransformer.PatsyTransformer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fits the transformer on input data <code>X</code> by constructing a design matrix given the <code>formula</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to fit.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>PatsyTransformer</code> <p>The fitted transformer.</p> Source code in <code>sklego/preprocessing/patsytransformer.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fits the transformer on input data `X` by constructing a design matrix given the `formula`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to fit.\n    y : array-like of shape (n_samples,), default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : PatsyTransformer\n        The fitted transformer.\n    \"\"\"\n    X_ = patsy.dmatrix(self.formula, X, NA_action=\"raise\", return_type=self.return_type)\n\n    # check the number of observations hasn't changed. This ought not to\n    # be necessary given NA_action='raise' above but just to be safe\n    if np.asarray(X_).shape[0] != np.asarray(X).shape[0]:\n        raise RuntimeError(\n            \"Number of observations has changed during fit. \"\n            \"This is likely because some rows have been removed \"\n            \"due to NA values.\"\n        )\n    self.design_info_ = X_.design_info\n    return self\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.patsytransformer.PatsyTransformer.transform","title":"<code>transform(X)</code>","text":"<p>Transform <code>X</code> by applying the fitted formula.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to transform.</p> required <p>Returns:</p> Type Description <code>DesignMatrix | DataFrame</code> <ul> <li>DesignMatrix if return_type=\"matrix\" (the default)</li> <li>pd.DataFrame if return_type=\"dataframe\"</li> </ul> Source code in <code>sklego/preprocessing/patsytransformer.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform `X` by applying the fitted formula.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to transform.\n\n    Returns\n    -------\n    patsy.DesignMatrix | pd.DataFrame\n\n        - DesignMatrix if return_type=\"matrix\" (the default)\n        - pd.DataFrame if return_type=\"dataframe\"\n    \"\"\"\n    check_is_fitted(self, \"design_info_\")\n    try:\n        return patsy.build_design_matrices([self.design_info_], X, return_type=self.return_type)[0]\n    except patsy.PatsyError as e:\n        raise RuntimeError from e\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.randomadder.RandomAdder","title":"<code>sklego.preprocessing.randomadder.RandomAdder</code>","text":"<p>             Bases: <code>TrainOnlyTransformerMixin</code>, <code>BaseEstimator</code></p> <p>The <code>RandomAdder</code> transformer adds random noise to the input data.</p> <p>This class is designed to be used during the training phase and not for transforming test data. Noise added is sampled from a normal distribution with mean 0 and standard deviation <code>noise</code>.</p> <p>Parameters:</p> Name Type Description Default <code>noise</code> <code>float</code> <p>The standard deviation of the normal distribution from which the noise is sampled.</p> <code>1.0</code> <code>random_state</code> <code>int | None</code> <p>The seed used by the random number generator.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>n_features_in_</code> <code>int</code> <p>Number of features seen during <code>fit</code>.</p> <code>dim_</code> <code>int</code> <p>Deprecated, please use <code>n_features_in_</code> instead.</p> <p>Examples:</p> <pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklego.preprocessing import RandomAdder\n\n# Create a pipeline with the RandomAdder and a LinearRegression model\npipeline = Pipeline([\n    ('random_adder', RandomAdder(noise=0.5, random_state=42)),\n    ('linear_regression', LinearRegression())\n])\n\n# Fit the pipeline with training data\npipeline.fit(X_train, y_train)\n\n# Use the fitted pipeline to make predictions\ny_pred = pipeline.predict(X_test)\n</code></pre> Source code in <code>sklego/preprocessing/randomadder.py</code> <pre><code>class RandomAdder(TrainOnlyTransformerMixin, BaseEstimator):\n    \"\"\"The `RandomAdder` transformer adds random noise to the input data.\n\n    This class is designed to be used during the training phase and not for transforming test data.\n    Noise added is sampled from a normal distribution with mean 0 and standard deviation `noise`.\n\n    Parameters\n    ----------\n    noise : float, default=1.0\n        The standard deviation of the normal distribution from which the noise is sampled.\n    random_state : int | None\n        The seed used by the random number generator.\n\n    Attributes\n    ----------\n    n_features_in_ : int\n        Number of features seen during `fit`.\n    dim_ : int\n        Deprecated, please use `n_features_in_` instead.\n\n    Examples\n    --------\n    ```py\n    from sklearn.pipeline import Pipeline\n    from sklearn.linear_model import LinearRegression\n    from sklego.preprocessing import RandomAdder\n\n    # Create a pipeline with the RandomAdder and a LinearRegression model\n    pipeline = Pipeline([\n        ('random_adder', RandomAdder(noise=0.5, random_state=42)),\n        ('linear_regression', LinearRegression())\n    ])\n\n    # Fit the pipeline with training data\n    pipeline.fit(X_train, y_train)\n\n    # Use the fitted pipeline to make predictions\n    y_pred = pipeline.predict(X_test)\n    ```\n    \"\"\"\n\n    def __init__(self, noise=1, random_state=None):\n        self.noise = noise\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \"\"\"Fit the transformer on training data `X` and `y` by checking the input data and record the number of\n        input features.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : RandomAdder\n            The fitted transformer.\n        \"\"\"\n        super().fit(X, y)\n        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n        self.n_features_in_ = X.shape[1]\n\n        return self\n\n    def transform_train(self, X):\n        r\"\"\"Transform training data by adding random noise sampled from $N(0, \\text{noise})$.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data for which the noise will be added.\n\n        Returns\n        -------\n        np.ndarray of shape (n_samples, n_features)\n            The data with the noise added.\n        \"\"\"\n        rs = check_random_state(self.random_state)\n        check_is_fitted(self, [\"n_features_in_\"])\n\n        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n\n        return X + rs.normal(0, self.noise, size=X.shape)\n\n    @property\n    def dim_(self):\n        warn(\n            \"Please use `n_features_in_` instead of `dim_`, `dim_` will be deprecated in future versions\",\n            DeprecationWarning,\n        )\n        return self.n_features_in_\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.randomadder.RandomAdder.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the transformer on training data <code>X</code> and <code>y</code> by checking the input data and record the number of input features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Training data.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>RandomAdder</code> <p>The fitted transformer.</p> Source code in <code>sklego/preprocessing/randomadder.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the transformer on training data `X` and `y` by checking the input data and record the number of\n    input features.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training data.\n    y : array-like of shape (n_samples,)\n        Target values.\n\n    Returns\n    -------\n    self : RandomAdder\n        The fitted transformer.\n    \"\"\"\n    super().fit(X, y)\n    X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n    self.n_features_in_ = X.shape[1]\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.randomadder.RandomAdder.transform_train","title":"<code>transform_train(X)</code>","text":"<p>Transform training data by adding random noise sampled from \\(N(0, \\text{noise})\\).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data for which the noise will be added.</p> required <p>Returns:</p> Type Description <code>np.ndarray of shape (n_samples, n_features)</code> <p>The data with the noise added.</p> Source code in <code>sklego/preprocessing/randomadder.py</code> <pre><code>def transform_train(self, X):\n    r\"\"\"Transform training data by adding random noise sampled from $N(0, \\text{noise})$.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data for which the noise will be added.\n\n    Returns\n    -------\n    np.ndarray of shape (n_samples, n_features)\n        The data with the noise added.\n    \"\"\"\n    rs = check_random_state(self.random_state)\n    check_is_fitted(self, [\"n_features_in_\"])\n\n    X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n\n    return X + rs.normal(0, self.noise, size=X.shape)\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.repeatingbasis.RepeatingBasisFunction","title":"<code>sklego.preprocessing.repeatingbasis.RepeatingBasisFunction</code>","text":"<p>             Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>The <code>RepeatingBasisFunction</code> transformer is designed to be used when the input data has a circular nature.</p> <p>For example, for days of the week you might face the problem that, conceptually, day 7 is as close to day 6 as it is to day 1. While numerically their distance is different.</p> <p>This transformer remedies that problem. The transformer selects a column and transforms it with a given number of repeating (radial) basis functions.</p> <p>Radial basis functions are bell-curve shaped functions which take the original data as input. The basis functions are equally spaced over the input range. The key feature of repeating basis functions is that they are continuous when moving from the max to the min of the input range. As a result these repeating basis functions can capture how close each datapoint is to the center of each repeating basis function, even when the input data has a circular nature.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>int | str</code> <p>Index or column name of the data to transform. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name.</p> <code>0</code> <code>remainder</code> <code>Literal[drop, passthrough]</code> <p>By default, only the specified column is transformed, and the non-specified columns are dropped. By specifying <code>remainder=\"passthrough\"</code>, all remaining columns will be automatically passed through. This subset of columns is concatenated with the output of the transformer.</p> <code>\"drop\"</code> <code>n_periods</code> <code>int</code> <p>Number of basis functions to create, i.e., the number of columns that will exit the transformer.</p> <code>12</code> <code>input_range</code> <code>Tuple[float, float] | List[float] | None</code> <p>The values at which the data repeats itself. For example, for days of the week this is (1,7). If <code>input_range=None</code> it is inferred from the training data.</p> <code>None</code> <code>width</code> <code>float</code> <p>Determines the width of the radial basis functions.</p> <code>1.0.</code> <p>Attributes:</p> Name Type Description <code>pipeline_</code> <code>ColumnTransformer</code> <p>Fitted <code>ColumnTransformer</code> object used to transform data with repeating basis functions.</p> Source code in <code>sklego/preprocessing/repeatingbasis.py</code> <pre><code>class RepeatingBasisFunction(TransformerMixin, BaseEstimator):\n    \"\"\"The `RepeatingBasisFunction` transformer is designed to be used when the input data has a circular nature.\n\n    For example, for days of the week you might face the problem that, conceptually, day 7 is as close to day 6 as it is\n    to day 1. While numerically their distance is different.\n\n    This transformer remedies that problem. The transformer selects a column and transforms it with a given number of\n    repeating (radial) basis functions.\n\n    Radial basis functions are bell-curve shaped functions which take the original data as input. The basis functions\n    are equally spaced over the input range. The key feature of repeating basis functions is that they are continuous\n    when moving from the max to the min of the input range. As a result these repeating basis functions can capture how\n    close each datapoint is to the center of each repeating basis function, even when the input data has a circular\n    nature.\n\n    Parameters\n    ----------\n    column : int | str, default=0\n        Index or column name of the data to transform. Integers are interpreted as positional columns, while\n        strings can reference DataFrame columns by name.\n    remainder : Literal[\"drop\", \"passthrough\"], default=\"drop\"\n        By default, only the specified column is transformed, and the non-specified columns are dropped.\n        By specifying `remainder=\"passthrough\"`, all remaining columns will be automatically passed through.\n        This subset of columns is concatenated with the output of the transformer.\n    n_periods : int, default=12\n        Number of basis functions to create, i.e., the number of columns that will exit the transformer.\n    input_range : Tuple[float, float] | List[float] | None, default=None\n        The values at which the data repeats itself. For example, for days of the week this is (1,7).\n        If `input_range=None` it is inferred from the training data.\n    width : float, default=1.0.\n        Determines the width of the radial basis functions.\n\n    Attributes\n    ----------\n    pipeline_ : ColumnTransformer\n        Fitted `ColumnTransformer` object used to transform data with repeating basis functions.\n    \"\"\"\n\n    def __init__(self, column=0, remainder=\"drop\", n_periods=12, input_range=None, width=1.0):\n        self.column = column\n        self.remainder = remainder\n        self.n_periods = n_periods\n        self.input_range = input_range\n        self.width = width\n\n    def fit(self, X, y=None):\n        \"\"\"Fit `RepeatingBasisFunction` transformer on input data `X`.\n        It uses `sklearn.compose.ColumnTransformer`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data used to compute the quantiles for capping.\n        y : array-like of shape (n_samples,), default=None\n            Ignored, present for compatibility.\n\n        Returns\n        -------\n        self : RepeatingBasisFunction\n            The fitted transformer.\n        \"\"\"\n        self.pipeline_ = ColumnTransformer(\n            [\n                (\n                    \"repeatingbasis\",\n                    _RepeatingBasisFunction(\n                        n_periods=self.n_periods,\n                        input_range=self.input_range,\n                        width=self.width,\n                    ),\n                    [self.column],\n                )\n            ],\n            remainder=self.remainder,\n        )\n\n        self.pipeline_.fit(X, y)\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform input data `X` with fitted `RepeatingBasisFunction` transformer.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to transform.\n\n        Returns\n        -------\n        X_transformed : array-like of shape (n_samples, n_periods)\n            Transformed data.\n        \"\"\"\n        check_is_fitted(self, [\"pipeline_\"])\n        return self.pipeline_.transform(X)\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.repeatingbasis.RepeatingBasisFunction.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit <code>RepeatingBasisFunction</code> transformer on input data <code>X</code>. It uses <code>sklearn.compose.ColumnTransformer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data used to compute the quantiles for capping.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Ignored, present for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>RepeatingBasisFunction</code> <p>The fitted transformer.</p> Source code in <code>sklego/preprocessing/repeatingbasis.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit `RepeatingBasisFunction` transformer on input data `X`.\n    It uses `sklearn.compose.ColumnTransformer`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data used to compute the quantiles for capping.\n    y : array-like of shape (n_samples,), default=None\n        Ignored, present for compatibility.\n\n    Returns\n    -------\n    self : RepeatingBasisFunction\n        The fitted transformer.\n    \"\"\"\n    self.pipeline_ = ColumnTransformer(\n        [\n            (\n                \"repeatingbasis\",\n                _RepeatingBasisFunction(\n                    n_periods=self.n_periods,\n                    input_range=self.input_range,\n                    width=self.width,\n                ),\n                [self.column],\n            )\n        ],\n        remainder=self.remainder,\n    )\n\n    self.pipeline_.fit(X, y)\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#sklego.preprocessing.repeatingbasis.RepeatingBasisFunction.transform","title":"<code>transform(X)</code>","text":"<p>Transform input data <code>X</code> with fitted <code>RepeatingBasisFunction</code> transformer.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The data to transform.</p> required <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>array-like of shape (n_samples, n_periods)</code> <p>Transformed data.</p> Source code in <code>sklego/preprocessing/repeatingbasis.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform input data `X` with fitted `RepeatingBasisFunction` transformer.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to transform.\n\n    Returns\n    -------\n    X_transformed : array-like of shape (n_samples, n_periods)\n        Transformed data.\n    \"\"\"\n    check_is_fitted(self, [\"pipeline_\"])\n    return self.pipeline_.transform(X)\n</code></pre>"},{"location":"user-guide/cross-validation/","title":"Cross Validation","text":""},{"location":"user-guide/cross-validation/#timegapsplit","title":"TimeGapSplit","text":"<p>We allow for a timeseries split that contains a gap.</p> <p>You won't always need it, but sometimes you consider these two situations;</p> <ul> <li>If you have multiple samples per timestamp: you want to make sure that a timestamp doesn\u2019t appear at the same time in training and validation folds.</li> <li> <p>If your target is looking \\(x\\) days ahead in the future. In this case you cannot construct the target of the last \\(x\\) days of your available data. It means that when you put your model in production, the first day that you are going to score is always \\(x\\) days after your last training sample, therefore you should select the best model according to that setup.</p> <p>In other words, if you keep that gap in the validation, your metric might be overestimated because those first \\(x\\) days might be easier to predict since they are closer to the training set. If you want to be strict in terms of robustness you might want to replicate in the CV exactly this real-world behaviour, and thus you want to introduce a gap of x days between your training and validation folds.</p> </li> </ul> <p><code>TimeGapSplit</code> provides 4 parameters to really reproduce your production implementation in your cross-validation schema. We will demonstrate this in a code example below.</p>"},{"location":"user-guide/cross-validation/#examples","title":"Examples","text":"<p>Let's make some random data to start with, and next define a plotting function.</p> <pre><code>from datetime import timedelta\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme()\n\nfrom sklego.model_selection import TimeGapSplit\n\n# Plotting helper function\ndef plot_cv(cv, X):\n    \"\"\"Plot all the folds on time axis\"\"\"\n    X_index_df = cv._join_date_and_x(X)\n\n    plt.figure(figsize=(16, 4))\n    for i, split in enumerate(cv.split(X)):\n        x_idx, y_idx = split\n        x_dates = X_index_df.iloc[x_idx][\"__date__\"].unique()\n        y_dates = X_index_df.iloc[y_idx][\"__date__\"].unique()\n        plt.plot(x_dates, i*np.ones(x_dates.shape), c=\"steelblue\")\n        plt.plot(y_dates, i*np.ones(y_dates.shape), c=\"orange\")\n\n    plt.legend((\"training\", \"validation\"), loc=\"upper left\")\n    plt.ylabel(\"Fold id\")\n    plt.axvline(x=X_index_df[\"__date__\"].min(), color=\"gray\", label=\"x\")\n    plt.axvline(x=X_index_df[\"__date__\"].max(), color=\"gray\", label=\"d\")\n\n# Random data creation\ndf = (pd.DataFrame(np.random.randint(0, 30, size=(30, 4)), columns=list(\"ABCy\"))\n      .assign(date=pd.date_range(start=\"1/1/2018\", end=\"1/30/2018\")[::-1]))\n\nprint(df.shape)\n# (30, 5)\n\nprint(df.head())\n</code></pre> A B C y date 28 9 24 5 2018-01-30 00:00:00 5 0 19 1 2018-01-29 00:00:00 8 1 29 2 2018-01-28 00:00:00 11 4 21 19 2018-01-27 00:00:00 19 26 6 2 2018-01-26 00:00:00 Example 1<pre><code>cv = TimeGapSplit(\n    date_serie=df[\"date\"],\n    train_duration=timedelta(days=10),\n    valid_duration=timedelta(days=2),\n    gap_duration=timedelta(days=1)\n)\n\nplot_cv(cv, df)\n</code></pre> <p></p> Example 2<pre><code>cv = TimeGapSplit(\n    date_serie=df[\"date\"],\n    train_duration=timedelta(days=10),\n    valid_duration=timedelta(days=5),\n    gap_duration=timedelta(days=1)\n)\n\nplot_cv(cv, df)\n</code></pre> <p></p> <p><code>window=\"expanding\"</code> is the closest to scikit-learn implementation:</p> Example 3<pre><code>cv = TimeGapSplit(\n    date_serie=df[\"date\"],\n    train_duration=timedelta(days=10),\n    valid_duration=timedelta(days=2),\n    gap_duration=timedelta(days=1),\n    window=\"expanding\"\n)\n\nplot_cv(cv, df)\n</code></pre> <p></p> <p>If <code>train_duration</code> is not passed the training duration is the maximum without overlapping validation folds:</p> Example 4<pre><code>cv = TimeGapSplit(\n    date_serie=df[\"date\"],\n    train_duration=None,\n    valid_duration=timedelta(days=3),\n    gap_duration=timedelta(days=2),\n    n_splits=3\n)\n\nplot_cv(cv, df)\n</code></pre> <p></p> <p>If train and valid duration would lead to unwanted amounts of splits n_splits can set a maximal amount of splits</p> Example 5<pre><code>cv = TimeGapSplit(\n    date_serie=df[\"date\"],\n    train_duration=timedelta(days=10),\n    valid_duration=timedelta(days=2),\n    gap_duration=timedelta(days=1),\n    n_splits=4\n)\n\nplot_cv(cv, df)\n</code></pre> <p></p> Summary<pre><code>cv.summary(df)\n</code></pre> Start date End date Period Unique days nbr samples 2018-01-01 00:00:00 2018-01-10 00:00:00 9 days 00:00:00 10 10 2018-01-12 00:00:00 2018-01-13 00:00:00 1 days 00:00:00 2 2 2018-01-06 00:00:00 2018-01-15 00:00:00 9 days 00:00:00 10 10 2018-01-17 00:00:00 2018-01-18 00:00:00 1 days 00:00:00 2 2 2018-01-10 00:00:00 2018-01-19 00:00:00 9 days 00:00:00 10 10 2018-01-21 00:00:00 2018-01-22 00:00:00 1 days 00:00:00 2 2 2018-01-15 00:00:00 2018-01-24 00:00:00 9 days 00:00:00 10 10 2018-01-26 00:00:00 2018-01-27 00:00:00 1 days 00:00:00 2 2"},{"location":"user-guide/cross-validation/#grouptimeseriessplit","title":"GroupTimeSeriesSplit","text":"<p>In a time series problem it is possible that not every time unit (e.g. years) has the same amount of rows/observations. This makes a normal kfold split impractical as you cannot specify a certain timeframe per fold (e.g. 5 years), because this can cause the folds' sizes to be very different.</p> <p>With <code>GroupTimeSeriesSplit</code> you can specify the amount of folds you want (e.g. <code>n_splits=3</code>) and <code>GroupTimeSeriesSplit</code> will calculate itself folds in such a way that the amount of observations per fold are as similar as possible.</p> <p>The folds are created with a smartly modified brute forced method. This still means that for higher <code>n_splits</code> values in combination with many different unique time periods (e.g. 100 different years, thus 100 groups) the generation of the optimal split points can take minutes to hours.</p> <p>Info</p> <p><code>UserWarnings</code> are raised when <code>GroupTimeSeriesSplit</code> expects to be running over a minute. Of course, this actual runtime depends on your machine's specifications.</p>"},{"location":"user-guide/cross-validation/#examples_1","title":"Examples","text":"<p>First let's create an example data set:</p> <pre><code>import numpy as np\nimport pandas as pd\n\nX = np.random.randint(low=1, high=1000, size=17)\ny = np.random.randint(low=1, high=1000, size=17)\ngroups = np.array([2000,2000,2000,2001,2002,2002,2003,2004,2004,2004,2004,2004,2005,2005,2006,2006,2007])\n\ndf = pd.DataFrame(np.vstack((X,y)).T, index=groups, columns=['X','y'])\ndf.head(10)\n</code></pre> X y 583 481 414 617 669 627 812 604 800 248 966 503 719 650 476 939 743 170 142 893 <p>Create a <code>GroupTimeSeriesSplit</code> cross-validator with kfold/n_splits = 3:</p> <pre><code>from sklego.model_selection import GroupTimeSeriesSplit\ncv = GroupTimeSeriesSplit(n_splits=3)\n\ndef print_folds(cv, X, y, groups):\n    for kfold, (train, test) in enumerate(cv.split(X, y, groups)):\n        print(f\"Fold {kfold+1}:\")\n        print(f\"Train = {df.iloc[train].index.tolist()}\")\n        print(f\"Test = {df.iloc[test].index.tolist()}\\n\\n\")\n\nprint_folds(cv, X, y, groups)\n</code></pre> <pre><code>Fold 1:\nTrain = [2000, 2000, 2000, 2001]\nTest = [2002, 2002, 2003]\n\n\nFold 2:\nTrain = [2002, 2002, 2003]\nTest = [2004, 2004, 2004, 2004, 2004]\n\n\nFold 3:\nTrain = [2004, 2004, 2004, 2004, 2004]\nTest = [2005, 2005, 2006, 2006, 2007]\n</code></pre> <p></p> <p>As you can see above <code>GroupTimeSeriesSplit</code> keeps the order of the time chronological and makes sure that the same time value won't appear in both the train and test set of the same fold.</p> <p><code>GroupTimeSeriesSplit</code> also has the <code>.summary()</code> method, in which is shown which time values are grouped together. Because of the chronological order the train and test folds need to be, the amount of <code>groups</code> is always <code>n_splits</code> + 1. (see the four folds in the image above with <code>Kfold=3</code>)</p> Summary<pre><code>cv.summary()\n</code></pre> index observations group obs_per_group ideal_group_size diff_from_ideal_group_size 2000 3 0 4 4 0 2001 1 0 4 4 0 2002 2 1 3 4 -1 2003 1 1 3 4 -1 2004 5 2 5 4 1 2005 2 3 5 4 1 2006 2 3 5 4 1 2007 1 3 5 4 1 <p>To use <code>GroupTimeSeriesSplit</code> with sklearn's GridSearchCV:</p> <pre><code>from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\n# reshape X for the Lasso model\nX = X.reshape(-1,1)\n\n# initiate the cross validator\ncv = GroupTimeSeriesSplit(n_splits=3)\n\n# generate the train-test splits\ncv_splits = cv.split(X=X, y=y, groups=groups)\n\n# initiate the Lasso model\nLasso(random_state=0, tol=0.1, alpha=0.8).fit(X, y, groups)\npipe = Pipeline([(\"reg\", Lasso(random_state=0, tol=0.1))])\n\n\n# initiate GridSearchCv with cv_splits as parameter\nalphas = [0.1, 0.5, 0.8]\ngrid = GridSearchCV(pipe, {\"reg__alpha\": alphas}, cv=cv_splits)\ngrid.fit(X, y)\ngrid.best_estimator_.get_params()[\"reg__alpha\"]\n# 0.8\n</code></pre>"},{"location":"user-guide/datasets/","title":"Datasets","text":"<p>Scikit-lego includes several datasets which can be used for testing purposes. Each dataset has different options for returning the data:</p> <ul> <li>When setting <code>as_frame=True</code> the data, including the target, is returned as a (pandas) dataframe.</li> <li>When setting <code>return_X_y=True</code> the data is returned directly as <code>(data, target)</code> instead of a dict object.</li> </ul> <p>This notebook describes the different sets included in Scikit-lego:</p> <ul> <li><code>sklego.datasets.load_abalone</code> loads in the abalone dataset</li> <li><code>sklego.datasets.load_arrests</code> loads in a dataset with fairness concerns</li> <li><code>sklego.datasets.load_chicken</code> loads in the joyful chickweight dataset</li> <li><code>sklego.datasets.load_heroes</code> loads a heroes of the storm dataset</li> <li><code>sklego.datasets.load_hearts</code> loads a dataset about hearts</li> <li><code>sklego.datasets.load_penguins</code> loads a lovely dataset about penguins</li> <li><code>sklego.datasets.fetch_creditcard</code> fetch a fraud dataset from openml</li> <li><code>sklego.datasets.make_simpleseries</code> make a simulated timeseries</li> </ul>"},{"location":"user-guide/datasets/#imports","title":"Imports","text":"<p>Loads necessary imports used in rest of the code snippets.</p> <pre><code>from collections import Counter\n\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nsns.set_theme()\n</code></pre>"},{"location":"user-guide/datasets/#abalone","title":"Abalone","text":"<p>Loads the abalone dataset where the goal is to predict the gender of the creature.</p> <pre><code>from sklego.datasets import load_abalone\n\ndf_abalone = load_abalone(as_frame=True)\ndf_abalone.head()\n</code></pre> sex length diameter height whole_weight shucked_weight viscera_weight shell_weight rings M 0.455 0.365 0.095 0.514 0.2245 0.101 0.15 15 M 0.35 0.265 0.09 0.2255 0.0995 0.0485 0.07 7 F 0.53 0.42 0.135 0.677 0.2565 0.1415 0.21 9 M 0.44 0.365 0.125 0.516 0.2155 0.114 0.155 10 I 0.33 0.255 0.08 0.205 0.0895 0.0395 0.055 7 <pre><code>X, y = load_abalone(return_X_y=True)\n\nplt.bar(Counter(y).keys(), Counter(y).values())\nplt.title(\"Distribution of sex (target)\")\n</code></pre> <p></p>"},{"location":"user-guide/datasets/#arrests","title":"Arrests","text":"<p>Loads the arrests dataset which can serve as a benchmark for fairness.</p> <p>It is data on the police treatment of individuals arrested in Toronto for simple possession of small quantities of marijuana.</p> <p>The goal is to predict whether or not the arrestee was released with a summons while maintaining a degree of fairness.</p> <pre><code>from sklego.datasets import load_arrests\n\ndf_arrests = load_arrests(as_frame=True)\ndf_arrests.head()\n</code></pre> released colour year age sex employed citizen checks Yes White 2002 21 Male Yes Yes 3 No Black 1999 17 Male Yes Yes 3 Yes White 2000 24 Male Yes Yes 3 No Black 2000 46 Male Yes Yes 1 Yes Black 1999 27 Female Yes Yes 1 <pre><code>X, y = load_arrests(return_X_y=True)\n\nplt.bar(Counter(y).keys(), Counter(y).values())\nplt.title(\"Distribution of released (target)\")\n</code></pre> <p></p>"},{"location":"user-guide/datasets/#chickens","title":"Chickens","text":"<p>Loads the chicken dataset. The data has 578 rows and 4 columns from an experiment on the effect of diet on early growth of chicks.</p> <p>The body weights of the chicks were measured at birth and every second day thereafter until day 20. They were also measured on day 21.</p> <p>There were four groups on chicks on different protein diets.</p> <pre><code>from sklego.datasets import load_chicken\n\ndf_chicken = load_chicken(as_frame=True)\ndf_chicken.head()\n</code></pre> weight time chick diet 42 0 1 1 51 2 1 1 59 4 1 1 64 6 1 1 76 8 1 1 <pre><code>X, y = load_chicken(return_X_y=True)\n\nplt.hist(y)\nplt.title(\"Distribution of weight (target)\")\n</code></pre> <p></p>"},{"location":"user-guide/datasets/#hearts","title":"Hearts","text":"<p>Loads the Cleveland Heart Diseases dataset. The goal is to predict the presence of a heart disease (target values 1, 2, 3, and 4).</p> <p>The data originates from research to heart diseases by four institutions and originally contains 76 attributes. Yet, all published experiments refer to using a subset of 13 features and one target.</p> <p>This implementation loads the Cleveland dataset of the research which is the only set used by ML researchers to this date.</p> <pre><code>from sklego.datasets import load_hearts\n\ndf_hearts = load_hearts(as_frame=True)\ndf_hearts.head()\n</code></pre> age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target 63 1 1 145 233 1 2 150 0 2.3 3 0 fixed 0 67 1 4 160 286 0 2 108 1 1.5 2 3 normal 1 67 1 4 120 229 0 2 129 1 2.6 2 2 reversible 0 37 1 3 130 250 0 0 187 0 3.5 3 0 normal 0 41 0 2 130 204 0 2 172 0 1.4 1 0 normal 0 <pre><code>X, y = load_hearts(return_X_y=True)\n\nplt.bar(Counter(y).keys(), Counter(y).values())\nplt.title(\"Distribution of presence of heart disease (target)\")\n</code></pre> <p></p>"},{"location":"user-guide/datasets/#heroes","title":"Heroes","text":"<p>A dataset from the video game Heroes of the storm.</p> <p>The goal of the dataset is to predict the attack type.</p> <p>Note that the pandas dataset returns more information.</p> <pre><code>from sklego.datasets import load_heroes\n\ndf_heroes = load_heroes(as_frame=True)\ndf_heroes.head()\n</code></pre> name attack_type role health attack attack_spd Artanis Melee Bruiser 2470 111 1 Chen Melee Bruiser 2473 90 1.11 Dehaka Melee Bruiser 2434 100 1.11 Imperius Melee Bruiser 2450 122 0.83 Leoric Melee Bruiser 2550 109 0.77 <pre><code>X, y = load_heroes(return_X_y=True)\n\nplt.bar(Counter(y).keys(), Counter(y).values())\nplt.title(\"Distribution of attack_type (target)\")\n</code></pre> <p></p>"},{"location":"user-guide/datasets/#penguins","title":"Penguins","text":"<p>Loads the penguins dataset, which is a lovely alternative for the iris dataset. We\"ve added this dataset for educational use.</p> <p>Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.</p> <p>The goal of the dataset is to predict which species of penguin a penguin belongs to.</p> <pre><code>from sklego.datasets import load_penguins\n\ndf_penguins = load_penguins(as_frame=True)\ndf_penguins.head()\n</code></pre> species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex Adelie Torgersen 39.1 18.7 181 3750 male Adelie Torgersen 39.5 17.4 186 3800 female Adelie Torgersen 40.3 18 195 3250 female Adelie Torgersen nan nan nan nan nan Adelie Torgersen 36.7 19.3 193 3450 female <pre><code>X, y = load_penguins(return_X_y=True)\n\nplt.bar(Counter(y).keys(), Counter(y).values())\nplt.title(\"Distribution of species (target)\")\n</code></pre> <p></p>"},{"location":"user-guide/datasets/#creditcard-frauds","title":"Creditcard frauds","text":"<p>Loads the creditcard dataset. Downloads it if necessary.</p> <p>Note that internally this is using <code>sklearn.datasets.fetch_openml</code>, which is experimental.</p> <pre><code>==============   ==============\nSamples total            284807\nDimensionality               29\nFeatures                   real\nTarget                 int 0, 1\n==============   ==============\n</code></pre> <p>The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset present transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions.</p> <p>The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.</p> <p>Please cite:     <pre><code>Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi.\nCalibrating Probability with Undersampling for Unbalanced Classification.\nIn Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n</code></pre></p> <pre><code>from sklego.datasets import fetch_creditcard\n\ndict_creditcard = fetch_creditcard(as_frame=True)\ndf_creditcard = dict_creditcard[\"frame\"]\ndf_creditcard.head()\n</code></pre> V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 Amount Class -1.35981 -0.0727812 2.53635 1.37816 -0.338321 0.462388 0.239599 0.0986979 0.363787 0.0907942 -0.5516 -0.617801 -0.99139 -0.311169 1.46818 -0.470401 0.207971 0.0257906 0.403993 0.251412 -0.0183068 0.277838 -0.110474 0.0669281 0.128539 -0.189115 0.133558 -0.0210531 149.62 0 1.19186 0.266151 0.16648 0.448154 0.0600176 -0.0823608 -0.078803 0.0851017 -0.255425 -0.166974 1.61273 1.06524 0.489095 -0.143772 0.635558 0.463917 -0.114805 -0.183361 -0.145783 -0.0690831 -0.225775 -0.638672 0.101288 -0.339846 0.16717 0.125895 -0.0089831 0.0147242 2.69 0 -1.35835 -1.34016 1.77321 0.37978 -0.503198 1.8005 0.791461 0.247676 -1.51465 0.207643 0.624501 0.0660837 0.717293 -0.165946 2.34586 -2.89008 1.10997 -0.121359 -2.26186 0.52498 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.0553528 -0.0597518 378.66 0 -0.966272 -0.185226 1.79299 -0.863291 -0.0103089 1.2472 0.237609 0.377436 -1.38702 -0.0549519 -0.226487 0.178228 0.507757 -0.287924 -0.631418 -1.05965 -0.684093 1.96578 -1.23262 -0.208038 -0.1083 0.0052736 -0.190321 -1.17558 0.647376 -0.221929 0.0627228 0.0614576 123.5 0 -1.15823 0.877737 1.54872 0.403034 -0.407193 0.0959215 0.592941 -0.270533 0.817739 0.753074 -0.822843 0.538196 1.34585 -1.11967 0.175121 -0.451449 -0.237033 -0.0381948 0.803487 0.408542 -0.0094307 0.798278 -0.137458 0.141267 -0.20601 0.502292 0.219422 0.215153 69.99 0 <pre><code>X, y = dict_creditcard[\"data\"], dict_creditcard[\"target\"]\n\nplt.bar(Counter(y).keys(), Counter(y).values())\nplt.title(\"Distribution of fraud (target)\")\n</code></pre> <p></p>"},{"location":"user-guide/datasets/#simpleseries","title":"Simpleseries","text":"<p>Generate a very simple timeseries dataset to play with. The generator assumes to generate daily data with a season, trend and noise.</p> <pre><code>from sklego.datasets import make_simpleseries\n\ndf_simpleseries = make_simpleseries(as_frame=True, n_samples=1500, trend=0.001)\ndf_simpleseries.head()\n</code></pre> yt 0 -0.335058 1 -0.283375 2 0.521791 3 0.50202 4 0.310048 <pre><code>plt.plot(df_simpleseries[\"yt\"])\nplt.title(\"Timeseries yt\")\n</code></pre> <p></p>"},{"location":"user-guide/debug-pipeline/","title":"Debug pipeline","text":"<p>This document demonstrates how you might use a <code>DebugPipeline</code>. It is much like a normal scikit-learn <code>Pipeline</code> but it offers more debugging options.</p> <p>We'll first set up libraries and config.</p> Setup<pre><code>import logging\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklego.pipeline import DebugPipeline\n\nlogging.basicConfig(\n    format=(\"[%(funcName)s:%(lineno)d] - %(message)s\"),\n    level=logging.INFO\n)\n</code></pre> <p>Next up, let's make a simple transformer.</p> Simple transformer<pre><code>n_samples, n_features = 3, 5\nX = np.zeros((n_samples, n_features))\ny = np.arange(n_samples)\n\n\nclass Adder(TransformerMixin, BaseEstimator):\n    def __init__(self, value):\n        self._value = value\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X + self._value\n\n    def __repr__(self):\n        return f\"Adder(value={self._value})\"\n\n\nsteps = [\n    (\"add_1\", Adder(value=1)),\n    (\"add_10\", Adder(value=10)),\n    (\"add_100\", Adder(value=100)),\n    (\"add_1000\", Adder(value=1000)),\n]\n</code></pre> <p>This pipeline behaves exactly the same as a normal pipeline. So let's use it.</p> Simple transformer<pre><code>pipe = DebugPipeline(steps)\n_ = pipe.fit(X, y=y)\n\nX_out = pipe.transform(X)\nprint(\"Transformed X:\\n\", X_out)\n</code></pre> <pre><code>Transformed X:\n [[1111. 1111. 1111. 1111. 1111.]\n [1111. 1111. 1111. 1111. 1111.]\n [1111. 1111. 1111. 1111. 1111.]]\n</code></pre>"},{"location":"user-guide/debug-pipeline/#log-statements","title":"Log statements","text":"<p>It is possible to set a <code>log_callback</code> variable that logs in between each step.</p> <p>Note</p> <p>There are three log statements while there are four steps, because there are three moments in between the steps. The output can be checked outside of the pipeline.</p> 'default' log_callback<pre><code>pipe = DebugPipeline(steps, log_callback=\"default\")\n_ = pipe.fit(X, y=y)\n\nX_out = pipe.transform(X)\nprint(\"Transformed X:\\n\", X_out)\n</code></pre> <pre><code>[default_log_callback:38] - [Adder(value=1)] shape=(3, 5) time=0s\n[default_log_callback:38] - [Adder(value=10)] shape=(3, 5) time=0s\n[default_log_callback:38] - [Adder(value=100)] shape=(3, 5) time=0s\nTransformed X:\n [[1111. 1111. 1111. 1111. 1111.]\n [1111. 1111. 1111. 1111. 1111.]\n [1111. 1111. 1111. 1111. 1111.]]\n</code></pre>"},{"location":"user-guide/debug-pipeline/#set-the-log_callback-function-later","title":"Set the <code>log_callback</code> function later","text":"<p>It is possible to set the <code>log_callback</code> later then initialisation.</p> log_callback after initialisation<pre><code>pipe = DebugPipeline(steps)\npipe.log_callback = \"default\"\n\n_ = pipe.fit(X, y=y)\n\nX_out = pipe.transform(X)\nprint(\"Transformed X:\\n\", X_out)\n</code></pre> <pre><code>[default_log_callback:38] - [Adder(value=1)] shape=(3, 5) time=0s\n[default_log_callback:38] - [Adder(value=10)] shape=(3, 5) time=0s\n[default_log_callback:38] - [Adder(value=100)] shape=(3, 5) time=0s\nTransformed X:\n [[1111. 1111. 1111. 1111. 1111.]\n [1111. 1111. 1111. 1111. 1111.]\n [1111. 1111. 1111. 1111. 1111.]]\n</code></pre>"},{"location":"user-guide/debug-pipeline/#custom-log_callback","title":"Custom <code>log_callback</code>","text":"<p>The custom log callback function expect the output of each step, which is an tuple containing the output of the step and the step itself, and the execution time of the step.</p> Custom log_callback<pre><code>def log_callback(output, execution_time, **kwargs):\n    \"\"\"My custom `log_callback` function\n\n    Parameters\n    ----------\n    output : tuple(\n            numpy.ndarray or pandas.DataFrame\n            :class:estimator or :class:transformer\n        )\n        The output of the step and a step in the pipeline.\n    execution_time : float\n        The execution time of the step.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    step_result, step = output\n    logger.info(f\"[{step}] shape={step_result.shape} \"\n                f\"nbytes={step_result.nbytes} time={execution_time}\")\n\n\npipe.log_callback = log_callback\n_ = pipe.fit(X, y=y)\n\nX_out = pipe.transform(X)\nprint(\"Transformed X:\\n\", X_out)\n</code></pre> <pre><code>[log_callback:16] - [Adder(value=1)] shape=(3, 5) nbytes=120 time=5.340576171875e-05\n[log_callback:16] - [Adder(value=10)] shape=(3, 5) nbytes=120 time=6.651878356933594e-05\n[log_callback:16] - [Adder(value=100)] shape=(3, 5) nbytes=120 time=6.723403930664062e-05\nTransformed X:\n [[1111. 1111. 1111. 1111. 1111.]\n [1111. 1111. 1111. 1111. 1111.]\n [1111. 1111. 1111. 1111. 1111.]]\n</code></pre>"},{"location":"user-guide/debug-pipeline/#feature-union","title":"Feature union","text":"<p>Feature union also works with the debug pipeline.</p> Feature union<pre><code>from sklearn.pipeline import FeatureUnion\n\npipe_w_default_log_callback = DebugPipeline(steps, log_callback='default')\npipe_w_custom_log_callback = DebugPipeline(steps, log_callback=log_callback)\n\npipe_union = FeatureUnion([\n    ('pipe_w_default_log_callback', pipe_w_default_log_callback),\n    ('pipe_w_custom_log_callback', pipe_w_custom_log_callback),\n])\n\n_ = pipe_union.fit(X, y=y)\n\nX_out = pipe_union.transform(X)\nprint('Transformed X:\\n', X_out)\n</code></pre> <pre><code>[default_log_callback:38] - [Adder(value=1)] shape=(3, 5) time=0s\n[default_log_callback:38] - [Adder(value=10)] shape=(3, 5) time=0s\n[default_log_callback:38] - [Adder(value=100)] shape=(3, 5) time=0s\n[log_callback:16] - [Adder(value=1)] shape=(3, 5) nbytes=120 time=4.482269287109375e-05\n[log_callback:16] - [Adder(value=10)] shape=(3, 5) nbytes=120 time=5.1021575927734375e-05\n[log_callback:16] - [Adder(value=100)] shape=(3, 5) nbytes=120 time=6.365776062011719e-05\nTransformed X:\n [[1111. 1111. 1111. 1111. 1111. 1111. 1111. 1111. 1111. 1111.]\n [1111. 1111. 1111. 1111. 1111. 1111. 1111. 1111. 1111. 1111.]\n [1111. 1111. 1111. 1111. 1111. 1111. 1111. 1111. 1111. 1111.]]\n</code></pre>"},{"location":"user-guide/debug-pipeline/#enough-logging","title":"Enough logging","text":"<p>Remove the <code>log_callback</code> function when not needed anymore.</p> Remove log_callback<pre><code>pipe.log_callback = None\n_ = pipe.fit(X, y=y)\n\nX_out = pipe.transform(X)\nprint('Transformed X:\\n', X_out)\n</code></pre> <pre><code>Transformed X:\n [[1111. 1111. 1111. 1111. 1111.]\n [1111. 1111. 1111. 1111. 1111.]\n [1111. 1111. 1111. 1111. 1111.]]\n</code></pre>"},{"location":"user-guide/fairness/","title":"Fairness","text":"<p>Scikit learn (pre version 1.2) came with the boston housing dataset. We can make a simple pipeline with it and make us a small model. We can even write the code to also make a plot that can convince us that we're doing well:</p> Predict Boston housing dataset<pre><code>import matplotlib.pylab as plt\nimport seaborn as sns\nsns.set_theme()\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\nX, y = load_boston(return_X_y=True)\n\npipe = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"model\", LinearRegression())\n])\n\nplt.scatter(pipe.fit(X, y).predict(X), y)\nplt.xlabel(\"predictions\")\nplt.ylabel(\"actual\")\nplt.title(\"plot that suggests it's not bad\");\n</code></pre> <p></p> <p>We could stop our research here if we think that our MSE is good enough but this would be dangerous. To find out why, we should look at the variables that are being used in our model.</p> Boston housing dataset<pre><code>print(load_boston()[\"DESCR\"][:1233])\n</code></pre> <p>Boston housing description</p> <pre><code>.. _boston_dataset:\n\nBoston house prices dataset\n---------------------------\n\n**Data Set Characteristics:**  \n\n    :Number of Instances: 506 \n\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000's\n</code></pre> <p>This dataset contains features like \"lower status of population\" and \"the proportion of blacks by town\".</p> <p>Danger</p> <p>This is bad! There's a real possibility that our model will overfit on MSE and underfit on fairness when we want to apply it. Scikit-Lego has some support to deal with fairness issues like this one.</p> <p>Dealing with issues such as fairness in machine learning can in general be done in three ways:</p> <ul> <li>Data preprocessing</li> <li>Model constraints</li> <li>Prediction postprocessing</li> </ul> <p>Before we can dive into methods for getting more fair predictions, we first need to define how to measure fairness.</p>"},{"location":"user-guide/fairness/#measuring-fairness-for-regression","title":"Measuring fairness for Regression","text":"<p>Measuring fairness can be done in many ways but we'll consider one definition: the output of the model is fair with regards to groups \\(A\\) and \\(B\\) if prediction has a distribution independent of group \\(A\\) or \\(B\\).</p> <p>In laymans terms: if group \\(A\\) and \\(B\\) don't get the same predictions: no bueno.</p> <p>Formally, how much the means of the distributions differ can be written as:</p> \\[fairness = \\left\\lvert \\frac{1}{|Z_1|} \\sum_{i \\in Z_1} \\hat{y}_{i} - \\frac{1}{|Z_0|} \\sum_{j \\in Z_0} \\hat{y}_{j} \\right\\rvert\\] <p>where \\(Z_1\\) is the subset of the population where our sensitive attribute is true, and \\(Z_0\\) the subset of the population where the sensitive attribute is false.</p> <p>To estimate this we'll use bootstrap sampling to measure the models bias.</p>"},{"location":"user-guide/fairness/#measuring-fairness-for-classification","title":"Measuring fairness for Classification","text":"<p>A common method for measuring fairness is demographic parity<sup>1</sup>, for example through the p-percent metric.</p> <p>The idea is that a decision \u2014 such as accepting or denying a loan application \u2014 ought to be independent of the protected attribute. In other words, we expect the positive rate in both groups to be the same. In the case of a binary decision \\(\\hat{y}\\) and a binary protected attribute \\(z\\), this constraint can be formalized by asking that</p> \\[P(\\hat{y}=1 | z=0)=P(\\hat{y}=1 | z=1)\\] <p>You can turn this into a metric by calculating how far short the decision process falls of this exact equality. This metric is called the p% score</p> \\[\\text{p% score} = \\min \\left(\\frac{P(\\hat{y}=1 | z=1)}{P(\\hat{y}=1 | z=0)}, \\frac{P(\\hat{y}=1 | z=0)}{P(\\hat{y}=1 | z=1)}\\right)\\] <p>In other words, membership in a protected class should have no correlation with the decision.</p> <p>In <code>sklego</code> this metric is implemented in as <code>p_percent_score</code> and it works as follows:</p> p% score<pre><code>import pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklego.metrics import p_percent_score\n\nsensitive_classification_dataset = pd.DataFrame({\n    \"x1\": [1, 0, 1, 0, 1, 0, 1, 1],\n    \"x2\": [0, 0, 0, 0, 0, 1, 1, 1],\n    \"y\": [1, 1, 1, 0, 1, 0, 0, 0]}\n)\n\nX, y = sensitive_classification_dataset.drop(columns=\"y\"), sensitive_classification_dataset[\"y\"]\nmod_unfair = LogisticRegression(solver=\"lbfgs\").fit(X, y)\n\nprint(\"p_percent_score:\", p_percent_score(sensitive_column=\"x2\")(mod_unfair, X))\n</code></pre> <p>p_percent_score: 0</p> <p>Of course, no metric is perfect. If, for example, we used this in a loan approval situation the demographic parity only looks at loans made and not at the rate at which loans are repaid.</p> <p>That might result in a lower percentage of qualified people who are given loans in one population than in another. Another way of measuring fairness could therefore be to measure equal opportunity<sup>2</sup>, implemented in <code>sklego</code> as <code>equal_opportunity_score</code>. This constraint would boil down to:</p> \\[P(\\hat{y}=1 | z=0, y=1)=P(\\hat{y}=1 | z=1, y=1)\\] <p>and be turned into a metric in the same way as above:</p> \\[\\text{equality of opportunity} = \\min \\left(\\frac{P(\\hat{y}=1 | z=1, y=1)}{P(\\hat{y}=1 | z=0, y=1)}, \\frac{P(\\hat{y}=1 | z=0, y=1)}{P(\\hat{y}=1 | z=1, y=1)}\\right)\\] <p>We can see in the example below that the equal opportunity score does not differ for the models as long as the records where <code>y_true = 1</code> are predicted correctly.</p> equality opportunity score<pre><code>import numpy as np\nimport pandas as pd\nfrom sklego.metrics import equal_opportunity_score\nimport types\n\nsensitive_classification_dataset = pd.DataFrame({\n    \"x1\": [1, 0, 1, 0, 1, 0, 1, 1],\n    \"x2\": [0, 0, 0, 0, 0, 1, 1, 1],\n    \"y\": [1, 1, 1, 0, 1, 0, 0, 1]}\n)\n\nX, y = sensitive_classification_dataset.drop(columns=\"y\"), sensitive_classification_dataset[\"y\"]\n\nmod_1 = types.SimpleNamespace()\nmod_1.predict = lambda X: np.array([1, 0, 1, 0, 1, 0, 1, 1])\nprint(\"equal_opportunity_score:\", equal_opportunity_score(sensitive_column=\"x2\")(mod_1, X, y))\n\nmod_1.predict = lambda X: np.array([1, 0, 1, 0, 1, 0, 0, 1])\nprint(\"equal_opportunity_score:\", equal_opportunity_score(sensitive_column=\"x2\")(mod_1, X, y))\n\nmod_1.predict = lambda X: np.array([1, 0, 1, 0, 1, 0, 0, 0])\nprint(\"equal_opportunity_score:\", equal_opportunity_score(sensitive_column=\"x2\")(mod_1, X, y))\n</code></pre> <pre><code>equal_opportunity_score: 0.75\nequal_opportunity_score: 0.75\nequal_opportunity_score: 0.0\n</code></pre>"},{"location":"user-guide/fairness/#data-preprocessing","title":"Data preprocessing","text":"<p>When doing data preprocessing we're trying to remove any bias caused by the sensitive variable from the input dataset. By doing this, we remain flexible in our choice of models.</p>"},{"location":"user-guide/fairness/#information-filter","title":"Information Filter","text":"<p>This is a great opportunity to use the <code>InformationFilter</code> which can filter the information of these two sensitive columns away as a transformation step.</p> <p>It does this by projecting all vectors away such that the remaining dataset is orthogonal to the sensitive columns.</p>"},{"location":"user-guide/fairness/#how-it-works","title":"How it works","text":"<p>The <code>InformationFilter</code> uses a variant of the Gram\u2013Schmidt process to filter information out of the dataset. We can make it visual in two dimensions;</p> <p> </p> <p>To explain what occurs in higher dimensions we need to resort to maths. Take a training matrix \\(X\\) that contains columns \\(x_1, ..., x_k\\).</p> <p>If we assume columns \\(x_1\\) and \\(x_2\\) to be the sensitive columns then the information filter will filter out information using the following approach:</p> \\[ \\begin{split} v_1 &amp; = x_1 \\\\ v_2 &amp; = x_2 - \\frac{x_2 v_1}{v_1 v_1}\\\\ v_3 &amp; = x_3 - \\frac{x_3 v_1}{v_1 v_1} - \\frac{x_3 v_2}{v_2 v_2}\\\\     &amp; ... \\\\ v_k &amp; = x_k - \\frac{x_k v_1}{v_1 v_1} - \\frac{x_k' v_2}{v_2 v_2} \\end{split} \\] <p>Concatenating our vectors (but removing the sensitive ones) gives us a new training matrix \\(X_{\\text{more fair}} =  [v_3, ..., v_k]\\).</p>"},{"location":"user-guide/fairness/#experiment","title":"Experiment","text":"<p>We will demonstrate the effect of applying this by benchmarking three things:</p> <ol> <li>Keep \\(X\\) as is.</li> <li>Drop the two columns that are sensitive.</li> <li>Use the information filter</li> </ol> <p>We'll use the regression metric defined above to show the differences in fairness.</p> Information Filter<pre><code>import pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklego.preprocessing import InformationFilter\n\nX, y = load_boston(return_X_y=True)\ndf = pd.DataFrame(X,\n    columns=[\"crim\", \"zn\", \"indus\", \"chas\", \"nox\", \"rm\", \"age\", \"dis\", \"rad\", \"tax\", \"ptratio\", \"b\", \"lstat\"]\n)\n\nX_drop = df.drop(columns=[\"lstat\", \"b\"])\n\nX_fair = InformationFilter([\"lstat\", \"b\"]).fit_transform(df)\nX_fair = pd.DataFrame(X_fair, columns=[n for n in df.columns if n not in [\"b\", \"lstat\"]])\n\n\ndef simple_mod():\n    \"\"\"Create a simple model\"\"\"\n    return Pipeline([(\"scale\", StandardScaler()), (\"mod\", LinearRegression())])\n\nbase_mod = simple_mod().fit(X, y)\ndrop_mod = simple_mod().fit(X_drop, y)\nfair_mod = simple_mod().fit(X_fair, y)\n\nbase_pred = base_mod.predict(X)\ndrop_pred = drop_mod.predict(X_drop)\nfair_pred = fair_mod.predict(X_fair)\n</code></pre> <p>We can see that the coefficients of the three models are indeed different.</p> Information Filter<pre><code>coefs = pd.DataFrame([\n    base_mod.steps[1][1].coef_,\n    drop_mod.steps[1][1].coef_,\n    fair_mod.steps[1][1].coef_\n    ],\n    columns=df.columns)\ncoefs\n</code></pre> crim zn indus chas nox rm age dis rad tax ptratio b lstat -0.928146 1.08157 0.1409 0.68174 -2.05672 2.67423 0.0194661 -3.10404 2.66222 -2.07678 -2.06061 0.849268 -3.74363 -1.5814 0.911004 -0.290074 0.884936 -2.56787 4.2647 -1.27073 -3.33184 2.21574 -2.05625 -2.1546 nan nan -0.763568 1.02805 0.0613932 0.697504 -1.60546 6.84677 -0.0579197 -2.5376 1.93506 -1.77983 -2.79307 nan nan Utils <pre><code># We're using \"lstat\" to select the group to keep things simple\nselector = df[\"lstat\"] &gt; np.quantile(df[\"lstat\"], 0.5)\n\ndef bootstrap_means(preds, selector, n=2500, k=25):\n    grp1 = np.random.choice(preds[selector], (n, k)).mean(axis=1)\n    grp2 = np.random.choice(preds[~selector], (n, k)).mean(axis=1)\n    return grp1 - grp2\n\n\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\n\nsns.set_theme()\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nplt.scatter(base_pred, y)\nplt.title(f\"MSE: {mean_squared_error(y, base_pred)}\")\nplt.subplot(122)\nplt.hist(bootstrap_means(base_pred, selector), bins=30, density=True, alpha=0.8)\nplt.title(f\"Fairness Proxy\");\nplt.savefig(_static_path / \"original-situation.png\")\nplt.clf()\n\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\n\nsns.set_theme()\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nplt.scatter(drop_pred, y)\nplt.title(f\"MSE: {mean_squared_error(y, drop_pred)}\")\nplt.subplot(122)\nplt.hist(bootstrap_means(base_pred, selector), bins=30, density=True, alpha=0.8)\nplt.hist(bootstrap_means(drop_pred, selector), bins=30, density=True, alpha=0.8)\nplt.title(f\"Fairness Proxy\");\n\nplt.savefig(_static_path / \"drop-two.png\")\nplt.clf()\n\n\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\n\nsns.set_theme()\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nplt.scatter(fair_pred, y)\nplt.title(f\"MSE: {mean_squared_error(y, fair_pred)}\")\nplt.subplot(122)\nplt.hist(bootstrap_means(base_pred, selector), bins=30, density=True, alpha=0.8)\nplt.hist(bootstrap_means(fair_pred, selector), bins=30, density=True, alpha=0.8)\nplt.title(f\"Fairness Proxy\");\n\nplt.savefig(_static_path / \"use-info-filter.png\")\nplt.clf()\n\nfrom sklego.linear_model import DemographicParityClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklego.metrics import p_percent_score\n\ndf_clf = df.assign(lstat=lambda d: d[\"lstat\"] &gt; np.median(d[\"lstat\"]))\ny_clf = y &gt; np.median(y)\n\nnormal_classifier = LogisticRegression(solver=\"lbfgs\")\n_ = normal_classifier.fit(df_clf, y_clf)\nfair_classifier = DemographicParityClassifier(sensitive_cols=\"lstat\", covariance_threshold=0.5)\n_ = fair_classifier.fit(df_clf, y_clf)\n\nfrom sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfair_classifier = GridSearchCV(\n    estimator=DemographicParityClassifier(sensitive_cols=\"lstat\", covariance_threshold=0.5),\n    param_grid={\"estimator__covariance_threshold\": np.linspace(0.01, 1.00, 20)},\n    cv=5,\n    refit=\"accuracy_score\",\n    return_train_score=True,\n    scoring={\n        \"p_percent_score\": p_percent_score(\"lstat\"),\n        \"accuracy_score\": make_scorer(accuracy_score)\n    }\n)\n\nfair_classifier.fit(df_clf, y_clf)\n\npltr = (pd.DataFrame(fair_classifier.cv_results_)\n        .set_index(\"param_estimator__covariance_threshold\"))\n\np_score = p_percent_score(\"lstat\")(normal_classifier, df_clf, y_clf)\nacc_score = accuracy_score(normal_classifier.predict(df_clf), y_clf)\n\n\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nsns.set_theme()\n\nplt.figure(figsize=(12, 3))\nplt.subplot(121)\nplt.plot(np.array(pltr.index), pltr[\"mean_test_p_percent_score\"], label=\"fairclassifier\")\nplt.plot(np.linspace(0, 1, 2), [p_score for _ in range(2)], label=\"logistic-regression\")\nplt.xlabel(\"covariance threshold\")\nplt.legend()\nplt.title(\"p% score\")\nplt.subplot(122)\nplt.plot(np.array(pltr.index), pltr[\"mean_test_accuracy_score\"], label=\"fairclassifier\")\nplt.plot(np.linspace(0, 1, 2), [acc_score for _ in range(2)], label=\"logistic-regression\")\nplt.xlabel(\"covariance threshold\")\nplt.legend()\nplt.title(\"accuracy\");\n\nplt.savefig(_static_path / \"demographic-parity-grid-results.png\")\nplt.clf()\n\nfrom sklego.linear_model import EqualOpportunityClassifier\n\nfair_classifier = GridSearchCV(\n    estimator=EqualOpportunityClassifier(\n        sensitive_cols=\"lstat\", \n        covariance_threshold=0.5,\n        positive_target=True,\n    ),\n    param_grid={\"estimator__covariance_threshold\": np.linspace(0.001, 1.00, 20)},\n    cv=5,\n    n_jobs=-1,\n    refit=\"accuracy_score\",\n    return_train_score=True,\n    scoring={\n        \"p_percent_score\": p_percent_score(\"lstat\"),\n        \"equal_opportunity_score\": equal_opportunity_score(\"lstat\"),\n        \"accuracy_score\": make_scorer(accuracy_score)\n    }\n)\n\nfair_classifier.fit(df_clf, y_clf)\n\npltr = (pd.DataFrame(fair_classifier.cv_results_)\n        .set_index(\"param_estimator__covariance_threshold\"))\n\np_score = p_percent_score(\"lstat\")(normal_classifier, df_clf, y_clf)\nacc_score = accuracy_score(normal_classifier.predict(df_clf), y_clf)\n\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nsns.set_theme()\n\nplt.figure(figsize=(12, 3))\nplt.subplot(121)\nplt.plot(np.array(pltr.index), pltr[\"mean_test_equal_opportunity_score\"], label=\"fairclassifier\")\nplt.plot(np.linspace(0, 1, 2), [p_score for _ in range(2)], label=\"logistic-regression\")\nplt.xlabel(\"covariance threshold\")\nplt.legend()\nplt.title(\"equal opportunity score\")\nplt.subplot(122)\nplt.plot(np.array(pltr.index), pltr[\"mean_test_accuracy_score\"], label=\"fairclassifier\")\nplt.plot(np.linspace(0, 1, 2), [acc_score for _ in range(2)], label=\"logistic-regression\")\nplt.xlabel(\"covariance threshold\")\nplt.legend()\nplt.title(\"accuracy\");\n\n\nplt.savefig(_static_path / \"equal-opportunity-grid-results.png\")\nplt.clf()\n</code></pre>"},{"location":"user-guide/fairness/#1-original-situation","title":"1. Original Situation","text":"Code to generate the plot <pre><code>import matplotlib.pylab as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\n\nsns.set_theme()\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nplt.scatter(base_pred, y)\nplt.title(f\"MSE: {mean_squared_error(y, base_pred)}\")\nplt.subplot(122)\nplt.hist(bootstrap_means(base_pred, selector), bins=30, density=True, alpha=0.8)\nplt.title(f\"Fairness Proxy\");\n</code></pre>"},{"location":"user-guide/fairness/#2-drop-two-columns","title":"2. Drop two columns","text":"Code to generate the plot <pre><code>import matplotlib.pylab as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\n\nsns.set_theme()\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nplt.scatter(drop_pred, y)\nplt.title(f\"MSE: {mean_squared_error(y, drop_pred)}\")\nplt.subplot(122)\nplt.hist(bootstrap_means(base_pred, selector), bins=30, density=True, alpha=0.8)\nplt.hist(bootstrap_means(drop_pred, selector), bins=30, density=True, alpha=0.8)\nplt.title(f\"Fairness Proxy\");\n</code></pre>"},{"location":"user-guide/fairness/#3-use-the-information-filter","title":"3. Use the Information Filter","text":"Code to generate the plot <pre><code>import matplotlib.pylab as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\n\nsns.set_theme()\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nplt.scatter(fair_pred, y)\nplt.title(f\"MSE: {mean_squared_error(y, fair_pred)}\")\nplt.subplot(122)\nplt.hist(bootstrap_means(base_pred, selector), bins=30, density=True, alpha=0.8)\nplt.hist(bootstrap_means(fair_pred, selector), bins=30, density=True, alpha=0.8)\nplt.title(f\"Fairness Proxy\");\n</code></pre> <p>There definitely is a balance between fairness and model accuracy. Which model you'll use depends on the world you want to create by applying your model.</p> <p>Note that you can combine models here to make an ensemble too. You can also use the difference between the first and last model as a proxy for bias.</p>"},{"location":"user-guide/fairness/#model-constraints","title":"Model constraints","text":"<p>Another way we could tackle this fairness problem would be to explicitly take fairness into account when optimizing the parameters of our model. This is implemented in the <code>DemographicParityClassifier</code> as well as the <code>EqualOpportunityClassifier</code>.</p> <p>Both these models are built as an extension of basic logistic regression. Where logistic regression optimizes the following problem:</p> \\[ \\begin{array}{cl} {\\operatorname{minimize}} &amp; -\\sum_{i=1}^{N} \\log p\\left(y_{i} | \\mathbf{x}_{i},\\boldsymbol{\\theta}\\right) \\end{array} \\] <p>We would like to instead optimize this:</p> \\[ \\begin{array}{cl} {\\operatorname{minimize}} &amp; -\\sum_{i=1}^{N} \\log p\\left(y_{i} | \\mathbf{x}_{i}, \\boldsymbol{\\theta}\\right) \\\\ {\\text { subject to }} &amp; \\text{fairness} \\geq \\mathbf{c} \\end{array} \\]"},{"location":"user-guide/fairness/#demographic-parity-classifier","title":"Demographic Parity Classifier","text":"<p>The p% score discussed above is a nice metric but unfortunately it is rather hard to directly implement in the formulation into our model as it is a non-convex function making it difficult to optimize directly. Also, as the p% rule only depends on which side of the decision boundary an observation lies, it is invariant in small changes in the decision boundary. This causes large saddle points in the objective making optimization even more difficult</p> <p>Instead of optimizing for the p% directly, we approximate it by taking the covariance between the users\u2019 sensitive attributes, \\(z\\)m, and the decision boundary. This results in the following formulation of our <code>DemographicParityClassifier</code>.</p> \\[ \\begin{array}{cl} {\\operatorname{minimize}} &amp; -\\sum_{i=1}^{N} \\log p\\left(y_{i} | \\mathbf{x}_{i}, \\boldsymbol{\\theta}\\right)\\\\ {\\text { subject to }} &amp; {\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right) d_\\boldsymbol{\\theta}\\left(\\mathbf{x}_{i}\\right) \\leq \\mathbf{c}} \\\\ {} &amp; {\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right) d_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{i}\\right) \\geq-\\mathbf{c}} \\end{array} \\] <p>Let's see what the effect of this is. As this is a Classifier and not a Regressor, we transform the target to a binary variable indicating whether it is above or below the median. Our p% metric also assumes a binary indicator for sensitive columns so we do the same for our <code>lstat</code> column.</p> <p>Fitting the model is as easy as fitting a normal sklearn model. We just need to supply the columns that should be treated as sensitive to the model, as well as the maximum covariance we want to have.</p> Demographic Parity Classifier<pre><code>from sklego.linear_model import DemographicParityClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklego.metrics import p_percent_score\n\ndf_clf = df.assign(lstat=lambda d: d[\"lstat\"] &gt; np.median(d[\"lstat\"]))\ny_clf = y &gt; np.median(y)\n\nnormal_classifier = LogisticRegression(solver=\"lbfgs\")\n_ = normal_classifier.fit(df_clf, y_clf)\nfair_classifier = DemographicParityClassifier(sensitive_cols=\"lstat\", covariance_threshold=0.5)\n_ = fair_classifier.fit(df_clf, y_clf)\n</code></pre> <p>Comparing the two models on their p% scores also shows that the fair classifier has a much higher fairness score at a slight cost in accuracy.</p> <p>We'll compare these two models by doing a gridsearch on the effect of the <code>covariance_threshold</code>.</p> <pre><code>from sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfair_classifier = GridSearchCV(\n    estimator=DemographicParityClassifier(sensitive_cols=\"lstat\", covariance_threshold=0.5),\n    param_grid={\"estimator__covariance_threshold\": np.linspace(0.01, 1.00, 20)},\n    cv=5,\n    refit=\"accuracy_score\",\n    return_train_score=True,\n    scoring={\n        \"p_percent_score\": p_percent_score(\"lstat\"),\n        \"accuracy_score\": make_scorer(accuracy_score)\n    }\n)\n\nfair_classifier.fit(df_clf, y_clf)\n\npltr = (pd.DataFrame(fair_classifier.cv_results_)\n        .set_index(\"param_estimator__covariance_threshold\"))\n\np_score = p_percent_score(\"lstat\")(normal_classifier, df_clf, y_clf)\nacc_score = accuracy_score(normal_classifier.predict(df_clf), y_clf)\n</code></pre> <p>The results of the grid search are shown below. Note that the logistic regression results are of the train set, not the test set. We can see that the increase in fairness comes at the cost of accuracy but this might literally be a fair tradeoff.</p> Code to generate the plot <pre><code>import matplotlib.pylab as plt\nimport seaborn as sns\n\nsns.set_theme()\n\nplt.figure(figsize=(12, 3))\nplt.subplot(121)\nplt.plot(np.array(pltr.index), pltr[\"mean_test_p_percent_score\"], label=\"fairclassifier\")\nplt.plot(np.linspace(0, 1, 2), [p_score for _ in range(2)], label=\"logistic-regression\")\nplt.xlabel(\"covariance threshold\")\nplt.legend()\nplt.title(\"p% score\")\nplt.subplot(122)\nplt.plot(np.array(pltr.index), pltr[\"mean_test_accuracy_score\"], label=\"fairclassifier\")\nplt.plot(np.linspace(0, 1, 2), [acc_score for _ in range(2)], label=\"logistic-regression\")\nplt.xlabel(\"covariance threshold\")\nplt.legend()\nplt.title(\"accuracy\");\n</code></pre> <p></p>"},{"location":"user-guide/fairness/#equal-opportunity","title":"Equal opportunity","text":"<p>In the same spirit as the <code>DemographicParityClassifier</code> discussed above, there is also an <code>EqualOpportunityClassifier</code> which optimizes</p> \\[ \\begin{array}{cl} {\\operatorname{minimize}} &amp; -\\sum_{i=1}^{N} \\log p\\left(y_{i} | \\mathbf{x}_{i}, \\boldsymbol{\\theta}\\right) \\\\ {\\text { subject to }} &amp; {\\frac{1}{POS} \\sum_{i=1}^{POS}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right) d_\\boldsymbol{\\theta}\\left(\\mathbf{x}_{i}\\right) \\leq \\mathbf{c}} \\\\ {} &amp; {\\frac{1}{POS} \\sum_{i=1}^{POS}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right) d_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{i}\\right) \\geq-\\mathbf{c}} \\end{array} \\] <p>where POS is the subset of the population where <code>y_true = positive_target</code>.</p> Equal Opportunity Classifier<pre><code>from sklego.linear_model import EqualOpportunityClassifier\n\nfair_classifier = GridSearchCV(\n    estimator=EqualOpportunityClassifier(\n        sensitive_cols=\"lstat\", \n        covariance_threshold=0.5,\n        positive_target=True,\n    ),\n    param_grid={\"estimator__covariance_threshold\": np.linspace(0.001, 1.00, 20)},\n    cv=5,\n    n_jobs=-1,\n    refit=\"accuracy_score\",\n    return_train_score=True,\n    scoring={\n        \"p_percent_score\": p_percent_score(\"lstat\"),\n        \"equal_opportunity_score\": equal_opportunity_score(\"lstat\"),\n        \"accuracy_score\": make_scorer(accuracy_score)\n    }\n)\n\nfair_classifier.fit(df_clf, y_clf)\n\npltr = (pd.DataFrame(fair_classifier.cv_results_)\n        .set_index(\"param_estimator__covariance_threshold\"))\n\np_score = p_percent_score(\"lstat\")(normal_classifier, df_clf, y_clf)\nacc_score = accuracy_score(normal_classifier.predict(df_clf), y_clf)\n</code></pre> Code to generate the plot <pre><code>import matplotlib.pylab as plt\nimport seaborn as sns\n\nsns.set_theme()\n\nplt.figure(figsize=(12, 3))\nplt.subplot(121)\nplt.plot(np.array(pltr.index), pltr[\"mean_test_equal_opportunity_score\"], label=\"fairclassifier\")\nplt.plot(np.linspace(0, 1, 2), [p_score for _ in range(2)], label=\"logistic-regression\")\nplt.xlabel(\"covariance threshold\")\nplt.legend()\nplt.title(\"equal opportunity score\")\nplt.subplot(122)\nplt.plot(np.array(pltr.index), pltr[\"mean_test_accuracy_score\"], label=\"fairclassifier\")\nplt.plot(np.linspace(0, 1, 2), [acc_score for _ in range(2)], label=\"logistic-regression\")\nplt.xlabel(\"covariance threshold\")\nplt.legend()\nplt.title(\"accuracy\");\n</code></pre> <p></p> <ol> <li> <p>M. Zafar et al. (2017), Fairness Constraints: Mechanisms for Fair Classification\u00a0\u21a9</p> </li> <li> <p>M. Hardt, E. Price and N. Srebro (2016), Equality of Opportunity in Supervised Learning\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/linear-models/","title":"Linear Models","text":"<p>There's a few linear models out there that we felt were generally useful. This document will highlight some of them.</p> Common imports <pre><code>import matplotlib.pylab as plt\nimport seaborn as sns\nsns.set_theme()\n</code></pre>"},{"location":"user-guide/linear-models/#lowess","title":"LOWESS","text":"<p>Lowess stands for LOcally WEighted Scatterplot Smoothing and has historically been used for smoothing but you can also use it for machine learning where you're interested in interpolating. Here's a demo of <code>LowessRegression</code> in action:</p> LowessRegression<pre><code>import numpy as np\nfrom sklego.linear_model import LowessRegression\n\nn = 100\nxs = np.linspace(0, np.pi, n)\nys = 1 + np.sin(xs) + np.cos(xs**2) + np.random.normal(0, 0.1, n)\n\nmod = LowessRegression(sigma=0.1).fit(xs.reshape(-1, 1), ys)\n\nxs_new = np.linspace(-1, np.pi + 1, n * 2)\npreds = mod.predict(xs_new.reshape(-1, 1))\n</code></pre> Code for plot generation <pre><code>plt.figure(figsize=(12, 4))\nplt.scatter(xs, ys)\nplt.plot(xs_new, preds, color=\"orange\")\nplt.title(\"Be careful with extrapolation here.\")\n\nplt.savefig(_static_path / \"lowess.png\")\nplt.clf()\n\n\ndef plot_grid_weights(sigmas, spans):\n    n, m = len(sigmas), len(spans)\n    _, axes = plt.subplots(n, m, figsize=(10, 7), sharex=True, sharey=True)\n    for i, ax in enumerate(axes.flat):\n        span = spans[i % m]\n        sigma = sigmas[int(i / n) % m]\n        mod = LowessRegression(sigma=sigma, span=span).fit(xs.reshape(-1, 1), ys)\n        wts = mod._calc_wts([1.5])\n        ax.plot(xs, wts, color=\"steelblue\")\n        ax.set_title(f\"$\\sigma$={sigma}, span={span}\")\n    return axes\n\n\nfig = plot_grid_weights(sigmas=[1.0, 0.1], spans=[0.1, 0.9])\n\nplt.savefig(_static_path / \"grid-span-sigma-01.png\")\nplt.clf()\n\n\ndef plot_spansigma(sigmas, spans):\n    n, m = len(sigmas), len(spans)\n    _, axes = plt.subplots(n, m, figsize=(10, 7), sharex=True, sharey=True)\n    for i, ax in enumerate(axes.flat):\n        span = spans[i % m]\n        sigma = sigmas[int(i / n) % m]\n        mod = LowessRegression(sigma=sigma, span=span).fit(xs.reshape(-1, 1), ys)\n        preds = mod.predict(xs_new.reshape(-1, 1))\n        ax.scatter(xs, ys)\n        ax.plot(xs_new, preds, color=\"orange\")\n        ax.set_title(f\"$\\sigma$={sigma}, span={span}\")\n    return axes\n\n\nfig = plot_spansigma(sigmas=[1.0, 0.1], spans=[0.1, 0.9])\nplt.savefig(_static_path / \"grid-span-sigma-02.png\")\nplt.clf()\n\n\n@gif.frame\ndef single_frame(i, sigma, with_pred=False):\n    mod = LowessRegression(sigma=sigma).fit(xs.reshape(-1, 1), ys)\n    preds = mod.predict(xs.reshape(-1, 1))\n    plt.figure(figsize=(10, 3))\n    wts = mod._calc_wts(xs[i])\n    plt.scatter(xs, ys, color=\"gray\")\n    plt.plot(xs, wts, color=\"red\", alpha=0.5)\n    for j in range(len(xs)):\n        plt.scatter([xs[j]], [ys[j]], alpha=wts[j], color=\"orange\")\n    if with_pred:\n        plt.plot(xs[:i], preds[:i], color=\"red\")\n    plt.title(f\"$\\sigma$={sigma}\")\n\n\nfor sigma, name, with_pred in zip((0.1, 0.1, 0.01), (\"01\", \"01\", \"001\"), (False, True, True)):\n    frames = [single_frame(i, sigma, with_pred=with_pred) for i in range(100)]\n    suffix = f\"{'-' + name if with_pred else ''}\"\n    gif.save(frames, str(_static_path / f\"lowess-rolling{suffix}.gif\"), duration=100)\n\n\nn = 100\nxs_orig = xs_sparse = np.linspace(0, np.pi, n)\n\nys_sparse = 1 + np.sin(xs_sparse) + np.cos(xs_sparse**2) + np.random.normal(0, 0.1, n)\nkeep = (xs_sparse &lt; 0.8) | (xs_sparse &gt; 1.6)\n\nxs_sparse, ys_sparse = xs_sparse[keep], ys_sparse[keep]\n\nmod_small = LowessRegression(sigma=0.01).fit(xs.reshape(-1, 1), ys)\nmod_big = LowessRegression(sigma=0.1).fit(xs.reshape(-1, 1), ys)\n\npreds_small = mod_small.predict(xs_orig.reshape(-1, 1))\npreds_big = mod_big.predict(xs_orig.reshape(-1, 1))\n\n\n@gif.frame\ndef double_frame(i):\n    plt.figure(figsize=(10, 3))\n    wts_small = mod_small._calc_wts(xs_orig[i])\n    wts_big = mod_big._calc_wts(xs_orig[i])\n\n    plt.scatter(xs_sparse, ys_sparse, color=\"gray\")\n\n    plt.plot(xs_orig, wts_big, color=\"green\", alpha=0.5)\n    plt.plot(xs_orig, wts_small, color=\"red\", alpha=0.5)\n\n    plt.plot(xs_orig[:i], preds_big[:i], color=\"green\", label=\"$\\sigma$=0.1\")\n    plt.plot(xs_orig[:i], preds_small[:i], color=\"red\", label=\"$\\sigma$=0.01\")\n\n    plt.legend()\n\n\nframes = [double_frame(i) for i in range(len(xs))]\ngif.save(frames, str(_static_path / \"lowess-two-predictions.gif\"), duration=100)\n\n################################# ProbWeightRegression ###################################\n##########################################################################################\n\nfrom sklearn.datasets import make_regression\nimport pandas as pd\n\nX, y = make_regression(n_samples=1000, n_features=10, random_state=42)\ndf = pd.DataFrame(X)\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklego.meta import EstimatorTransformer\nfrom sklego.linear_model import ProbWeightRegression\nfrom sklego.preprocessing import ColumnSelector\n\npipe = Pipeline([\n    (\"models\", FeatureUnion([\n        (\"path1\", Pipeline([\n            (\"select1\", ColumnSelector([0, 1, 2, 3, 4])),\n            (\"pca\", PCA(n_components=3)),\n            (\"linear\", EstimatorTransformer(LinearRegression()))\n        ])),\n        (\"path2\", Pipeline([\n            (\"select2\", ColumnSelector([5,6,7,8,9])),\n            (\"pca\", PCA(n_components=2)),\n            (\"linear\", EstimatorTransformer(LinearRegression()))\n        ]))\n    ])),\n    (\"prob_weight\", ProbWeightRegression())\n])\n\ngrid = GridSearchCV(estimator=pipe, param_grid={}, cv=3).fit(df, y)\n\nfrom sklearn import set_config\nset_config(display=\"diagram\")\ngrid\n\nfrom sklearn.utils import estimator_html_repr\nwith open(_static_path / \"grid.html\", \"w\") as f:\n    f.write(estimator_html_repr(grid))\n\ngrid.best_estimator_[1].coefs_\n# array([0.03102466, 0.96897535])\n\n#################################### LADRegression #######################################\n##########################################################################################\n\nimport numpy as np\n\nnp.random.seed(0)\nX = np.linspace(0, 1, 20)\ny = 3 * X + 1 + 0.5 * np.random.randn(20)\nX = X.reshape(-1, 1)\n\ny[10] = 8\ny[15] = 15\n\nplt.figure(figsize=(16, 4))\nplt.scatter(X, y)\n\nplt.savefig(_static_path / \"lad-data.png\")\nplt.clf()\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nx = np.array([0, 1]).reshape(-1, 1)\nplt.figure(figsize=(16, 4))\nplt.scatter(X, y)\nplt.plot(x, LinearRegression().fit(X, y).predict(x), \"r\");\n\nplt.savefig(_static_path / \"lr-fit.png\")\nplt.clf()\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklego.linear_model import LADRegression\n\nx = np.array([0, 1]).reshape(-1, 1)\nplt.figure(figsize=(16, 4))\nplt.scatter(X, y)\nplt.plot(x, LinearRegression().fit(X, y).predict(x), \"r\");\nplt.plot(x, LADRegression().fit(X, y).predict(x), \"g\");\n\nplt.savefig(_static_path / \"lad-fit.png\")\nplt.clf()\n\n################################# QuantileRegression #####################################\n##########################################################################################\n\nimport numpy as np\nfrom sklego.linear_model import QuantileRegression\n\nnp.random.seed(123)\nX = np.arange(100).reshape(-1, 1)\ny = 2*X.ravel() + X.ravel()*np.random.standard_cauchy(100)\n\nq_10 = QuantileRegression(quantile=0.1).fit(X, y)\nq_90 = QuantileRegression(quantile=0.9).fit(X, y)\nlad = QuantileRegression().fit(X, y)\n\nplt.plot(X, y)\nplt.plot(X, lad.predict(X))\nplt.fill_between(X.ravel(), q_10.predict(X), q_90.predict(X), alpha=0.33, color=\"orange\");\n\nplt.savefig(_static_path / \"quantile-fit.png\")\nplt.clf()\n</code></pre> <p></p> <p>The line does not look linear but that's because internally, during prediction, many weighted linear regressions are happening. The gif below demonstrates how the data is being weighted when we would make a prediction.</p> <p></p>"},{"location":"user-guide/linear-models/#details-on-sigma","title":"Details on <code>sigma</code>","text":"<p>We'll also show two different prediction outcomes depending on the hyperparameter <code>sigma</code>:</p> <p></p> <p></p> <p>You may be tempted now to think that a lower sigma always has a better fit, but you need to be careful here. The data might have gaps and larger sigma values will be able to properly regularize.</p> <p></p> <p>Note that this regression also works in higher dimensions but the main downside of this approach is that it is really slow when making predictions.</p> <p>If you want to get advanced there's also a hyperparameter <code>span</code> but you'll really need to know what you're doing. It was added for completeness but the authors of this package have yet to find a proper usecase for it.</p>"},{"location":"user-guide/linear-models/#details-on-span","title":"Details on <code>span</code>","text":"<p>The <code>span</code> parameter can be used to force that you'll only include a certain percentage of your dataset. Technically without a <code>span</code> you are still using all the data in your dataset, albeit with small weights if they are far away.</p> <p>The effect of the <code>span</code> parameter on the weights can be seen below:</p> <p></p> <p>This will also effect the predictions.</p> <p></p> <p>You may need to squint your eyes a bit to see it, but lower spans cause more jiggles and less smooth curves.</p>"},{"location":"user-guide/linear-models/#probweightregression","title":"ProbWeightRegression","text":"<p>Note, this is a somewhat experimental feature. We found this feature to be plausibly useful but we've not seen it cause a big \"win\" yet.</p> <p>Let's say that you're interested in combining a few models for a regression task.</p> <p>You could put them together in an ensemble. Say we've got predictions \\(y_1, y_2, y_3\\), each of which come from respectable models, then you may want to combine these together in another linear regression.</p> <p>This way the new, hopefully best, prediction \\(y_*\\) is defined via:</p> \\[ y^* = w_1 y_1 + w_2 y_2 + w_3 y_3 \\] <p>This can be a valid way of reweighing. But there's one issue: technically the weights \\(w_1, w_2, w_3\\) can sum to a number that isn't one. Since that's numerically totally possible we need to be aware that we can end up in a strange situation.</p> <p>The <code>ProbWeightRegression</code> addresses this by assuming that every input it receives is the output of a model and it will ensure that they are reweighed with a constraint in mind. For this usecase, it would optimise:</p> \\[ \\begin{split} \\text{minimize} &amp; \\sum_i (y_i - (w_1 y_{1,i} + w_2 y_{2,i} + w_3 y_{3,i}))^2 \\\\ \\text{subject to} &amp; \\\\ &amp; \\text{ } w_1 + w_2 + w_3 = 1 \\\\ &amp; \\text{ } w_1 \\geq 0, w_2 \\geq 0, w_3 \\geq 0 \\end{split} \\] <p>The final positivity constraint is optional in our model.</p> <p>Here's an example usage of <code>ProbWeightRegression</code> in action:</p> <pre><code>from sklearn.datasets import make_regression\nimport pandas as pd\n\nX, y = make_regression(n_samples=1000, n_features=10, random_state=42)\ndf = pd.DataFrame(X)\n</code></pre> <p>We've turned the array into a dataframe so that we can apply the <code>ColumnSelector</code>.</p> ProbWeightRegression<pre><code>from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklego.meta import EstimatorTransformer\nfrom sklego.linear_model import ProbWeightRegression\nfrom sklego.preprocessing import ColumnSelector\n\npipe = Pipeline([\n    (\"models\", FeatureUnion([\n        (\"path1\", Pipeline([\n            (\"select1\", ColumnSelector([0, 1, 2, 3, 4])),\n            (\"pca\", PCA(n_components=3)),\n            (\"linear\", EstimatorTransformer(LinearRegression()))\n        ])),\n        (\"path2\", Pipeline([\n            (\"select2\", ColumnSelector([5,6,7,8,9])),\n            (\"pca\", PCA(n_components=2)),\n            (\"linear\", EstimatorTransformer(LinearRegression()))\n        ]))\n    ])),\n    (\"prob_weight\", ProbWeightRegression())\n])\n\ngrid = GridSearchCV(estimator=pipe, param_grid={}, cv=3).fit(df, y)\n</code></pre> <pre><code>from sklearn import set_config\nset_config(display=\"diagram\")\ngrid\n</code></pre> <pre>GridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('models',\n                                        FeatureUnion(transformer_list=[('path1',\n                                                                        Pipeline(steps=[('select1',\n                                                                                         ColumnSelector(columns=[0,\n                                                                                                                 1,\n                                                                                                                 2,\n                                                                                                                 3,\n                                                                                                                 4])),\n                                                                                        ('pca',\n                                                                                         PCA(n_components=3)),\n                                                                                        ('linear',\n                                                                                         EstimatorTransformer(estimator=LinearRegression()))])),\n                                                                       ('path2',\n                                                                        Pipeline(steps=[('select2',\n                                                                                         ColumnSelector(columns=[5,\n                                                                                                                 6,\n                                                                                                                 7,\n                                                                                                                 8,\n                                                                                                                 9])),\n                                                                                        ('pca',\n                                                                                         PCA(n_components=2)),\n                                                                                        ('linear',\n                                                                                         EstimatorTransformer(estimator=LinearRegression()))]))])),\n                                       ('prob_weight',\n                                        ProbWeightRegression())]),\n             param_grid={})</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV<pre>GridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('models',\n                                        FeatureUnion(transformer_list=[('path1',\n                                                                        Pipeline(steps=[('select1',\n                                                                                         ColumnSelector(columns=[0,\n                                                                                                                 1,\n                                                                                                                 2,\n                                                                                                                 3,\n                                                                                                                 4])),\n                                                                                        ('pca',\n                                                                                         PCA(n_components=3)),\n                                                                                        ('linear',\n                                                                                         EstimatorTransformer(estimator=LinearRegression()))])),\n                                                                       ('path2',\n                                                                        Pipeline(steps=[('select2',\n                                                                                         ColumnSelector(columns=[5,\n                                                                                                                 6,\n                                                                                                                 7,\n                                                                                                                 8,\n                                                                                                                 9])),\n                                                                                        ('pca',\n                                                                                         PCA(n_components=2)),\n                                                                                        ('linear',\n                                                                                         EstimatorTransformer(estimator=LinearRegression()))]))])),\n                                       ('prob_weight',\n                                        ProbWeightRegression())]),\n             param_grid={})</pre>estimator: Pipeline<pre>Pipeline(steps=[('models',\n                 FeatureUnion(transformer_list=[('path1',\n                                                 Pipeline(steps=[('select1',\n                                                                  ColumnSelector(columns=[0,\n                                                                                          1,\n                                                                                          2,\n                                                                                          3,\n                                                                                          4])),\n                                                                 ('pca',\n                                                                  PCA(n_components=3)),\n                                                                 ('linear',\n                                                                  EstimatorTransformer(estimator=LinearRegression()))])),\n                                                ('path2',\n                                                 Pipeline(steps=[('select2',\n                                                                  ColumnSelector(columns=[5,\n                                                                                          6,\n                                                                                          7,\n                                                                                          8,\n                                                                                          9])),\n                                                                 ('pca',\n                                                                  PCA(n_components=2)),\n                                                                 ('linear',\n                                                                  EstimatorTransformer(estimator=LinearRegression()))]))])),\n                ('prob_weight', ProbWeightRegression())])</pre>models: FeatureUnion<pre>FeatureUnion(transformer_list=[('path1',\n                                Pipeline(steps=[('select1',\n                                                 ColumnSelector(columns=[0, 1,\n                                                                         2, 3,\n                                                                         4])),\n                                                ('pca', PCA(n_components=3)),\n                                                ('linear',\n                                                 EstimatorTransformer(estimator=LinearRegression()))])),\n                               ('path2',\n                                Pipeline(steps=[('select2',\n                                                 ColumnSelector(columns=[5, 6,\n                                                                         7, 8,\n                                                                         9])),\n                                                ('pca', PCA(n_components=2)),\n                                                ('linear',\n                                                 EstimatorTransformer(estimator=LinearRegression()))]))])</pre>path1ColumnSelector<pre>ColumnSelector(columns=[0, 1, 2, 3, 4])</pre>PCA<pre>PCA(n_components=3)</pre>linear: EstimatorTransformer<pre>EstimatorTransformer(estimator=LinearRegression())</pre>estimator: LinearRegression<pre>LinearRegression()</pre>LinearRegression<pre>LinearRegression()</pre>path2ColumnSelector<pre>ColumnSelector(columns=[5, 6, 7, 8, 9])</pre>PCA<pre>PCA(n_components=2)</pre>linear: EstimatorTransformer<pre>EstimatorTransformer(estimator=LinearRegression())</pre>estimator: LinearRegression<pre>LinearRegression()</pre>LinearRegression<pre>LinearRegression()</pre>ProbWeightRegression<pre>ProbWeightRegression()</pre> <p>You can see that the <code>ProbWeightRegression</code> indeeds sums to one.</p> <pre><code>grid.best_estimator_[1].coefs_\n# array([0.03102466, 0.96897535])\n</code></pre>"},{"location":"user-guide/linear-models/#least-absolute-deviation-regression","title":"Least Absolute Deviation Regression","text":"<p>Imagine that you have a dataset with some outliers.</p> Data with outliers<pre><code>import numpy as np\n\nnp.random.seed(0)\nX = np.linspace(0, 1, 20)\ny = 3 * X + 1 + 0.5 * np.random.randn(20)\nX = X.reshape(-1, 1)\n\ny[10] = 8\ny[15] = 15\n\nplt.figure(figsize=(16, 4))\nplt.scatter(X, y)\n</code></pre> <p></p> <p>A simple linear regression will not do a good job since it is distracted by the outliers. That is because it optimizes the mean squared error</p> \\[ \\sum_i \\left(y_i-\\textrm{model}(x_i)\\right)^2 \\] <p>which penalizes a few large errors more than many tiny errors. For example, if \\(y-\\text{model}(x) = 4\\) for some single observation, the MSE here is 16. If there are two observations with \\(y_1 - \\text{model}(x_1) = 2\\) and \\(y_2 - \\text{model}(x_2) = 2\\), the MSE is 8 in total, which is less than for one larger error.</p> <p>Note that the sum of the errors is the same in both cases.</p> <p>Hence, linear regression does the following:</p> LinearRegression fit<pre><code>import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nx = np.array([0, 1]).reshape(-1, 1)\nplt.figure(figsize=(16, 4))\nplt.scatter(X, y)\nplt.plot(x, LinearRegression().fit(X, y).predict(x), \"r\");\n</code></pre> <p></p> <p>By changing the loss function to the mean absolute deviation</p> \\[ \\sum_i \\left|y_i-\\textrm{model}(x_i)\\right|\\enspace\\] <p>we can let the model put the same focus on each error.</p> <p>This yields the Least Absolute Deviation regression that tries to agree with the majority of the points.</p> <p>Here an example of LADRegression in action:</p> LADRegression fit<pre><code>import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklego.linear_model import LADRegression\n\nx = np.array([0, 1]).reshape(-1, 1)\nplt.figure(figsize=(16, 4))\nplt.scatter(X, y)\nplt.plot(x, LinearRegression().fit(X, y).predict(x), \"r\");\nplt.plot(x, LADRegression().fit(X, y).predict(x), \"g\");\n</code></pre> <p></p>"},{"location":"user-guide/linear-models/#see-also","title":"See also","text":"<p>scikit-learn tackles this problem by offering a variety of robust regressors. Many of them use an indirect approach to reduce the effect of outliers. RANSAC, for example, samples random points from the dataset until it consists of all inliers.</p> <p>The closest thing to LADRegression that scikit-learn offers is the HuberRegressor with a loss function that is partly a squared and partly an absolute error. However, it is more complicated and requires hyperparameter tuning to unleash its full potential.</p>"},{"location":"user-guide/linear-models/#quantileregression","title":"QuantileRegression","text":"<p>This is an extension of the LADRegression (see above). While the LADRegression outputs a line that over- and underestimates around 50% of the data, the QuantileRegression yields lines with different over- and underestimation shares. This can be used for creating simple confidence intervals around predictions. As an example, consider the following:</p> <ol> <li>Create a QuantileRegression with quantile=0.1,</li> <li>create a QuantileRegression with quantile=0.9,</li> </ol> <p>then around 80% of the data is between these two lines.</p> QuantileRegression fit<pre><code>import numpy as np\nfrom sklego.linear_model import QuantileRegression\n\nnp.random.seed(123)\nX = np.arange(100).reshape(-1, 1)\ny = 2*X.ravel() + X.ravel()*np.random.standard_cauchy(100)\n\nq_10 = QuantileRegression(quantile=0.1).fit(X, y)\nq_90 = QuantileRegression(quantile=0.9).fit(X, y)\nlad = QuantileRegression().fit(X, y)\n\nplt.plot(X, y)\nplt.plot(X, lad.predict(X))\nplt.fill_between(X.ravel(), q_10.predict(X), q_90.predict(X), alpha=0.33, color=\"orange\");\n</code></pre> <p></p>"},{"location":"user-guide/meta-models/","title":"Meta Models","text":"<p>Certain models in scikit-lego are meta. Meta models are models that depend on other estimators that go in and these models will add features to the input model.</p> <p>One way of thinking of a meta model is to consider it to be a way to decorate a model.</p> <p>This part of the documentation will highlight a few of them.</p>"},{"location":"user-guide/meta-models/#thresholder","title":"Thresholder","text":"<p>The <code>Thresholder</code> can help tweak recall and precision of a model by moving the threshold value of <code>predict_proba</code>.</p> <p>Commonly this threshold is set at 0.5 for two classes. This meta-model can decorate/wrap an estimator with two classes such that the threshold moves.</p> <p>We demonstrate how that works below. First we'll import the necessary libraries and generate a skewed dataset.</p> Skewed dataset<pre><code>import matplotlib.pylab as plt\nimport seaborn as sns\n\nsns.set_theme()\ncmap=sns.color_palette(\"flare\", as_cmap=True)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, make_scorer\n\nfrom sklego.meta import Thresholder\n\nX, y = make_blobs(1000, centers=[(0, 0), (1.5, 1.5)], cluster_std=[1, 0.5])\nplt.scatter(X[:, 0], X[:, 1], c=y, s=5, cmap=cmap);\n</code></pre> <p></p> <p>Next we'll make a cross validation pipeline to try out this thresholder.</p> Cross validation pipeline<pre><code># %%time\n\npipe = Pipeline([\n    (\"model\", Thresholder(LogisticRegression(solver=\"lbfgs\"), threshold=0.1))\n])\n\nmod = GridSearchCV(\n    estimator=pipe,\n    param_grid={\"model__threshold\": np.linspace(0.1, 0.9, 500)},\n    scoring={\n        \"precision\": make_scorer(precision_score),\n        \"recall\": make_scorer(recall_score),\n        \"accuracy\": make_scorer(accuracy_score)\n    },\n    refit=\"precision\",\n    cv=5\n)\n\n_ = mod.fit(X, y)\n</code></pre> <pre><code>CPU times: user 15.4 s, sys: 63.3 ms, total: 15.4 s\nWall time: 15.4 s\n</code></pre> <p>With this cross validation trained, we'll make a chart to show the effect of changing the threshold value.</p> Threshold chart<pre><code>(pd.DataFrame(mod.cv_results_)\n .set_index(\"param_model__threshold\")\n [[\"mean_test_precision\", \"mean_test_recall\", \"mean_test_accuracy\"]]\n .plot(figsize=(16, 6)));\n</code></pre> <p></p> <p>Increasing the threshold will increase the precision but as expected this is at the cost of recall (and accuracy).</p>"},{"location":"user-guide/meta-models/#saving-compute","title":"Saving Compute","text":"<p>Technically, you may not need to refit the underlying model that the <code>Thresholder</code> model wraps around.</p> <p>In those situations you can set the <code>refit</code> parameter to <code>False</code>. If you've got a predefined single model and you're only interested in tuning the cutoff this might make everything run a whole lot faster.</p> Cross validation pipeline - no refit<pre><code># %%time\n\n# Train an original model\norig_model = LogisticRegression(solver=\"lbfgs\")\norig_model.fit(X, y)\n\n# Ensure that refit=False\npipe = Pipeline([\n    (\"model\", Thresholder(orig_model, threshold=0.1, refit=False))\n])\n\n# This should now be a fair bit quicker.\nmod = GridSearchCV(\n    estimator=pipe,\n    param_grid = {\"model__threshold\": np.linspace(0.1, 0.9, 50)},\n    scoring={\n        \"precision\": make_scorer(precision_score),\n        \"recall\": make_scorer(recall_score),\n        \"accuracy\": make_scorer(accuracy_score)\n    },\n    refit=\"precision\",\n    cv=5\n)\n\n_ = mod.fit(X, y);\n</code></pre> <pre><code>CPU times: user 918 ms, sys: 0 ns, total: 918 ms\nWall time: 917 ms\n</code></pre>"},{"location":"user-guide/meta-models/#grouped-prediction","title":"Grouped Prediction","text":"<p>To help explain what it can do we'll consider three methods to predict the chicken weight.</p> <p>The chicken data has 578 rows and 4 columns from an experiment on the effect of diet on early growth of chicks. The body weights of the chicks were measured at birth and every second day thereafter until day 20. They were also measured on day 21. There were four groups on chicks on different protein diets.</p>"},{"location":"user-guide/meta-models/#setup","title":"Setup","text":"<p>Let's first load a bunch of things to do this.</p> Setup<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nfrom sklego.datasets import load_chicken\nfrom sklego.preprocessing import ColumnSelector\n\ndef plot_model(model):\n    df = load_chicken(as_frame=True)\n\n    _ = model.fit(df[[\"diet\", \"time\"]], df[\"weight\"])\n    metric_df = (df[[\"diet\", \"time\", \"weight\"]]\n        .assign(pred=lambda d: model.predict(d[[\"diet\", \"time\"]]))\n    )\n\n    metric = mean_absolute_error(metric_df[\"weight\"], metric_df[\"pred\"])\n\n    plt.figure(figsize=(12, 4))\n    plt.scatter(df[\"time\"], df[\"weight\"])\n    for i in [1, 2, 3, 4]:\n        pltr = metric_df[[\"time\", \"diet\", \"pred\"]].drop_duplicates().loc[lambda d: d[\"diet\"] == i]\n        plt.plot(pltr[\"time\"], pltr[\"pred\"], color=\".rbgy\"[i])\n    plt.title(f\"linear model per group, MAE: {np.round(metric, 2)}\");\n</code></pre> <p>This code will be used to explain the steps below.</p>"},{"location":"user-guide/meta-models/#model-1-linear-regression-with-dummies","title":"Model 1: Linear Regression with Dummies","text":"<p>First we start with a baseline. We'll use a linear regression and add dummies for the <code>diet</code> column.</p> Baseline model<pre><code>feature_pipeline = Pipeline([\n    (\"datagrab\", FeatureUnion([\n         (\"discrete\", Pipeline([\n             (\"grab\", ColumnSelector(\"diet\")),\n             (\"encode\", OneHotEncoder(categories=\"auto\", sparse=False))\n         ])),\n         (\"continuous\", Pipeline([\n             (\"grab\", ColumnSelector(\"time\")),\n             (\"standardize\", StandardScaler())\n         ]))\n    ]))\n])\n\npipe = Pipeline([\n    (\"transform\", feature_pipeline),\n    (\"model\", LinearRegression())\n])\n\nplot_model(pipe)\n</code></pre> <p></p> <p>Because the model is linear the dummy variable causes the intercept to change but leaves the gradient untouched. This might not be what we want from a model.</p> <p>So let's see how the grouped model can address this.</p>"},{"location":"user-guide/meta-models/#model-2-linear-regression-in-groupedpredictor","title":"Model 2: Linear Regression in GroupedPredictor","text":"<p>The goal of the GroupedPredictor is to allow us to split up our data.</p> <p>The image below demonstrates what will happen.</p> <p></p> <p>We train 5 models in total because the model will also train a fallback automatically (you can turn this off via <code>use_fallback=False</code>).</p> <p>The idea behind the fallback is that we can predict something if there is a group at prediction time which is unseen during training.</p> <p>Each model will accept features that are in <code>X</code> that are not part of the grouping variables. In this case each group will model based on the <code>time</code> since <code>weight</code> is what we're trying to predict.</p> <p>Applying this model to the dataframe is easy.</p> GroupedPredictor model<pre><code>from sklego.meta import GroupedPredictor\nmod = GroupedPredictor(LinearRegression(), groups=[\"diet\"])\nplot_model(mod)\n</code></pre> <p></p> <p>Such model looks a bit better.</p>"},{"location":"user-guide/meta-models/#model-3-dummy-regression-in-groupedestimation","title":"Model 3: Dummy Regression in GroupedEstimation","text":"<p>We could go a step further and train a DummyRegressor per diet per timestep.</p> <p>The code below works similar as the previous example but one difference is that the grouped model does not receive a dataframe but a numpy array.</p> <p></p> <p>Note that we're also grouping over more than one column here. The code that does this is listed below.</p> GroupedEstimator with DummyRegressor<pre><code>from sklearn.dummy import DummyRegressor\n\nfeature_pipeline = Pipeline([\n    (\"datagrab\", FeatureUnion([\n         (\"discrete\", Pipeline([\n             (\"grab\", ColumnSelector(\"diet\")),\n         ])),\n         (\"continuous\", Pipeline([\n             (\"grab\", ColumnSelector(\"time\")),\n         ]))\n    ]))\n])\n\npipe = Pipeline([\n    (\"transform\", feature_pipeline),\n    (\"model\", GroupedPredictor(DummyRegressor(strategy=\"mean\"), groups=[0, 1]))\n])\n\nplot_model(pipe)\n</code></pre> <p></p> <p>Note that these predictions seems to yield the lowest error but take it with a grain of salt since these errors are only based on the train set.</p>"},{"location":"user-guide/meta-models/#grouped-transformation","title":"Grouped Transformation","text":"<p>We can apply grouped prediction on estimators that have a <code>.predict()</code> implemented but we're also able to do something similar for transformers, like <code>StandardScaler</code>.</p> Load penguins<pre><code>from sklego.datasets import load_penguins\n\ndf_penguins = (\n    load_penguins(as_frame=True)\n    .dropna()\n    .drop(columns=[\"island\", \"bill_depth_mm\", \"bill_length_mm\", \"species\"])\n)\n\ndf_penguins.head()\n</code></pre> flipper_length_mm body_mass_g sex 181 3750 male 186 3800 female 195 3250 female 193 3450 female 190 3650 male <p>Let's say that we're interested in scaling the numeric data in this dataframe. If we apply a normal <code>StandardScaler</code> then we'll likely get clusters appear for all the <code>species</code> and for the <code>sex</code>. It may be the case (for fairness reasons) that we don\"t mind the clusters based on <code>species</code> but that we do mind the clusters based on <code>sex</code>.</p> <p>In these scenarios the <code>GroupedTransformer</code> can help out. We can specify a grouping column that the data needs to be split on before the transformation is applied.</p> GroupedTransformer<pre><code>from sklearn.preprocessing import StandardScaler\nfrom sklego.meta import GroupedTransformer\n\nX = df_penguins.drop(columns=[\"sex\"]).values\n\nX_tfm = StandardScaler().fit_transform(X)\nX_tfm_grp = (GroupedTransformer(\n    transformer=StandardScaler(),\n    groups=[\"sex\"]\n    )\n    .fit_transform(df_penguins)\n)\n</code></pre> <p></p> Code for plotting the transformed data <pre><code>import matplotlib.pylab as plt\nimport seaborn as sns\nsns.set_theme()\n\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\n\nplt.scatter(X_tfm[:, 0], X_tfm[:, 1], c=df_penguins[\"sex\"] == \"male\", cmap=cmap)\nplt.xlabel(\"norm flipper len\")\nplt.ylabel(\"norm body mass\")\nplt.title(\"scaled data, not normalised by gender\")\n\nplt.subplot(122)\nplt.scatter(X_tfm_grp[:, 0], X_tfm_grp[:, 1], c=df_penguins[\"sex\"] == \"male\", cmap=cmap)\nplt.xlabel(\"norm flipper len\")\nplt.ylabel(\"norm body mass\")\nplt.title(\"scaled data *and* normalised by gender\");\n</code></pre> <p>You can see that there are certainly still clusters. These are caused by the fact that there's different <code>species</code> of penguin in our dataset. However you can see that when we apply our <code>GroupedTransformer</code> that we're suddenly able to normalise towards <code>sex</code> as well.</p>"},{"location":"user-guide/meta-models/#other-scenarios","title":"Other scenarios","text":"<p>This transformer also has use-cases beyond fairness. You could use this transformer to causally compensate for subgroups in your data.</p> <p>For example, for predicting house prices, using the surface of a house relatively to houses in the same neighborhood could be a more relevant feature than the surface relative to all houses.</p>"},{"location":"user-guide/meta-models/#decayed-estimation","title":"Decayed Estimation","text":"<p>Often you are interested in predicting the future. You use the data from the past in an attempt to achieve this and it  could be said that perhaps data from the far history is less relevant than data from the recent past.</p> <p>This is the idea behind the <code>DecayEstimator</code> meta-model. It looks at the order of data going in and it will assign a higher importance to recent rows that occurred recently and a lower importance to older rows.</p> <p>Recency is based on the order so it is important that the dataset that you pass in is correctly ordered beforehand.</p> <p>we'll demonstrate how it works by applying it on a simulated timeseries problem.</p> TimeSeries data<pre><code>from sklego.datasets import make_simpleseries\n\nyt = make_simpleseries(seed=1)\ndf = (pd.DataFrame({\"yt\": yt,\n                   \"date\": pd.date_range(\"2000-01-01\", periods=len(yt))})\n      .assign(m=lambda d: d.date.dt.month)\n      .reset_index())\n\nplt.figure(figsize=(12, 3))\nplt.plot(make_simpleseries(seed=1));\n</code></pre> <p></p> <p>We will create two models on this dataset. One model calculates the average value per month in our timeseries and the other does the same thing but will decay the importance of making accurate predictions for the far history.</p> DecayEstimator<pre><code>from sklearn.dummy import DummyRegressor\nfrom sklego.meta import GroupedPredictor, DecayEstimator\n\nmod1 = (GroupedPredictor(DummyRegressor(), groups=[\"m\"])\n        .fit(df[[\"m\"]], df[\"yt\"]))\n\nmod2 = (GroupedPredictor(DecayEstimator(DummyRegressor(), decay_func=\"exponential\", decay_rate=0.9), groups=[\"m\"])\n        .fit(df[[\"index\", \"m\"]], df[\"yt\"]))\n\nplt.figure(figsize=(12, 3))\nplt.plot(df[\"yt\"], alpha=0.5);\nplt.plot(mod1.predict(df[[\"m\"]]), label=\"grouped\")\nplt.plot(mod2.predict(df[[\"index\", \"m\"]]), label=\"decayed\")\nplt.legend();\n</code></pre> <p></p> <p>The decay parameter has a lot of influence on the effect of the model but one can clearly see that we shift focus to the more recent data.</p>"},{"location":"user-guide/meta-models/#decay-functions","title":"Decay Functions","text":"<p>scikit-lego provides a set of decay functions that can be used to decay the importance of older data. The default decay function used in <code>DecayEstimator</code> is the <code>exponential_decay</code> function (<code>decay_func=\"exponential\"</code>).</p> <p>Out of the box there are four decay functions available:</p> <p></p> Code for plotting the decay functions <pre><code>from sklego.meta._decay_utils import exponential_decay, linear_decay, sigmoid_decay, stepwise_decay\n\nfig = plt.figure(figsize=(12, 6))\n\nfor i, name, func, kwargs in zip(\n    range(1, 5),\n    (\"exponential\", \"linear\", \"sigmoid\", \"stepwise\"),\n    (exponential_decay, linear_decay, sigmoid_decay, stepwise_decay),\n    ({\"decay_rate\": 0.995}, {\"min_value\": 0.1}, {}, {\"n_steps\": 8})\n    ):\n\n    ax = fig.add_subplot(2, 2, i)\n    x, y = None, np.arange(1000)\n    ax.plot(func(x,y, **kwargs))\n    ax.set_title(f'decay_func=\"{name}\"')\n\nplt.tight_layout()\n</code></pre> <p>The arguments of these functions can be passed along to the <code>DecayEstimator</code> class as keyword arguments:</p> <pre><code>DecayEstimator(..., decay_func=\"linear\", min_value=0.5)\n</code></pre> <p>To see which keyword arguments are available for each decay function, please refer to the Decay Functions API section.</p> <p>Notice that passing a string to refer to the built-in decays is just a convenience.</p> <p>Therefore it is also possible to create a custom decay function and pass it along to the <code>DecayEstimator</code> class, as long as the first two arguments of the function are <code>X</code> and <code>y</code> and the return shape is the same as <code>y</code>:</p> Custom decay function<pre><code>def custom_decay(X, y, alpha, beta, gamma):\n    \"\"\"My custom decay function where the magic happens\"\"\"\n    ...\n    return decay_values\n\nDecayEstimator(...,\n    decay_func=custom_decay,\n    alpha=some_alpha, beta=some_beta, gamma=some_gamma\n)\n</code></pre>"},{"location":"user-guide/meta-models/#confusion-balancer","title":"Confusion Balancer","text":"<p>Disclaimer</p> <p>This feature is an experimental.</p> <p>We added the <code>ConfusionBalancer</code> as experimental feature to the meta estimators that can be used to force balance in the confusion matrix of an estimator. Consider the following dataset:</p> Make blobs <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nsns.set_theme()\ncmap=sns.color_palette(\"flare\", as_cmap=True)\nnp.random.seed(42)\n\nn1, n2, n3 = 100, 500, 50\nX = np.concatenate([np.random.normal(0, 1, (n1, 2)),\n                    np.random.normal(2, 1, (n2, 2)),\n                    np.random.normal(3, 1, (n3, 2))],\n                   axis=0)\ny = np.concatenate([np.zeros((n1, 1)),\n                    np.ones((n2, 1)),\n                    np.zeros((n3, 1))],\n                   axis=0).reshape(-1)\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap);\n</code></pre> <p></p> <p>Let's take this dataset and train a simple classifier against it.</p> <pre><code>from sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\n\nmod = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=10000)\ncfm = confusion_matrix(y, mod.fit(X, y).predict(X))\ncfm\n# array([[ 72,  78],\n#        [  4, 496]])\n</code></pre> <p>The confusion matrix is not ideal. This is in part because the dataset is slightly imbalanced but in general it is also because of the way the algorithm works.</p> <p>Let's see if we can learn something else from this confusion matrix. We might transform the counts into probabilities.</p> <pre><code>cfm.T / cfm.T.sum(axis=1).reshape(-1, 1)\n# array([[0.94736842, 0.05263158],\n#        [0.1358885 , 0.8641115 ]])\n</code></pre> <p>Let's consider the number 0.1359 in the lower left corner. This number represents the probability that the actually class 0 while the model predicts class 1. In math terms we might write this as \\(P(C_1 | M_1)\\) where \\(C_i\\) denotes the actual label while \\(M_i\\) denotes the label given by the algorithm.</p> <p>The idea now is that we might rebalance our original predictions \\(P(M_i)\\) by multiplying them;</p> \\[ P_{\\text{corrected}}(C_1) = P(C_1|M_0) p(M_0) + P(C_1|M_1) p(M_1) \\] <p>In general this can be written as:</p> \\[ P_{\\text{corrected}}(C_i) = \\sum_j P(C_i|M_j) p(M_j) \\] <p>In laymens terms; we might be able to use the confusion matrix to learn from our mistakes. By how much we correct is something that we can tune with a hyperparameter.</p> \\[ P_{\\text{corrected}}(C_i) = \\alpha \\sum_j P(C_i|M_j) p(M_j) + (1-\\alpha) p(M_j) \\] <p>We'll perform an optimistic demonstration below.</p> Help functions <pre><code>def false_positives(mod, x, y):\n    return (mod.predict(x) != y)[y == 1].sum()\n\ndef false_negatives(mod, x, y):\n    return (mod.predict(x) != y)[y == 0].sum()\n</code></pre> Confusion Balancer<pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklego.meta import ConfusionBalancer\n\ncf_mod = ConfusionBalancer(LogisticRegression(solver=\"lbfgs\", max_iter=1000), alpha=1.0)\n\ngrid = GridSearchCV(\n    cf_mod,\n    param_grid={\"alpha\": np.linspace(-1.0, 3.0, 31)},\n    scoring={\n        \"accuracy\": make_scorer(accuracy_score),\n        \"positives\": false_positives,\n        \"negatives\": false_negatives\n    },\n    n_jobs=-1,\n    return_train_score=True,\n    refit=\"negatives\",\n    cv=5\n)\ngrid\n</code></pre> <p></p> Code to generate the plot <pre><code>df = pd.DataFrame(grid.fit(X, y).cv_results_)\nplt.figure(figsize=(12, 3))\n\nplt.subplot(121)\nplt.plot(df[\"param_alpha\"], df[\"mean_test_positives\"], label=\"false positives\")\nplt.plot(df[\"param_alpha\"], df[\"mean_test_negatives\"], label=\"false negatives\")\nplt.legend()\nplt.subplot(122)\nplt.plot(df[\"param_alpha\"], df[\"mean_test_accuracy\"], label=\"test accuracy\")\nplt.plot(df[\"param_alpha\"], df[\"mean_train_accuracy\"], label=\"train accuracy\")\nplt.legend();\n</code></pre> <p>It seems that we can pick a value for \\(\\alpha\\) such that the confusion matrix is balanced. there's also a modest increase in accuracy for this balancing moment.</p> <p>It should be emphasized though that this feature is experimental. There have been dataset/model combinations where this effect seems to work very well while there have also been situations where this trick does not work at all.</p> <p>It also deserves mentioning that there might be alternative to your problem. If your dataset is suffering from a huge class imbalance then you might be better off by having a look at the imbalanced-learn project.</p>"},{"location":"user-guide/meta-models/#zero-inflated-regressor","title":"Zero-Inflated Regressor","text":"<p>There are regression datasets that contain an unusually high amount of zeroes as the targets.</p> <p>This can be the case if you want to predict a count of rare events, such as defects in manufacturing, the amount of some natural disasters or the amount of crimes in some neighborhood.</p> <p>Usually nothing happens, meaning the target count is zero, but sometimes we actually have to do some modelling work.</p> <p>The classical machine learning algorithms can have a hard time dealing with such datasets.</p> <p>Take linear regression for example: the chance of outputting an actual zero is diminishing.</p> <p>Sure, you can get regions where you are close to zero, but modelling an output of exactly zero is infeasible in  general. The same goes for neural networks.</p> <p>What we can do circumvent these problems is the following:</p> <ol> <li>Train a classifier to tell us whether the target is zero, or not.</li> <li>Train a regressor on all samples with a non-zero target.</li> </ol> <p>By putting these two together in an obvious way, we get the <code>ZeroInflatedRegressor</code>. You can use it like this:</p> ZeroInflatedRegressor<pre><code>import numpy as np\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklego.meta import ZeroInflatedRegressor\n\nnp.random.seed(0)\nX = np.random.randn(10000, 4)\ny = ((X[:, 0]&gt;0) &amp; (X[:, 1]&gt;0)) * np.abs(X[:, 2] * X[:, 3]**2) # many zeroes here, in about 75% of the cases.\n\nzir = ZeroInflatedRegressor(\n    classifier=RandomForestClassifier(random_state=0),\n    regressor=RandomForestRegressor(random_state=0)\n)\n\nprint(\"ZIR (RFC+RFR) r\u00b2:\", cross_val_score(zir, X, y).mean())\nprint(\"RFR r\u00b2:\", cross_val_score(RandomForestRegressor(random_state=0), X, y).mean())\n</code></pre> <pre><code>ZIR (RFC+RFR) r\u00b2: 0.8992404366385873\nRFR r\u00b2: 0.8516522752031502\n</code></pre>"},{"location":"user-guide/meta-models/#outlier-classifier","title":"Outlier Classifier","text":"<p>Outlier models are unsupervised so they don't have <code>predict_proba</code> or <code>score</code> methods.</p> <p>In case you have some labelled samples of which you know they should be outliers, it could be useful to calculate metrics as if the outlier model was a classifier.</p> <p>Moreover, if outlier models had a <code>predict_proba</code> method, you could use a classification model combined with an outlier detection model in a <code>StackingClassifier</code> and take advantage of probability outputs of both categories of models.</p> <p>To this end, the <code>OutlierClassifier</code> turns an outlier model into a classification model.</p> <p>A field of application is fraud: when building a model to detect fraud, you often have some labelled positives but you know there should be more, even in your train data.</p> <p>If you only use anomaly detection, you don't make use of the information in your labelled data set. If you only use a classifier, you might have insufficient data to do proper train-test splitting and perform hyperparameter optimization.</p> <p>Therefore one could combine the two approaches as shown below to get the best of both worlds.</p> <p>In this example, we change the outlier model <code>IsolationForest</code> into a classifier using the <code>OutlierClassifier</code>. We create a random dataset with 1% outliers.</p> OutlierClassifier<pre><code>import numpy as np\nfrom sklego.meta.outlier_classifier import OutlierClassifier\nfrom sklearn.ensemble import IsolationForest\n\nn_normal = 10_000\nn_outlier = 100\nnp.random.seed(0)\nX = np.hstack((np.random.normal(size=n_normal), np.random.normal(10, size=n_outlier))).reshape(-1,1)\ny = np.hstack((np.asarray([0]*n_normal), np.asarray([1]*n_outlier)))\n\nclf = OutlierClassifier(IsolationForest(n_estimators=1000, contamination=n_outlier/n_normal, random_state=0))\nclf.fit(X, y)\n</code></pre> <pre>OutlierClassifier(model=IsolationForest(contamination=0.01, n_estimators=1000,\n                                        random_state=0))</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.OutlierClassifier<pre>OutlierClassifier(model=IsolationForest(contamination=0.01, n_estimators=1000,\n                                        random_state=0))</pre>model: IsolationForest<pre>IsolationForest(contamination=0.01, n_estimators=1000, random_state=0)</pre>IsolationForest<pre>IsolationForest(contamination=0.01, n_estimators=1000, random_state=0)</pre> <p>Anomaly detection algorithms in scikit-Learn return values <code>-1</code> for inliers and <code>1</code> for outliers.</p> <p>As you can see, the <code>OutlierClassifier</code> predicts inliers as <code>0</code> and outliers as <code>1</code>:</p> OutlierClassifier output<pre><code>print(\"inlier: \", clf.predict([[0]]))\nprint(\"outlier: \", clf.predict([[10]]))\n</code></pre> <pre><code>inlier:  [0.]\noutlier:  [1.]\n</code></pre> <p>The <code>predict_proba</code> method returns probabilities for both classes (inlier, outlier):</p> OutlierClassifier predict_proba<pre><code>clf.predict_proba([[10]])\n</code></pre> <p>array([[0.0376881, 0.9623119]])</p> <p>The <code>OutlierClassifier</code> can be combined with any classification model in the <code>StackingClassifier</code> as follows:</p> Stacking OutlierClassifier<pre><code>from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n\nestimators = [\n    (\"anomaly\", OutlierClassifier(IsolationForest())),\n    (\"classifier\", RandomForestClassifier())\n    ]\n\nstacker = StackingClassifier(estimators, stack_method=\"predict_proba\", passthrough=True)\nstacker.fit(X,y)\n</code></pre> <pre>StackingClassifier(estimators=[('anomaly',\n                                OutlierClassifier(model=IsolationForest())),\n                               ('classifier', RandomForestClassifier())],\n                   passthrough=True, stack_method='predict_proba')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StackingClassifier<pre>StackingClassifier(estimators=[('anomaly',\n                                OutlierClassifier(model=IsolationForest())),\n                               ('classifier', RandomForestClassifier())],\n                   passthrough=True, stack_method='predict_proba')</pre>anomalymodel: IsolationForest<pre>IsolationForest()</pre>IsolationForest<pre>IsolationForest()</pre>classifierRandomForestClassifier<pre>RandomForestClassifier()</pre>final_estimatorLogisticRegression<pre>LogisticRegression()</pre>"},{"location":"user-guide/meta-models/#ordinal-classification","title":"Ordinal Classification","text":"<p>Ordinal classification (sometimes also referred to as Ordinal Regression) involves predicting an ordinal target variable, where the classes have a meaningful order. Examples of this kind of problem are: predicting customer satisfaction on a scale from 1 to 5, predicting the severity of a disease, predicting the quality of a product, etc.</p> <p>The <code>OrdinalClassifier</code> is a meta-model that can be used to transform any classifier into an ordinal classifier by fitting N-1 binary classifiers, each handling a specific class boundary, namely: \\(P(y &lt;= 1), P(y &lt;= 2), ..., P(y &lt;= N-1)\\).</p> <p>This implementation is based on the paper A simple approach to ordinal classification and it allows to predict the ordinal probabilities of each sample belonging to a particular class.</p> Graphical representation <p>An image (from the paper itself) is worth a thousand words: </p> <p>mord library</p> <p>If you are looking for a library that implements other ordinal classification algorithms, you can have a look at the mord library.</p> Ordinal Data<pre><code>import pandas as pd\n\nurl = \"https://stats.idre.ucla.edu/stat/data/ologit.dta\"\ndf = pd.read_stata(url).assign(apply_codes = lambda t: t[\"apply\"].cat.codes)\n\ntarget = \"apply_codes\"\nfeatures = [c for c in df.columns if c not in {target, \"apply\"}]\n\nX, y = df[features].to_numpy(), df[target].to_numpy()\ndf.head()\n</code></pre> apply pared public gpa apply_codes very likely 0 0 3.26 2 somewhat likely 1 0 3.21 1 unlikely 1 1 3.94 0 somewhat likely 0 0 2.81 1 somewhat likely 0 0 2.53 1 <p>Description of the dataset from statsmodels tutorial:</p> <p>This dataset is about the probability for undergraduate students to apply to graduate school given three exogenous variables:</p> <ul> <li>their grade point average (<code>gpa</code>), a float between 0 and 4.</li> <li><code>pared</code>, a binary that indicates if at least one parent went to graduate school.</li> <li><code>public</code>, a binary that indicates if the current undergraduate institution of the student is &gt; public or private.</li> </ul> <p><code>apply</code>, the target variable is categorical with ordered categories: \"unlikely\" &lt; \"somewhat likely\" &lt; \"very likely\".</p> <p>[...]</p> <p>For more details see the the Documentation of OrderedModel, the UCLA webpage.</p> <p>The only transformation we are applying to the data is to convert the target variable to an ordinal categorical variable by mapping the ordered categories to integers using their (pandas) category codes.</p> <p>We are now ready to train a <code>OrdinalClassifier</code> on this dataset:</p> OrdinalClassifier<pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklego.meta import OrdinalClassifier\n\nord_clf = OrdinalClassifier(LogisticRegression(), n_jobs=-1, use_calibration=False)\n_ = ord_clf.fit(X, y)\nord_clf.predict_proba(X[0])\n</code></pre> <p>[[0.54883853 0.36225347 0.088908]]</p>"},{"location":"user-guide/meta-models/#probability-calibration","title":"Probability Calibration","text":"<p>The <code>OrdinalClassifier</code> emphasizes the importance of proper probability estimates for its functionality. It is recommended to use the <code>CalibratedClassifierCV</code> class from scikit-learn to calibrate the probabilities of the binary classifiers.</p> <p>Probability calibration is not enabled by default, but we provide a convenient keyword argument <code>use_calibration</code> to enable it as follows:</p> OrdinalClassifier with probability calibration<pre><code>from sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklego.meta import OrdinalClassifier\n\ncalibration_kwargs = {...}\n\nord_clf = OrdinalClassifier(\n    estimator=LogisticRegression(),\n    use_calibration=True,\n    calibration_kwargs=calibration_kwargs\n)\n\n# This is equivalent to:\nestimator = CalibratedClassifierCV(LogisticRegression(), **calibration_kwargs)\nord_clf = OrdinalClassifier(estimator)\n</code></pre>"},{"location":"user-guide/meta-models/#computation-time","title":"Computation Time","text":"<p>As a meta-estimator, the <code>OrdinalClassifier</code> fits N-1 binary classifiers, which may be computationally expensive, especially with a large number of samples, features, or a complex classifier.</p>"},{"location":"user-guide/mixture-methods/","title":"Mixture Methods","text":"<p>Gaussian Mixture Models (GMMs) are flexible building blocks for other machine learning algorithms.</p> <p>This is in part because they are great approximations for general probability distributions but also because they remain somewhat interpretable even when the dataset gets very complex.</p> <p>This package makes use of GMMs to construct other algorithms.</p>"},{"location":"user-guide/mixture-methods/#classification","title":"Classification","text":"<p>Below is some example code of how you might use a GMMClassifier from sklego to perform classification.</p> GMMClassifier<pre><code>import numpy as np\nimport matplotlib.pylab as plt\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklego.mixture import GMMClassifier\n\nn = 1000\nX, y = make_moons(n)\nX = X + np.random.normal(n, 0.12, (n, 2))\nX = StandardScaler().fit_transform(X)\nU = np.random.uniform(-2, 2, (10000, 2))\n\nmod = GMMClassifier(n_components=4).fit(X, y)\n\nplt.figure(figsize=(14, 5))\nplt.subplot(121)\nplt.scatter(X[:, 0], X[:, 1], c=mod.predict(X), s=8)\nplt.title(\"classes of points\");\n\nplt.subplot(122)\nplt.scatter(U[:, 0], U[:, 1], c=mod.predict_proba(U)[:, 1], s=8)\nplt.title(\"classifier boundary\");\n</code></pre> <p></p>"},{"location":"user-guide/mixture-methods/#outlier-detection","title":"Outlier Detection","text":"<p>Below is some example code of how you might use a GMM from sklego to do outlier detection.</p> <p>Note that the GMMOutlierDetector generates prediction values that are either -1 (outlier) or +1 (normal).</p> GMMOutlierDetector<pre><code>import numpy as np\nimport matplotlib.pylab as plt\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklego.mixture import GMMOutlierDetector\n\nn = 1000\nX = make_moons(n)[0] + np.random.normal(n, 0.12, (n, 2))\nX = StandardScaler().fit_transform(X)\nU = np.random.uniform(-2, 2, (10000, 2))\n\nmod = GMMOutlierDetector(n_components=16, threshold=0.95).fit(X)\n\nplt.figure(figsize=(14, 5))\nplt.subplot(121)\nplt.scatter(X[:, 0], X[:, 1], c=mod.score_samples(X), s=8)\nplt.title(\"likelihood of points given mixture of 16 gaussians\");\n\nplt.subplot(122)\nplt.scatter(U[:, 0], U[:, 1], c=mod.predict(U), s=8)\nplt.title(\"outlier selection\");\n</code></pre> <p></p> <p>Remark that with a GMM there are multiple ways to select outliers. Instead of selection points that are beyond the likely quantile threshold one can also specify the number of standard deviations away from the most likely standard deviations a given point it.</p> Different thresholds <pre><code>plt.figure(figsize=(14, 5))\nfor i in range(1, 5):\n    mod = GMMOutlierDetector(n_components=16, threshold=i, method=\"stddev\").fit(X)\n    plt.subplot(140 + i)\n    plt.scatter(U[:, 0], U[:, 1], c=mod.predict(U), s=8)\n    plt.title(f\"outlier sigma={i}\");\n</code></pre> <p></p>"},{"location":"user-guide/mixture-methods/#detection-details","title":"Detection Details","text":"<p>The outlier detection methods that we use are based on the likelihoods that come out of the estimated Gaussian Mixture.</p> <p>Depending on the setting you choose we have a different method for determining if a point is inside or outside the threshold.</p> <ol> <li>If the <code>\"quantile\"</code> method is used, we take all the likelihood scores found that the GMM associates on a training dataset to determine where to set a threshold. The threshold value must be between 0 and 1 here.</li> <li>If the <code>\"stddev\"</code> method is used, then the threshold value is now interpreted as the number of standard deviations lower than the mean we are. We only calculate the standard deviation on the lower scores because there's usually more variance here.     !!! note         This setting allows you to be much more picky in selecting than the <code>\"quantile\"</code> one since this method allows you to be more exclusive than the <code>\"quantile\"</code> method with threshold equal to one.</li> </ol> <p></p> <p>As a sidenote: this image was generated with some dummy data, but its code can be found below:</p> <p>Code for plot generation</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom scipy.stats import gaussian_kde\n\nsns.set_theme()\n\nscore_samples = np.random.beta(220, 10, 3000)\ndensity = gaussian_kde(score_samples)\nlikelihood_range = np.linspace(0.80, 1.0, 10000)\n\nindex_max_y = np.argmax(density(likelihood_range))\nmean_likelihood = likelihood_range[index_max_y]\nnew_likelihoods = score_samples[score_samples &lt; mean_likelihood]\nnew_likelihoods_std = np.sqrt(np.sum((new_likelihoods - mean_likelihood) ** 2) / (len(new_likelihoods) - 1))\n\nplt.figure(figsize=(14, 3))\nplt.subplot(121)\nplt.plot(likelihood_range, density(likelihood_range), \"k\")\nxs = np.linspace(0.8, 1.0, 2000)\nplt.fill_between(xs, density(xs), alpha=0.8)\nplt.title(\"log-lik values from with GMM, quantile is based on blue part\")\n\nplt.subplot(122)\nplt.plot(likelihood_range, density(likelihood_range), \"k\")\nplt.vlines(mean_likelihood, 0, density(mean_likelihood), linestyles=\"dashed\")\nxs = np.linspace(0.8, mean_likelihood, 2000)\nplt.fill_between(xs, density(xs), alpha=0.8)\nplt.title(\"log-lik values from with GMM, stddev is based on blue part\")\n</code></pre>"},{"location":"user-guide/naive-bayes/","title":"Naive Bayes","text":"<p>Naive Bayes models are flexible and interpretable. In scikit-lego we've added support for a Gaussian Mixture variant of the algorithm.</p> <p></p> <p>An example of the usage of algorithm can be found below.</p>"},{"location":"user-guide/naive-bayes/#example","title":"Example","text":"<p>Let's first import the dependencies and create some data. This code will create a plot of the dataset we'll try to predict.</p> Simulated dataset<pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nsns.set_theme()\n\nn = 10000\n\ndef make_arr(mu1, mu2, std1=1, std2=1, p=0.5):\n    res = np.where(np.random.uniform(0, 1, n) &gt; p,\n                    np.random.normal(mu1, std1, n),\n                    np.random.normal(mu2, std2, n));\n    return np.expand_dims(res, 1)\n\nnp.random.seed(42)\nX1 = np.concatenate([make_arr(0, 4), make_arr(0, 4)], axis=1)\nX2 = np.concatenate([make_arr(-3, 7), make_arr(2, 2)], axis=1)\n\nplt.figure(figsize=(4,4))\nplt.scatter(X1[:, 0], X1[:, 1], alpha=0.5)\nplt.scatter(X2[:, 0], X2[:, 1], alpha=0.5)\nplt.title(\"simulated dataset\");\n</code></pre> <p></p> <p>Note that this dataset would be hard to classify directly if we would be using a standard Gaussian Naive Bayes algorithm since the orange class is multipeaked over two clusters.</p> <p>To demonstrate this we'll run our <code>GaussianMixtureNB</code> algorithm with one or two gaussians that the mixture is allowed to find.</p> GaussianMixtureNB model<pre><code>from sklego.naive_bayes import GaussianMixtureNB\ncmap=sns.color_palette(\"flare\", as_cmap=True)\n\nX = np.concatenate([X1, X2])\ny = np.concatenate([np.zeros(n), np.ones(n)])\nplt.figure(figsize=(8, 8))\nfor i, k in enumerate([1, 2]):\n    mod = GaussianMixtureNB(n_components=k).fit(X, y)\n    plt.subplot(220 + i * 2 + 1)\n    pred = mod.predict_proba(X)[:, 0]\n    plt.scatter(X[:, 0], X[:, 1], c=pred, cmap=cmap)\n    plt.title(f\"predict_proba k={k}\")\n\n    plt.subplot(220 + i * 2 + 2)\n    pred = mod.predict(X)\n    plt.scatter(X[:, 0], X[:, 1], c=pred, cmap=cmap)\n    plt.title(f\"predict k={k}\");\n</code></pre> <p></p> <p>Note that the second plot fits the original much better.</p> <p>We can even zoom in on this second algorithm by having it sample what it believes is the distribution on each column.</p> Model density Model density<pre><code>gmm1 = mod.gmms_[0.0]\ngmm2 = mod.gmms_[1.0]\nplt.figure(figsize=(8, 8))\n\nplt.subplot(221)\nplt.hist(gmm1[0].sample(n)[0], 30)\nplt.title(\"model 1 - column 1 density\")\nplt.subplot(222)\nplt.hist(gmm1[1].sample(n)[0], 30)\nplt.title(\"model 1 - column 2 density\")\nplt.subplot(223)\nplt.hist(gmm2[0].sample(n)[0], 30)\nplt.title(\"model 2 - column 1 density\")\nplt.subplot(224)\nplt.hist(gmm2[1].sample(n)[0], 30)\nplt.title(\"model 2 - column 2 density\");\n</code></pre> <p></p>"},{"location":"user-guide/outliers/","title":"Outliers","text":"<p>This package offers a few algorithms that might help you find outliers. Note that we offer a subset of algorithms that we could not find elsewhere. If you're interested in more algorithms we might recommend you have a look at pyod too. That said, we'll demonstrate a few approaches here.</p>"},{"location":"user-guide/outliers/#decomposition-based-detection","title":"Decomposition Based Detection","text":"<p>The scikit-learn ecosystem offers many tools for dimensionality reduction. Two popular variants are PCA and UMAP. What is nice about both of these methods is that they can reduce the data but also apply the inverse operation.</p> <p></p> <p>This is similar to what an autoencoder might do. But let's now say that we have a dataset \\(X\\) and that we're happy with our dimensionality reduction technique. In this situation there's a balance between reduction of data and loss of information.</p> <p>Suppose that we have a datapoint \\(x_{\\text{orig}}\\) we pass through our transformer after which we try to reconstruct it again. If \\(x_{\\text{orig}}\\) differs a lot from \\(x_{\\text{reconstruct}}\\) then we may have a good candidate to investigate as an outlier.</p> <p>We'll demonstrate both methods briefly below, using this following function to make some plots.</p> <p>Data and functionalities</p> <pre><code>import matplotlib.pylab as plt\nimport seaborn as sns\n\nfrom sklearn.datasets import load_iris\nfrom pandas.plotting import parallel_coordinates\nsns.set_theme()\n\nX_orig, y = load_iris(return_X_y=True, as_frame=True)\n\ndef plot_model(mod, components, threshold):\n    mod = mod(n_components=components, threshold=threshold, random_state=111).fit(X_orig)\n    X = X_orig.copy()\n    X['label'] = mod.predict(X)\n\n    plt.figure(figsize=(12, 3))\n    plt.subplot(121)\n    parallel_coordinates(X.loc[lambda d: d['label'] == 1], class_column='label', alpha=0.5)\n    parallel_coordinates(X.loc[lambda d: d['label'] == -1], class_column='label', color='red', alpha=0.7)\n    plt.title(\"outlier shown via parallel coordinates\")\n\n    if components == 2:\n        plt.subplot(122)\n        X_reduced = mod.transform(X_orig)\n        plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=X['label'], cmap=\"coolwarm_r\")\n        plt.title(\"outlier shown in 2d\");\n</code></pre>"},{"location":"user-guide/outliers/#pca-demonstration","title":"PCA Demonstration","text":"<p>Let's start with PCA methods to decompose and reconstruct the data, wrapped in the class <code>PCAOutlierDetection</code>.</p> <pre><code>from sklego.decomposition import PCAOutlierDetection\nplot_model(PCAOutlierDetection, components=2, threshold=0.1)\n</code></pre> <p></p>"},{"location":"user-guide/outliers/#umap-demonstration","title":"UMAP Demonstration","text":"<p>Let's now do the same with UMAP, wrapped in the class <code>UMAPOutlierDetection</code>.</p> <pre><code>from sklego.decomposition import UMAPOutlierDetection\nplot_model(UMAPOutlierDetection, components=2, threshold=0.1)\n</code></pre> <p></p> <p>One thing to keep in mind here: UMAP is a lot slower.</p>"},{"location":"user-guide/outliers/#interpretation-of-hyperparams","title":"Interpretation of Hyperparams","text":"<p>Both methods have a <code>n_components</code> and <code>threshold</code> parameter. The former tells the underlying transformer how many components to reduce to while the latter tells the model when to consider a reconstruction error \"too big\" for a datapoint not to be an outlier.</p> <p>If the relative error is larger than the set threshold it will be detected as an outlier. Typically that means that the threshold will be a lower value between 0.0 and 0.1. You can also specify an <code>absolute</code> threshold if that is preferable.</p> <p>The other parameters in both models are unique to their underlying transformer method.</p>"},{"location":"user-guide/outliers/#density-based-detection","title":"Density Based Detection","text":"<p>We've also got a few outlier detection techniques that are density based approaches. You will find a subset documented in the mixture method section but for completeness we will also list them below here as a comparison.</p>"},{"location":"user-guide/outliers/#gmmoutlierdetector-demonstration","title":"GMMOutlierDetector Demonstration","text":"<pre><code>from sklego.mixture import GMMOutlierDetector\n\nmod = GMMOutlierDetector(n_components=4, threshold=0.99).fit(X_orig)\nX = X_orig.copy()\nX['label'] = mod.predict(X)\n\nplt.figure(figsize=(12, 3))\nparallel_coordinates(X.loc[lambda d: d['label'] == 1], class_column='label', alpha=0.5)\nparallel_coordinates(X.loc[lambda d: d['label'] == -1], class_column='label', color='red', alpha=0.7)\nplt.title(\"outlier shown via parallel coordinates\");\n</code></pre>"},{"location":"user-guide/outliers/#bayesiangmmoutlierdetector-demonstration","title":"BayesianGMMOutlierDetector Demonstration","text":"<pre><code>from sklego.mixture import BayesianGMMOutlierDetector\n\nmod = BayesianGMMOutlierDetector(n_components=4, threshold=0.99).fit(X_orig)\nX = X_orig.copy()\nX['label'] = mod.predict(X)\n\nplt.figure(figsize=(12, 3))\nparallel_coordinates(X.loc[lambda d: d['label'] == 1], class_column='label', alpha=0.5)\nparallel_coordinates(X.loc[lambda d: d['label'] == -1], class_column='label', color='red', alpha=0.7)\nplt.title(\"outlier shown via parallel coordinates\");\n</code></pre> <p>Note that for these density based approaches the threshold needs to be interpreted differently. If you're interested, you can find more information here.</p>"},{"location":"user-guide/outliers/#model-based-outlier-detection","title":"Model Based Outlier Detection","text":"<p>Suppose that you've got an accurate model. Then you could argue that when a datapoint disagrees with your model that it might be an outlier.</p> <p>This library offers meta models that wrap estimators in order to become outlier detection models.</p>"},{"location":"user-guide/outliers/#regression-based","title":"Regression Based","text":"<p>If you have a regression model then we offer a <code>RegressionOutlierDetector</code>. This model takes the output of the regression model and compares it against the true regression labels. If the difference between the label and predicted value is larger than a threshold then we output an outlier flag.</p> <p>Note that in order to be complaint to the scikit-learn API we require that the <code>y</code>-label for the regression to be part of the <code>X</code> dataset.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklego.meta import RegressionOutlierDetector\n\nsns.set_theme()\n\n# generate random data for illustrative example\nnp.random.seed(42)\nX = np.random.normal(0, 1, (100, 1))\ny = 1 + np.sum(X, axis=1).reshape(-1, 1) + np.random.normal(0, 0.2, (100, 1))\nfor i in [20, 25, 50, 80]:\n    y[i] += 2\nX = np.concatenate([X, y], axis=1)\n\n# fit and plot\nmod = RegressionOutlierDetector(LinearRegression(), column=1)\nmod.fit(X)\nplt.scatter(X[:, 0], X[:, 1], c=mod.predict(X), cmap='coolwarm_r');\n</code></pre> <p></p>"},{"location":"user-guide/pandas-pipelines/","title":"Pandas pipelines","text":"<p>Method chaining is a great way for writing pandas code as it allows us to go from:</p> <pre><code>raw_data = pd.read_parquet(...)\ndata_with_types = set_dtypes(raw_data)\ndata_without_outliers = remove_outliers(data_with_types)\n</code></pre> <p>to</p> <pre><code>data = (\n    pd.read_parquet(...)\n    .pipe(set_dtypes)\n    .pipe(remove_outliers)\n)\n</code></pre> <p>But it does come at a cost, mostly in our ability to debug long pipelines. If there's a mistake somewhere along the way, you can only inspect the end result and lose the ability to inspect intermediate results.</p> <p>A mitigation for this is to add decorators to your pipeline functions that log common attributes of your dataframe on each step:</p>"},{"location":"user-guide/pandas-pipelines/#logging-in-method-chaining","title":"Logging in method chaining","text":"<p>In order to use the logging capabilitites we first need to ensure we have a proper logger configured. We do this by running</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre> <p>Next load some data:</p> <pre><code>from sklego.datasets import load_chicken\n\nchickweight = load_chicken(as_frame=True)\n</code></pre> <p>If we now add a <code>log_step</code> decorator to our pipeline function and execute the function, we see that we get some logging statements for free:</p> <pre><code>from sklego.pandas_utils import log_step\n\n@log_step\ndef set_dtypes(chickweight):\n    return chickweight.assign(\n        diet=lambda d: d['diet'].astype('category'),\n        chick=lambda d: d['chick'].astype('category'),\n    )\n\nchickweight.pipe(set_dtypes).head()\n</code></pre> <pre><code>[set_dtypes(df)] time=0:00:00.015196 n_obs=578, n_col=4\n\n   weight  time chick diet\n0      42     0     1    1\n1      51     2     1    1\n2      59     4     1    1\n3      64     6     1    1\n4      76     8     1    1\n</code></pre> <p>We can choose to log at different log levels by changing the <code>print_fn</code> argument of the <code>log_step</code> decorator.</p> <p>For example if we have a <code>remove_outliers</code> function that calls different outlier removal functions for different types of outliers, we might in general be only interested in the total outliers removed. In order to get that, we set the log level for our specific implementations to <code>logging.debug</code>:</p> <pre><code>@log_step(print_fn=logging.debug)\ndef remove_dead_chickens(chickweight):\n    dead_chickens = chickweight.groupby('chick').size().loc[lambda s: s &lt; 12]\n    return chickweight.loc[lambda d: ~d['chick'].isin(dead_chickens)]\n\n\n@log_step(print_fn=logging.info)\ndef remove_outliers(chickweight):\n    return chickweight.pipe(remove_dead_chickens)\n\nchickweight.pipe(set_dtypes).pipe(remove_outliers).head()\n</code></pre> <pre><code>DEBUG:root:[remove_dead_chickens(df)] time=0:00:00.005965 n_obs=519, n_col=4\nINFO:root:[remove_outliers(df)] time=0:00:00.008321 n_obs=519, n_col=4\n[set_dtypes(df)] time=0:00:00.001860 n_obs=578, n_col=4\n\n   weight  time chick diet\n0      42     0     1    1\n1      51     2     1    1\n2      59     4     1    1\n3      64     6     1    1\n4      76     8     1    1\n</code></pre> <p>The <code>log_step</code> function has some settings that let you tweak what exactly to log:</p> <ul> <li><code>time_taken</code>: log the time it took to execute the function (default True)</li> <li><code>shape</code>: log the output shape of the function (default True)</li> <li><code>shape_delta</code>: log the difference in shape between input and output (default False)</li> <li><code>names</code>: log the column names if the output (default False)</li> <li><code>dtypes</code>: log the dtypes of the columns of the output (default False)</li> </ul> <p>For example, if we don't care how long a function takes, but do want to see how many rows are removed if we remove dead chickens:</p> <pre><code>@log_step(time_taken=False, shape=False, shape_delta=True)\ndef remove_dead_chickens(chickweight):\n    dead_chickens = chickweight.groupby('chick').size().loc[lambda s: s &lt; 12]\n    return chickweight.loc[lambda d: ~d['chick'].isin(dead_chickens)]\n\nchickweight.pipe(remove_dead_chickens).head()\n</code></pre> <pre><code>[remove_dead_chickens(df)] delta=(-59, 0)\n\n   weight  time  chick  diet\n0      42     0      1     1\n1      51     2      1     1\n2      59     4      1     1\n3      64     6      1     1\n4      76     8      1     1\n</code></pre> <p>We can also define custom logging functions by using <code>log_step_extra</code>.</p> <p>This takes any number of functions (&gt; 1) that can take the output dataframe and return some output that can be converted to a string.</p> <p>For example, if we want to log some arbitrary message and the number of unique chicks in our dataset, we can do:</p> <pre><code>from sklego.pandas_utils import log_step_extra\n\ndef count_unique_chicks(df, **kwargs):\n    return \"nchicks=\" + str(df[\"chick\"].nunique())\n\ndef display_message(df, msg):\n    return msg\n\n\n@log_step_extra(count_unique_chicks)\ndef start_pipe(df):\n    \"\"\"Get initial chick count\"\"\"\n    return df\n\n\n@log_step_extra(count_unique_chicks, display_message, msg=\"without diet 1\")\ndef remove_diet_1_chicks(df):\n    return df.loc[df[\"diet\"] != 1]\n\n(chickweight\n .pipe(start_pipe)\n .pipe(remove_diet_1_chicks)\n .head()\n)\n</code></pre> <pre><code>[start_pipe(df)] nchicks=50\n[remove_diet_1_chicks(df)] nchicks=30 without diet 1\n\n     weight  time  chick  diet\n220      40     0     21     2\n221      50     2     21     2\n222      62     4     21     2\n223      86     6     21     2\n224     125     8     21     2\n</code></pre>"},{"location":"user-guide/preprocessing/","title":"Preprocessing","text":"<p>There are many preprocessors in scikit-lego and in this document we would like to highlight a few such that you might be inspired to use pipelines a little bit more flexibly.</p>"},{"location":"user-guide/preprocessing/#estimators-as-transformers","title":"Estimators as Transformers","text":"<p>Sometimes you'd like the output of a model to be available as a feature that you might use as input for another model. The issue here is that scikit learn pipelines usually only allow a single model at the end of a pipeline.</p> <p>One solution to this problem is to turn the model into a transformer. To convert a model to become a transformer you can use the <code>EstimatorTransformer</code> from the <code>meta</code> module.</p>"},{"location":"user-guide/preprocessing/#example-1","title":"Example 1","text":"<p>Let's demonstrate one example. Below we describe how to create a pipeline with two models that each see the same dataset. Note that the output of this pipeline is still only a transformer pipeline.</p> <p></p> <pre><code>import numpy as np\nimport pandas as pd\n\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nfrom sklego.meta import EstimatorTransformer\nfrom sklego.preprocessing import ColumnSelector\n\nnp.random.seed(42)\nn = 1000\nX = np.random.uniform(0, 1, (n, 2))\ny = X.sum(axis=1) + np.random.uniform(0, 1, (n,))\ndf = pd.DataFrame({\"x1\": X[:, 0], \"x2\": X[:, 1], \"y\": y})\n\npipeline = Pipeline([\n    (\"grab_columns\", ColumnSelector([\"x1\", \"x2\"])),\n    (\"ml_features\", FeatureUnion([\n        (\"model_1\",  EstimatorTransformer(LinearRegression())),\n        (\"model_2\",  EstimatorTransformer(Ridge()))\n    ]))\n])\n\npipeline.fit(df, y).transform(df)\n</code></pre> <pre><code>array([[1.84239085, 1.8381264 ],\n       [1.84487058, 1.84095898],\n       [0.78867225, 0.79690879],\n       ...,\n       [1.92562838, 1.92076151],\n       [1.52504886, 1.52524312],\n       [0.81791076, 0.82568794]])\n</code></pre>"},{"location":"user-guide/preprocessing/#example-2","title":"Example 2","text":"<p>Here's another example that works a little bit differently. Here we have two models that each see different data.</p> <p></p> <pre><code>pipeline = Pipeline([\n    (\"grab_columns\", ColumnSelector([\"x1\", \"x2\"])),\n    (\"ml_features\", FeatureUnion([\n        (\"p1\", Pipeline([\n            (\"grab1\", ColumnSelector([\"x1\"])),\n            (\"mod1\", EstimatorTransformer(LinearRegression()))\n        ])),\n        (\"p2\", Pipeline([\n            (\"grab2\", ColumnSelector([\"x2\"])),\n            (\"mod2\", EstimatorTransformer(LinearRegression()))\n        ]))\n    ]))\n])\n\npipeline.fit(df, y).transform(df)\n</code></pre> <pre><code>array([[1.3810049 , 1.96265338],\n       [1.75182446, 1.5942067 ],\n       [1.15431258, 1.13093337],\n       ...,\n       [1.7719303 , 1.65521752],\n       [1.98484405, 1.03984466],\n       [1.05164825, 1.26300114]])\n</code></pre>"},{"location":"user-guide/preprocessing/#concatenating-pipelines","title":"Concatenating Pipelines","text":"<p>Transformers in scikit-learn typically do not add features. They replace them. Take <code>PCA</code> for example.</p> <p></p> <p>The new dataset that comes out \\(X^{\\text{new}}\\) would no longer have columns \\({x_1,...,x_4}\\) but would instead replace them with \\({x_{\\text{PCA}_1}, x_{\\text{PCA}_2}}\\).</p> <p>If we rethink the pipeline a little bit we might not have to loose the original data.</p> <p></p> <p>If you don't want to loose data, you can make use of a <code>FeatureUnion</code> and a <code>IdentityTransformer</code>.</p> <pre><code>import numpy as np\n\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklego.preprocessing import IdentityTransformer\nnp.random.seed(42)\n\nn = 100\nX = np.random.uniform(0, 1, (n, 4))\n\npipeline = Pipeline([\n    (\"split\", FeatureUnion([\n        (\"orig\", IdentityTransformer()),\n        (\"pca\", PCA(2)),\n    ]))\n])\n\nX_new = pipeline.fit_transform(X)\n</code></pre> <p>You can check below that this pipeline will concatenate features without replacing them.</p> <pre><code>print(np.round(X_new[:3], 4))\n</code></pre> <pre><code>array([[ 0.3745,  0.9507,  0.732 ,  0.5987,  0.4888, -0.0206],\n       [ 0.156 ,  0.156 ,  0.0581,  0.8662, -0.1584,  0.4143],\n       [ 0.6011,  0.7081,  0.0206,  0.9699,  0.1228, -0.1769]])\n</code></pre> <pre><code>print(np.round(X[:3], 4))\n</code></pre> <pre><code>array([[0.3745, 0.9507, 0.732 , 0.5987],\n       [0.156 , 0.156 , 0.0581, 0.8662],\n       [0.6011, 0.7081, 0.0206, 0.9699]])\n</code></pre>"},{"location":"user-guide/preprocessing/#column-capping","title":"Column Capping","text":"<p>Some models are great at interpolation but less good at extrapolation.</p> <p>One way to potentially circumvent this problem is by capping extreme values that occur in the dataset \\(X\\).</p> <p></p> <p>Let's demonstrate how <code>ColumnCapper</code> works in a few examples below.</p> <pre><code>import numpy as np\nfrom sklego.preprocessing import ColumnCapper\n\nnp.random.seed(42)\nX = np.random.uniform(0, 1, (100000, 2))\n\ncc = ColumnCapper()\noutput = cc.fit(X).transform(X)\nprint(f\"min capped at  5th quantile: {output.min(axis=0)}\")\nprint(f\"max capped at 95th quantile: {output.max(axis=0)}\")\n\ncc = ColumnCapper(quantile_range=(10, 90))\noutput = cc.fit(X).transform(X)\nprint(f\"min capped at 10th quantile: {output.min(axis=0)}\")\nprint(f\"max capped at 90th quantile: {output.max(axis=0)}\")\n\n# min capped at  5th quantile: [0.05120598 0.0502972 ]\n# max capped at 95th quantile: [0.94966328 0.94964339]\n# min capped at 10th quantile: [0.10029693 0.09934085]\n# max capped at 90th quantile: [0.90020412 0.89859006]\n</code></pre> <p>Warning</p> <p>Note that the `ColumnCapper`` does not deal with missing values but it does support pandas dataframes as well as infinite values.</p> <pre><code>arr = np.array([[0.0, np.inf],\n                [-np.inf, 1.0]])\ncc.transform(arr)\n</code></pre> <pre><code>array([[0.10029693, 0.89859006],\n    [0.10029693, 0.89859006]])\n</code></pre>"},{"location":"user-guide/preprocessing/#patsy-formulas","title":"Patsy Formulas","text":"<p>If you're used to the statistical programming language R you might have seen a formula object before. This is an object that represents a shorthand way to design variables used in a statistical model.</p> <p>The patsy python project took this idea and made it available for python. From sklego we've made a wrapper, called <code>PatsyTransformer</code>, such that you can also use these in your pipelines.</p> <pre><code>import pandas as pd\nfrom sklego.preprocessing import PatsyTransformer\n\ndf = pd.DataFrame({\n    \"a\": [1, 2, 3, 4, 5],\n    \"b\": [\"yes\", \"yes\", \"no\", \"maybe\", \"yes\"],\n    \"y\": [2, 2, 4, 4, 6]\n})\nX, y = df[[\"a\", \"b\"]], df[[\"y\"]].to_numpy()\n\npt = PatsyTransformer(\"a + np.log(a) + b\")\npt.fit(X, y).transform(X)\n</code></pre> <pre><code>DesignMatrix with shape (5, 5)\n  Intercept  b[T.no]  b[T.yes]  a  np.log(a)\n          1        0         1  1    0.00000\n          1        0         1  2    0.69315\n          1        1         0  3    1.09861\n          1        0         0  4    1.38629\n          1        0         1  5    1.60944\n  Terms:\n    'Intercept' (column 0)\n    'b' (columns 1:3)\n    'a' (column 3)\n    'np.log(a)' (column 4)\n</code></pre> <p>You might notice that the first column contains the constant array equal to one. You might also expect 3 dummy variable columns instead of 2.</p> <p>This is because the design matrix from patsy attempts to keep the columns in the matrix linearly independent of each other.</p> <p>If this is not something you'd want to create you can choose to omit it by indicating \"-1\" in the formula.</p> <pre><code>pt = PatsyTransformer(\"a + np.log(a) + b - 1\")\npt.fit(X, y).transform(X)\n</code></pre> <pre><code>DesignMatrix with shape (5, 5)\n  b[maybe]  b[no]  b[yes]  a  np.log(a)\n         0      0       1  1    0.00000\n         0      0       1  2    0.69315\n         0      1       0  3    1.09861\n         1      0       0  4    1.38629\n         0      0       1  5    1.60944\n  Terms:\n    'b' (columns 0:3)\n    'a' (column 3)\n    'np.log(a)' (column 4)\n</code></pre> <p>You'll notice that now the constant array is gone and it is replaced with a dummy array. Again this is now possible because patsy wants to guarantee that each column in this matrix is linearly independent of each other.</p> <p>The formula syntax is pretty powerful, if you'd like to learn we refer you to formulas documentation.</p>"},{"location":"user-guide/preprocessing/#repeating-basis-function-transformer","title":"Repeating Basis Function Transformer","text":"<p>Some variables are of a circular nature. For example, the days of the year, 1-Jan-2019 (day 1) is just as close to 2-Jan-2019 (day 2) as it is to 31-Dec-2018 (day 365).</p> <p>If you would encode day of year numerically you would lose this information, as 1 close 2 to but far from 365. The <code>RepeatingBasisFunction</code> transformer can remedy this problem.</p> <p>The transformer selects a column and transforms it with a given number of repeating (radial) basis functions, which have a bell curve shape. The basis functions are equally spaced over the input range. The key feature of repeating basis functions is that they are continuous when moving from the max to the min of the input range.</p> <p>As a result these repeating basis functions can capture how close each datapoint is to the center of each repeating basis function, even when the input data has a circular nature.</p>"},{"location":"user-guide/preprocessing/#example","title":"Example","text":"<p>Let's make some random data to start with. We have input data <code>day</code>, <code>day_of_year</code> and target <code>y</code>.</p> Data<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme()\n\n# generate features\nX = pd.DataFrame({\n    \"day\": np.arange(4*365),\n    \"day_of_year\": (1 + np.arange(4*365)) % 365\n})\n\n# generate target\nsignal1 = 4 + 3*np.sin(X[\"day\"]/365*2*np.pi)\nsignal2 = 4 * np.sin(X[\"day\"]/365*4*np.pi+365/2)\nnoise = np.random.normal(0, 0.9, len(X[\"day\"]))\ny = signal1 + signal2 + noise\n\n# plot\nfig = plt.figure(figsize=(17,3))\nax = fig.add_subplot(111)\nax.plot(X[\"day\"], y);\n</code></pre> <p></p> <p>Let's now create repeating basis functions based on <code>day_of_year</code>:</p> <pre><code>from sklego.preprocessing import RepeatingBasisFunction\n\nN_PERIODS = 5\nrbf = RepeatingBasisFunction(\n    n_periods=N_PERIODS,\n    remainder=\"passthrough\",\n    column=\"day_of_year\",\n    input_range=(1,365)\n)\n\n_ = rbf.fit(X)\nXt = rbf.transform(X)\n</code></pre> <p>Now let's plot our transformed features:</p> <pre><code>fig, axes = plt.subplots(nrows=Xt.shape[1], figsize=(17,12))\nfor i in range(Xt.shape[1]):\n    axes[i].plot(X[\"day\"], Xt[:,i])\n</code></pre> <p></p> <p>The <code>day_of_year</code> feature has been replaced with <code>N_PERIODS</code> repeating basis functions. These are bell curves that are equidistant over the 1-365 range. Each curve captures the information of being close to a particular <code>day_of_year</code>.</p> <p>For example, the curve in the top row captures how close a day is to new year's day. It peaks on day 1 with a value of 1 and smoothly drops at an equal rate in December and in the rest of January.</p> <p>Note, how the <code>day</code> feature still exists, in the transformed feature set as a result of the <code>remainder=\"passthrough\"</code> setting. The default setting <code>remainder=\"drop\"</code> will only keep the repeating basis functions and drop all columns of the original dataset.</p>"},{"location":"user-guide/preprocessing/#example-regression","title":"Example Regression","text":"<p>Let's use these features below in a regression.</p> <pre><code>from sklearn.linear_model import LinearRegression\n\nplt.figure(figsize=(17,3))\nplt.plot(X[\"day\"], y)\nplt.plot(X[\"day\"], LinearRegression().fit(Xt, y).predict(Xt), linewidth=2.0)\nplt.title(\"pretty fly for a linear regression\");\n</code></pre> <p></p> <p>Note that you can make this approach even more powerful for timeseries by choosing to ignore the far away past.</p> <p>To explore this idea we've also implemented a <code>DecayEstimator</code>. For more information see the section on meta estimators for this.</p>"},{"location":"user-guide/preprocessing/#interval-encoders","title":"Interval Encoders","text":"<p>Sometimes a linear regression doesn't entirely do what you'd like. Take this pattern;</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\n\nsns.set_theme()\n\nxs = np.arange(0, 600)/100/np.pi\nys = np.sin(xs) + np.random.normal(0, 0.1, 600)\n\npred_ys = LinearRegression().fit(xs.reshape(-1, 1), ys).predict(xs.reshape(-1, 1))\nplt.scatter(xs, ys)\nplt.scatter(xs, pred_ys)\nplt.title(\"not really the right pattern\");\n</code></pre> <p></p> <p>What we could do though, is preprocess the data such that it can be passed to a linear regression. We could construct intervals in the <code>x</code> values, smooth with regards to <code>y</code> and interpolate in between. You can see a demo of this below using the <code>IntervalEncoder</code> from sklego.</p> <pre><code>from sklego.preprocessing import IntervalEncoder\n\nplt.figure(figsize = (16, 3))\n\nfor idx, sigma in enumerate([1, 0.1, 0.01, 0.001]):\n    plt.subplot(140 + idx + 1)\n    fs = IntervalEncoder(n_chunks=20, span=sigma, method='normal').fit(xs.reshape(-1, 1), ys)\n    plt.scatter(xs, ys);\n    plt.plot(xs, fs.transform(xs.reshape(-1, 1)), color='orange', linewidth=2.0)\n    plt.title(f\"span={sigma}\");\n</code></pre> <p></p> <p>Note that we extrapolate using the estimates of the intervals at the edges. This ensures that we can make predictions out of sample.</p> <pre><code>from sklego.preprocessing import IntervalEncoder\n\nplt.figure(figsize = (16, 3))\n\nxs_extra = np.array([-1] + list(xs) + [3])\nfor idx, sigma in enumerate([1, 0.1, 0.01, 0.001]):\n    plt.subplot(140 + idx + 1)\n    fs = IntervalEncoder(n_chunks=20, span=sigma, method='normal').fit(xs.reshape(-1, 1), ys)\n    plt.scatter(xs, ys);\n    plt.plot(xs_extra, fs.transform(xs_extra.reshape(-1, 1)), color='orange', linewidth=2.0)\n    plt.title(f\"span={sigma}\");\n</code></pre> <p></p>"},{"location":"user-guide/preprocessing/#monotonic-encoding","title":"Monotonic Encoding","text":"<p>At the moment this feature is useful because it allows us to encode non-linear relationships. The real power of this approach is that we might apply constraints. We could create features that are strictly monotonic. When such features are passed to a model that respects these constraints then we might prevent artificial stupidity because we can force domain knowledge to be taken into account.</p> <p>Let's first define a function that helps us generate multiple datasets.</p> <pre><code>def generate_dataset(start, n=600):\n    xs = np.arange(start, start + n)/100/np.pi\n    y = np.sin(xs) + np.random.normal(0, 0.1, n)\n    return xs.reshape(-1, 1), y\n</code></pre> <p>Now that this is in there, let's first show the behavior of the <code>method=\"average\"</code> and <code>method=\"normal\"</code> settings.</p> <pre><code>i = 0\nplt.figure(figsize=(12, 6))\nfor method in ['average', 'normal']:\n    for data_init in [50, 600, 1200, 2100]:\n        i += 1\n        X, y = generate_dataset(start=data_init)\n        encoder = IntervalEncoder(n_chunks = 40, method=method, span=0.2)\n        plt.subplot(240 + i)\n        plt.title(f\"method={method}\")\n        plt.scatter(X.reshape(-1), y);\n        plt.plot(X.reshape(-1), encoder.fit_transform(X, y), color='orange', linewidth=2.0);\n</code></pre> <p></p> <p>Now let's see what occurs when we add a constraint that enforces the feature to only be <code>method=\"increasing\"</code> or <code>method=\"decreasing\"</code>.</p> <pre><code>i = 0\nplt.figure(figsize=(12, 6))\nfor method in ['increasing', 'decreasing']:\n    for data_init in [50, 600, 1200, 2100]:\n        i += 1\n        X, y = generate_dataset(start=data_init)\n        encoder = IntervalEncoder(n_chunks = 40, method=method, span=0.2)\n        plt.subplot(240 + i)\n        plt.title(f\"method={method}\")\n        plt.scatter(X.reshape(-1), y);\n        plt.plot(X.reshape(-1), encoder.fit_transform(X, y), color='orange', linewidth=2.0);\n</code></pre> <p></p> <p>If these features are now passed to a model that supports monotonicity constraints then we can build models with guarantees.</p>"}]}